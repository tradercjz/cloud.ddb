===== ./api/dependencies.py =====
# FILE: ./api/dependencies.py
from fastapi import Request, WebSocket
from arq.connections import ArqRedis

def get_arq_pool_from_context(context: Request | WebSocket) -> ArqRedis:
    """Internal helper to get the pool from either context type."""
    return context.app.state.arq_pool

async def get_arq_pool_http(request: Request) -> ArqRedis:
    """
    A FastAPI dependency for HTTP routes to retrieve the ARQ Redis pool.
    """
    return get_arq_pool_from_context(request)

async def get_arq_pool_ws(websocket: WebSocket) -> ArqRedis:
    """
    A FastAPI dependency for WebSocket routes to retrieve the ARQ Redis pool.
    """
    return get_arq_pool_from_context(websocket)
===== ./api/v1/__init__.py =====

===== ./api/v1/api.py =====
from fastapi import APIRouter
from api.v1.endpoints import auth, chat, environments, feedback

api_router = APIRouter()
api_router.include_router(auth.router, prefix="/auth", tags=["auth"])
api_router.include_router(environments.router, prefix="/environments", tags=["environments"])
api_router.include_router(chat.router, prefix="/chat", tags=["chat"])
api_router.include_router(feedback.router, prefix="/feedback", tags=["feedback"]) 
===== ./api/v1/endpoints/__init__.py =====

===== ./api/v1/endpoints/auth.py =====
from fastapi import APIRouter, Depends, HTTPException
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.orm import Session
from datetime import timedelta
from db import crud
from db.session import get_db
from core.security import create_access_token, verify_password
from schemas import Token, UserCreate # Assuming you might add user creation later
router = APIRouter()
@router.post("/token", response_model=Token)
async def login_for_access_token(db: Session = Depends(get_db), form_data: OAuth2PasswordRequestForm = Depends()):
    user = await crud.get_user_by_username(db, username=form_data.username)
    if not user or not verify_password(form_data.password, user.hashed_password):
        raise HTTPException(status_code=400, detail="Incorrect username or password")
    access_token_expires = timedelta(minutes=30)
    access_token = create_access_token(
        data={"sub": user.username}, expires_delta=access_token_expires
    )
    return {"access_token": access_token, "token_type": "bearer"}
===== ./api/v1/endpoints/environments.py =====

import uuid
from fastapi import APIRouter, Depends, HTTPException, Request, status, Response
from typing import List, Optional
from arq.connections import ArqRedis
import openai
from fastapi import APIRouter, Depends, HTTPException, Request, Response
import httpx
from starlette.background import BackgroundTask
from starlette.responses import StreamingResponse
from agent.code_executor import CodeExecutor
from agent.interactive_sql_executor import InteractiveSQLExecutor
from agent.tool_manager import ToolManager
from schemas import EnvironmentCreate, EnvironmentPublic, InteractiveSQLRequest, UserInDB, ChatQueryRequest, ChatQueryResponse
from db.session import get_db
from db import crud
from core.security import get_current_user
from worker import WorkerSettings
from api.dependencies import get_arq_pool_http, get_arq_pool_ws
from services.aliyun_eci import aliyun_service
import dolphindb
from typing import Dict, Any
from fastapi.responses import StreamingResponse
import json
import asyncio
from utils.json_utils import custom_json_serializer
from pydantic import BaseModel
import queue
import threading
from agent.tools.enhanced_ddb_tools import (
    InspectDatabaseTool, ListTablesTool, DescribeTableTool, QueryDataTool,
    CreateSampleDataTool, OptimizeQueryTool, GetFunctionDocumentationTool, SearchKnowledgeBaseTool,
)
from agent.tools.ddb_tools import RunDolphinDBScriptTool
from agent.tools.interactive_tools import AskForHumanFeedbackTool, PlanModeResponseTool
from agent.tools.completion_tool import AttemptCompletionTool
from fastapi import WebSocket, WebSocketDisconnect
import websockets
from starlette.responses import Response


router = APIRouter()

@router.post("/", response_model=EnvironmentPublic, status_code=status.HTTP_202_ACCEPTED)
async def create_environment(
    env_in: EnvironmentCreate,
    db = Depends(get_db),
    current_user: UserInDB = Depends(get_current_user),
    arq_pool: ArqRedis = Depends(get_arq_pool_http)
):
    """
    Create a new DolphinDB environment.
    This starts an asynchronous background task.
    """
    new_env = await crud.create_environment(db, env=env_in, owner_id=current_user.id)
    await arq_pool.enqueue_job("create_environment_task", new_env.id)
    return new_env

@router.get("/", response_model=List[EnvironmentPublic])
async def list_environments(
    db = Depends(get_db),
    current_user: UserInDB = Depends(get_current_user)
):
    """
    List all environments for the current user.
    """
    return await crud.list_environments_by_owner(db, owner_id=current_user.id)

@router.get("/{env_id}", response_model=EnvironmentPublic)
async def get_environment_status(
    env_id: str,
    db = Depends(get_db),
    current_user: UserInDB = Depends(get_current_user)
):
    """

    Get the status and details of a specific environment.
    """
    env = await crud.get_environment(db, env_id=env_id)
    if not env or env.owner_id != current_user.id:
        raise HTTPException(status_code=404, detail="Environment not found")
    
    if env.status == "RUNNING" and env.container_group_id:
        live_instances = await aliyun_service.describe_instances_batch(
            env.region_id, [env.container_group_id]
        )
        if env.container_group_id not in live_instances:
            print(f"Reactive check failed for {env_id}. Updating status.")
            await crud.update_environment_status(
                db, env.id, "DELETED", "Instance was not found on the cloud provider (verified on-demand)."
            )
            # Re-fetch the updated record to return to the user
            env = await crud.get_environment(db, env_id=env_id)

    return env

@router.delete("/{env_id}", status_code=status.HTTP_202_ACCEPTED)
async def delete_environment(
    env_id: str,
    db = Depends(get_db),
    current_user: UserInDB = Depends(get_current_user),
    arq_pool: ArqRedis = Depends(get_arq_pool_http)
):
    """
    Schedule an environment for deletion.
    """
    env = await crud.get_environment(db, env_id=env_id)
    if not env or env.owner_id != current_user.id:
        raise HTTPException(status_code=404, detail="Environment not found")
    
    await crud.update_environment_status(db, env_id, "DELETING", "Scheduled for deletion.")
    await arq_pool.enqueue_job("delete_environment_task", env_id)
    return {"message": "Environment deletion scheduled."}

@router.get("/{env_id}/connection", status_code=status.HTTP_200_OK)
async def check_environment_connection(
    env_id: str,
    response: Response, # 引入Response对象以便设置状态码
    db = Depends(get_db),
    current_user: UserInDB = Depends(get_current_user)
):
    """
    Checks the connectivity to a specific DolphinDB environment.
    """
    # 1. 从数据库获取环境信息
    env = await crud.get_environment(db, env_id=env_id)

    # 2. 授权检查：确保环境存在且属于当前用户
    if not env or env.owner_id != current_user.id:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Environment not found")

    # 3. 状态检查：确保环境是 RUNNING 状态
    if env.status != "RUNNING" or not env.public_ip:
        # 使用 409 Conflict 表示资源存在但状态不适合操作
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT, 
            detail=f"Environment is not in RUNNING state. Current status: {env.status}"
        )

    # 4. 尝试连接 DolphinDB 实例
    s = dolphindb.session()

    try:
        print(f"Attempting to connect to {env.public_ip}:{env.port} for env {env.id}...")
        s.connect(env.public_ip, env.port, "admin", "123456")
        # 运行一个简单的无害命令来验证连接是否真的可用
        s.run("1+1")
        print("Connection successful.")
        return {"status": "connected", "message": "Successfully connected to the DolphinDB instance."}
    except Exception as e:
        print(f"Connection failed: {e}")
        # 使用 503 Service Unavailable 表示后端服务暂时无法访问
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Failed to connect to the DolphinDB instance: {str(e)}"
        )
    finally:
        s.close()

@router.get("/{env_id}/schema", response_model=Dict[str, Any])
async def get_environment_schema(
    env_id: str,
    db = Depends(get_db),
    current_user: UserInDB = Depends(get_current_user)
):
    """
    Retrieves the database schema from a specific DolphinDB environment.
    """
    # 1. 授权和基础状态检查 (与上一个接口类似)
    env = await crud.get_environment(db, env_id=env_id)
    if not env or env.owner_id != current_user.id:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Environment not found")
    if env.status != "RUNNING" or not env.public_ip:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail=f"Environment is not in RUNNING state. Current status: {env.status}"
        )
    
    # 2. 连接 DolphinDB 并获取 Schema
    s = dolphindb.session()
    try:
        s.connect(env.public_ip, env.port, "admin", "123456")
        
        # 定义一个DolphinDB脚本来获取所有DFS数据库中的表及其schema
        # 注意: 这里我们只关注DFS数据库，因为它们是分布式且持久化的。
        # 您也可以修改脚本以包含内存表等。
        script = """
        def get_dfs_schema() {
            dfs_dbs = getClusterDFSDatabases()
            
            schema_info = dict(STRING,ANY)
            for(db in dfs_dbs){
                tables = getTables(database(db))
                db_tables_info = dict(STRING,ANY)
                for (table_name in tables) {
                    // 加载表对象以获取schema
                    tbl = loadTable(db, table_name)
                    col_defs = tbl.schema().colDefs
                    cols = []
                    for(col in col_defs){
                        cols.append!({
                            "name": col.name,
                            "type": col.typeString,
                            "extra": col.extra
                        })
                    }
                        
                    db_tables_info[table_name] = cols
                }
                schema_info[db] = db_tables_info
            }
            return schema_info
        }
        get_dfs_schema()
        """
        
        # 执行脚本
        schema_result = s.run(script)
        
        # 如果没有DFS数据库或表，结果可能是None或空字典，这都是正常情况
        if not schema_result:
            return {}
            
        return schema_result

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Failed to retrieve schema from the DolphinDB instance: {str(e)}"
        )
    finally:
        s.close()

from core.config import settings
if settings.OPENAI_API_BASE_URL:
    client = openai.OpenAI(
        api_key=settings.OPENAI_API_KEY,
        base_url=settings.OPENAI_API_BASE_URL,
    )
else:
    # 否则，使用默认的OpenAI官方服务
    client = openai.OpenAI(
        api_key=settings.OPENAI_API_KEY,
    )


@router.post("/{env_id}/chat", response_model=ChatQueryResponse)
async def environment_chat(
    env_id: str,
    request: ChatQueryRequest,
    db = Depends(get_db),
    current_user: UserInDB = Depends(get_current_user)
):
    """
    Handles AI-driven chat queries for a DolphinDB environment.
    """
    # 1. 授权和基础状态检查
    env = await crud.get_environment(db, env_id=env_id)
    if not env or env.owner_id != current_user.id:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Environment not found")
    if env.status != "RUNNING" or not env.public_ip:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail=f"Environment is not in RUNNING state. Current status: {env.status}"
        )
        
    # 2. 构建 Prompt
    # 这是“提示工程”的核心，需要不断优化
    table_schemas_str = "\n".join([f"Table `{name}` schema: {schema}" for name, schema in request.selected_tables_schema.items()])
    
    prompt_messages = [
        {
            "role": "system",
            "content": (
                "You are an expert in DolphinDB scripting. Your task is to convert a user's natural language question "
                "into an executable DolphinDB script based on the provided table schemas. "
                "You must follow these rules:\n"
                "1. ONLY respond with the DolphinDB script itself, enclosed in a single markdown code block.\n"
                "2. Do not provide any explanation, preamble, or additional text.\n"
                "3. If the user's question seems dangerous or unrelated to data querying (e.g., asking to delete data or asking about your identity), "
                "respond with a single word: 'ERROR'.\n"
                "4. The script should select at most 1000 rows to avoid excessive data transfer."
            )
        },
        {
            "role": "user",
            "content": (
                f"Here are the table schemas:\n{table_schemas_str}\n\n"
                f"Here is my question: \"{request.query}\"\n\n"
                f"Provide the DolphinDB script."
            )
        }
    ]

    # 3. 调用 OpenAI API
    generated_script = None
    try:
        print("Sending prompt to OpenAI...")
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",  # 您可以换成 gpt-4 或其他模型
            messages=prompt_messages,
            temperature=0.0 # 低温以获得更确定性的、可重复的脚本
        )
        response_text = completion.choices[0].message.content.strip()
        
        # 从markdown代码块中提取脚本
        if response_text.startswith("```") and response_text.endswith("```"):
            generated_script = response_text.split('\n', 1)[1].rsplit('\n', 1)[0].strip()
        elif response_text == 'ERROR':
             return ChatQueryResponse(response_type="error", data="The query is potentially unsafe or irrelevant.", generated_script=None)
        else:
            generated_script = response_text # 如果模型没有返回markdown，直接使用

    except Exception as e:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Error calling OpenAI API: {str(e)}")

    if not generated_script:
        return ChatQueryResponse(response_type="error", data="AI failed to generate a valid script.", generated_script=None)

    # 4. 连接 DolphinDB 并执行脚本
    s = dolphindb.session()
    try:
        s.connect(env.public_ip, env.port, "admin", "123456")
        print(f"Executing generated script:\n{generated_script}")
        result = s.run(generated_script)

        # 将结果转换为前端友好的格式 (例如，如果结果是Pandas DataFrame)
        # DolphinDB Python API 返回的类型多样，需要做一些处理
        import pandas as pd
        if isinstance(result, pd.DataFrame):
            # 将DataFrame转换为字典列表，并处理NaN/NaT等JSON不兼容的值
            result_json = result.to_dict(orient='records')
        elif isinstance(result, list) or isinstance(result, dict):
             result_json = result
        else: # 标量值
            result_json = [{"result": result}]

        return ChatQueryResponse(response_type="table", data=result_json, generated_script=generated_script)

    except Exception as e:
        return ChatQueryResponse(response_type="error", data=f"Error executing DolphinDB script: {str(e)}", generated_script=generated_script)
    finally:
        s.close()

@router.post("/{env_id}/chat-stream")
async def environment_chat_stream(
    env_id: str,
    request: ChatQueryRequest,
    db = Depends(get_db),
    current_user: UserInDB = Depends(get_current_user)
):
    """
    Handles AI-driven chat queries for a DolphinDB environment using a streaming response.
    """
    # 1. 授权和基础状态检查 (与非流式版本相同)
    env = await crud.get_environment(db, env_id=env_id)
    if not env or env.owner_id != current_user.id:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Environment not found")
    if env.status != "RUNNING" or not env.public_ip:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail=f"Environment is not in RUNNING state. Current status: {env.status}"
        )

    # 2. 定义一个异步生成器函数，它将逐步产生事件
    async def event_generator():
        # --- 事件 1: 告知前端流程开始 ---
        yield f"data: {json.dumps({'type': 'status', 'content': 'Analyzing query...'})}\n\n"
        await asyncio.sleep(0.1) # 短暂暂停，确保消息能及时发送

        # --- 步骤 A: 构建 Prompt 并调用 OpenAI ---
        generated_script = None
        try:
            table_schemas_str = "\n".join([f"Table `{name}` schema: {schema}" for name, schema in request.selected_tables_schema.items()])
            prompt_messages = [
                {"role": "system", "content": "You are a DolphinDB expert... (和之前一样的系统提示)"}, # 为简洁省略，请使用之前的完整提示
                {"role": "user", "content": f"Schemas:\n{table_schemas_str}\n\nQuestion: \"{request.query}\"\n\nScript:"}
            ]

            # --- 事件 2: 告知前端正在调用 AI ---
            yield f"data: {json.dumps({'type': 'status', 'content': 'Generating script with AI model...'})}\n\n"
            await asyncio.sleep(0.1)

            completion = await asyncio.to_thread(
                client.chat.completions.create,
                model=settings.OPENAI_MODEL_NAME,
                messages=prompt_messages,
                temperature=0.0
            )
            response_text = completion.choices[0].message.content.strip()

            # 从markdown代码块中提取脚本
            if response_text.startswith("```") and response_text.endswith("```"):
                generated_script = response_text.split('\n', 1)[1].rsplit('\n', 1)[0].strip()
            elif response_text == 'ERROR':
                yield f"data: {json.dumps({'type': 'error', 'content': 'The query is potentially unsafe or irrelevant.'})}\n\n"
                return # 终止生成器
            else:
                generated_script = response_text
            
            if not generated_script:
                yield f"data: {json.dumps({'type': 'error', 'content': 'AI failed to generate a valid script.'})}\n\n"
                return

            # --- 事件 3: 将生成的脚本发送给前端 ---
            yield f"data: {json.dumps({'type': 'generated_script', 'content': generated_script})}\n\n"
            await asyncio.sleep(0.1)

        except Exception as e:
            error_message = f"Error calling AI API: {str(e)}"
            yield f"data: {json.dumps({'type': 'error', 'content': error_message})}\n\n"
            return # 出现错误，终止生成器

        # --- 步骤 B: 连接 DolphinDB 并执行脚本 ---
        s = dolphindb.session()
        try:
            # --- 事件 4: 告知前端正在执行脚本 ---
            yield f"data: {json.dumps({'type': 'status', 'content': 'Executing script on DolphinDB instance...'})}\n\n"
            await asyncio.sleep(0.1)

            # DolphinDB的connect和run是阻塞IO操作，使用asyncio.to_thread在单独线程中运行以避免阻塞事件循环
            await asyncio.to_thread(s.connect, env.public_ip, env.port, "admin", "123456")
            result = await asyncio.to_thread(s.run, generated_script)


            # --- 事件 5: 发送最终结果 ---
            import pandas as pd
            if isinstance(result, pd.DataFrame):
                # 将NaN, NaT等替换为None(null)，以便JSON序列化
                result_df = result.where(pd.notnull(result), None)
                result_json = result_df.to_dict(orient='records')
            elif isinstance(result, list) or isinstance(result, dict):
                result_json = result
            else:
                result_json = [{"result": result}]
            final_json_string = json.dumps(
                {'type': 'final_result', 'content': result_json},
                default=custom_json_serializer
            )
            yield f"data: {final_json_string}\n\n"

        except Exception as e:
            error_message = f"Error executing DolphinDB script: {str(e)}"
            yield f"data: {json.dumps({'type': 'error', 'content': error_message})}\n\n"
            return
        finally:
            s.close()
                
        # --- 事件 6: 告知流程结束 ---
        yield f"data: {json.dumps({'type': 'done', 'content': 'Process finished.'})}\n\n"


    # 3. 返回一个StreamingResponse，它会消耗上面定义的异步生成器
    return StreamingResponse(event_generator(), media_type="text/event-stream")

@router.post("/{env_id}/interactive-sql")
async def run_interactive_sql_task(
    env_id: str,
    request: InteractiveSQLRequest,
    fastapi_request: Request,
    db = Depends(get_db),
    current_user: UserInDB = Depends(get_current_user)
):
    """
    Executes a stateless interactive SQL Agent loop for a specific environment.
    Streams back all status updates from the agent's execution cycle.
    """
    # 1. 授权和环境检查
    env = await crud.get_environment(db, env_id=env_id)
    if not env or env.owner_id != current_user.id:
        raise HTTPException(status_code=404, detail="Environment not found")
    if env.status != "RUNNING" or not env.public_ip:
        raise HTTPException(status_code=409, detail="Environment is not in a RUNNING state.")

    # 2. 获取在应用启动时创建的执行器模板
    executor_template: InteractiveSQLExecutor = fastapi_request.app.state.interactive_executor_template

    # 3. 动态创建本次请求专用的、配置了正确数据库连接的组件
    try:
        connection_details = {"host": env.public_ip, "port": env.port, "user": "admin", "password": "123456"}
        
        # 创建本次请求专用的CodeExecutor
        request_specific_executor = CodeExecutor(**connection_details)

        request_specific_tool_manager =  ToolManager([
            RunDolphinDBScriptTool(executor=request_specific_executor),
            GetFunctionDocumentationTool(project_path="."),
            InspectDatabaseTool(executor=request_specific_executor),
            ListTablesTool(executor=request_specific_executor),
            DescribeTableTool(executor=request_specific_executor),
            QueryDataTool(executor=request_specific_executor),
            CreateSampleDataTool(executor=request_specific_executor),
            OptimizeQueryTool(executor=request_specific_executor),
            AskForHumanFeedbackTool(),
            PlanModeResponseTool(),
            AttemptCompletionTool(),
            SearchKnowledgeBaseTool()
        ])
        
        # 创建本次请求专用的InteractiveSQLExecutor
        request_specific_interactive_executor = InteractiveSQLExecutor(
            tool_manager=request_specific_tool_manager
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to configure agent for environment: {e}")
        
    # 4. 定义异步生成器以处理SSE流
    async def event_generator():
        q = queue.Queue()

        def agent_thread_target():
            """在单独线程中运行同步的Agent任务生成器"""
            try:
                user_input = request.conversation_history[-1]['content'] if request.conversation_history else ""
                
                # 使用我们为本次请求特制的执行器
                task_generator = request_specific_interactive_executor.execute_task(
                    user_input=user_input,
                    conversation_history=request.conversation_history,
                    injected_context=request.injected_context
                )
                
                for update in task_generator:
                    q.put(update)
            except Exception as e:
                import traceback
                print(f"Error in agent thread: {e}\n{traceback.format_exc()}")
                q.put(e)
            finally:
                # 确保无论如何都有结束信号
                q.put(None)
                # 关闭本次请求的数据库连接
                request_specific_executor.close()

        # 启动工作线程
        thread = threading.Thread(target=agent_thread_target)
        thread.start()

        # 从队列中读取并流式发送结果
        while True:
            if await fastapi_request.is_disconnected():
                print(f"Client for env {env_id} disconnected, stopping agent task.")
                # 可以在这里添加更复杂的线程停止逻辑
                break
            
            try:
                update = q.get_nowait()
            except queue.Empty:
                await asyncio.sleep(0.05)
                continue

            if update is None: # 结束信号
                break
            
            if isinstance(update, Exception):
                yield f"data: {json.dumps({'type': 'error', 'content': str(update)})}\n\n"
                break
            
            # 将Pydantic模型或字典转换为JSON字符串
            if isinstance(update, BaseModel):
                # 使用自定义序列化器处理特殊类型
                json_payload = update.model_dump_json()
            elif isinstance(update, dict):
                # 对于字典，我们需要手动调用json.dumps并传入default处理器
                json_payload = json.dumps(update, default=str) # 使用str作为简单的备选方案
            else:
                json_payload = json.dumps({"type": "unknown", "content": str(update)})
            
            yield f"data: {json_payload}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")



# 创建一个可复用的 httpx 异步客户端
# allow_redirects=False 很重要，让重定向交由浏览器处理
# timeout 设置得长一些，以防 code-server 有长时间操作
client = httpx.AsyncClient(base_url="", timeout=30.0, follow_redirects=False)

@router.post("/{env_id}/codeserver/ticket", status_code=200)
async def create_codeserver_access_ticket(
    env_id: str,
    db = Depends(get_db),
    current_user: UserInDB = Depends(get_current_user), # 这个 endpoint 需要认证
    redis: ArqRedis = Depends(get_arq_pool_http)
):
    """
    Creates a short-lived, single-use ticket for accessing the code-server iframe.
    """
    # 验证用户是否有权访问此环境
    env = await crud.get_environment(db, env_id=env_id)
    if not env or env.owner_id != current_user.id:
        raise HTTPException(status_code=404, detail="Environment not found")

    ticket = f"cs-ticket-{uuid.uuid4().hex}"
    # 在 Redis 中存储 ticket，并设置 60 秒过期
    # key: ticket, value: env_id (或者更复杂的json，包含用户信息)
    await redis.set(ticket, env_id, ex=60)
    
    print(f"Generated ticket {ticket} for env {env_id}")
    return {"ticket": ticket}

@router.api_route("/{env_id}/codeserver/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS", "HEAD"])
async def reverse_proxy_code_server(
    env_id: str,
    path: str,
    request: Request,
    response: Response,
    ticket: Optional[str] = None,
    db = Depends(get_db),
    redis: ArqRedis = Depends(get_arq_pool_http)
):
    """
    Reverse proxy for the code-server instance associated with an environment.
    It forwards all requests under /codeserver/{path} to the actual code-server.
    """
    

    env = await crud.get_environment(db, env_id=env_id)
    if not env or env.status != "RUNNING" or not env.code_server_public_ip:
        logger.error(f"[{env_id}] Environment not ready for proxy. Status: {env.status if env else 'Not Found'}, IP: {env.code_server_public_ip if env else 'N/A'}")
        raise HTTPException(status_code=409, detail="Code server is not available for this environment.")
    
    # 2. 构建目标 URL
    target_url = f"http://{env.code_server_public_ip}:{env.code_server_port}/{path}"
    
    print(f"--- REVERSE PROXY DEBUG ---")
    print(f"Request path: /{path}")
    print(f"Target URL constructed: {target_url}")
    print(f"--------------------------")

    # 3. 准备请求参数，并转发请求
    # 特别注意要排除 host header，因为它应该指向内部服务，而不是我们的代理
    headers = {k: v for k, v in request.headers.items() if k.lower() != 'host'}
    
    # 确保 forwarded-for header 被正确设置
    headers['x-forwarded-for'] = request.client.host
    
    # 构建请求
    rp_req = client.build_request(
        method=request.method,
        url=target_url,
        headers=headers,
        params=request.query_params,
        content=await request.body()
    )

    # 发送请求
    try:
        rp_resp = await client.send(rp_req, stream=True)
    except httpx.RequestError as e:
        raise HTTPException(status_code=503, detail=f"Service unavailable: cannot connect to code-server. {e}")

    # 4. 流式回传响应
    # 这对于 WebSocket 和大文件传输至关重要
    return StreamingResponse(
        rp_resp.aiter_raw(),
        status_code=rp_resp.status_code,
        headers=rp_resp.headers,
        background=BackgroundTask(rp_resp.aclose) # 确保请求关闭
    )
    

import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@router.websocket("/{env_id}/codeserver/{path:path}")
async def websocket_proxy(
    websocket: WebSocket,
    env_id: str,
    path: str,
    ticket: Optional[str] = None,
    db = Depends(get_db),
    redis: ArqRedis = Depends(get_arq_pool_ws)
):
    """
    Manually implements a reverse proxy for WebSocket connections with detailed logging.
    """

    # --- 获取环境信息 ---
    env = await crud.get_environment(db, env_id=env_id)
    if not env or env.status != "RUNNING" or not env.code_server_public_ip:
        logger.error(f"[{env_id}] Environment not ready for WebSocket. Status: {env.status if env else 'Not Found'}, IP: {env.code_server_public_ip if env else 'N/A'}")
        await websocket.close(code=4004, reason="Code server is not available.")
        return
        
    # --- 构建目标 URL ---
    target_uri = f"ws://{env.code_server_public_ip}:{env.code_server_port}/{path}?{str(websocket.query_params)}"
    
    # --- 日志点 3: 准备接受客户端连接 ---
    logger.info(f"[{env_id}] Accepting client WebSocket connection...")
    await websocket.accept()
    logger.info(f"[{env_id}] Client WebSocket connection accepted.")

    try:
        # 准备转发的 headers
        forward_headers = dict(websocket.headers)
        target_host = f"{env.code_server_public_ip}:{env.code_server_port}"
        forward_headers['host'] = target_host

        # --- 日志点 4: 尝试连接上游服务器 ---
        logger.info(f"[{env_id}] Attempting to connect to upstream WebSocket: {target_uri}")
        
        async with websockets.connect(
            target_uri, 
            #additional_headers=forward_headers,
            open_timeout=15  # 将超时时间延长到15秒，以便观察
        ) as upstream_ws:
            
            # --- 日志点 5: 成功连接上游服务器 ---
            logger.info(f"[{env_id}] Successfully connected to upstream WebSocket. Starting bi-directional proxy.")
            
            # --- 双向转发逻辑 ---
            async def consumer(client_ws: WebSocket, upstream_ws_conn):
                try:
                    async for message in client_ws.iter_bytes():
                        await upstream_ws_conn.send(message)
                except WebSocketDisconnect:
                    logger.info(f"[{env_id}] Client WebSocket disconnected (consumer side).")

            async def producer(client_ws: WebSocket, upstream_ws_conn):
                try:
                    async for message in upstream_ws_conn:
                        await client_ws.send_bytes(message)
                except websockets.exceptions.ConnectionClosed:
                    logger.info(f"[{env_id}] Upstream WebSocket disconnected (producer side).")

            consumer_task = asyncio.create_task(consumer(websocket, upstream_ws))
            producer_task = asyncio.create_task(producer(websocket, upstream_ws))
            
            done, pending = await asyncio.wait(
                [consumer_task, producer_task],
                return_when=asyncio.FIRST_COMPLETED,
            )
            
            for task in pending:
                task.cancel()
            
            # --- 日志点 6: 代理循环结束 ---
            logger.info(f"[{env_id}] WebSocket proxy loop finished.")

    except asyncio.TimeoutError:
        logger.error(f"[{env_id}] TIMEOUT connecting to upstream WebSocket: {target_uri}")
        await websocket.close(code=1011, reason="Upstream connection timed out.")
    except websockets.exceptions.InvalidStatusCode as e:
        logger.error(f"[{env_id}] Upstream WebSocket connection failed with HTTP status {e.status_code}: {target_uri}")
        await websocket.close(code=4000 + e.status_code, reason=f"Upstream server returned status {e.status_code}")
    except WebSocketDisconnect:
        logger.info(f"[{env_id}] Client disconnected before upstream connection was fully established.")
    except Exception as e:
        logger.error(f"[{env_id}] An unexpected error occurred in WebSocket proxy: {e}", exc_info=True)
        if not websocket.client_state == websockets.protocol.State.CLOSED:
            await websocket.close(code=1011, reason="Proxy error")
===== ./api/v1/endpoints/chat.py =====
# FILE: ./api/v1/endpoints/chat.py

from typing import Optional
from fastapi import APIRouter, Depends, HTTPException, Request
from schemas import InteractiveSQLRequest, UserInDB
from core.security import get_current_user
from db.session import get_db
from db import crud, models

# --- Import necessary agent components ---
import queue
import threading
import asyncio
import json
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from agent.interactive_sql_executor import InteractiveSQLExecutor
from agent.tool_manager import ToolManager
from agent.code_executor import CodeExecutor

# --- Import ALL tools ---
from agent.tools.enhanced_ddb_tools import (
    InspectDatabaseTool, ListTablesTool, DescribeTableTool, QueryDataTool,
    CreateSampleDataTool, OptimizeQueryTool, GetFunctionDocumentationTool, SearchKnowledgeBaseTool,
)
from agent.tools.ddb_tools import RunDolphinDBScriptTool
from agent.tools.interactive_tools import AskForHumanFeedbackTool, PlanModeResponseTool
from agent.tools.completion_tool import AttemptCompletionTool
from agent.tools.file_tools import WriteFileTool

router = APIRouter()

@router.post("/")
async def interactive_chat(
    request: InteractiveSQLRequest,
    fastapi_request: Request, # Renamed to avoid conflict
    db = Depends(get_db),
    current_user: UserInDB = Depends(get_current_user)
):
    """
    Handles all interactive AI chat sessions.
    - If `env_id` is provided, it connects to the specified DolphinDB environment.
    - Otherwise, it runs in a database-agnostic mode with limited tools (e.g., for RAG).
    """
    env: Optional[models.Environment] = None
    
    print(f"Interactive chat requested by user {current_user.username}, env_id={request.env_id}")
    
    # --- Conditional Environment Loading ---
    if request.env_id:
        env = await crud.get_environment(db, env_id=request.env_id)
        if not env or env.owner_id != current_user.id:
            raise HTTPException(status_code=404, detail="Environment not found or you don't have access.")
        if env.status != "RUNNING" or not env.public_ip:
            raise HTTPException(status_code=409, detail="Environment is not in a RUNNING state.")

    main_loop = asyncio.get_running_loop()

    # The async generator and threading logic remains the same
    async def event_generator():
        q = queue.Queue()
        request_specific_executor: Optional[CodeExecutor] = None

        def agent_thread_target(loop):
            nonlocal request_specific_executor
            try:
                # --- Conditional Tool & Agent Configuration ---
                tools = []
                print("XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX")
                print(f"env:{env}")
                if env: # Database-aware mode
                    connection_details = {"host": env.public_ip, "port": env.port, "user": "admin", "password": "123456"}
                    request_specific_executor = CodeExecutor(**connection_details)
                    
                    # Full set of tools
                    tools = [
                        RunDolphinDBScriptTool(executor=request_specific_executor),
                        GetFunctionDocumentationTool(project_path="."),
                        InspectDatabaseTool(executor=request_specific_executor),
                        ListTablesTool(executor=request_specific_executor),
                        DescribeTableTool(executor=request_specific_executor),
                        QueryDataTool(executor=request_specific_executor),
                        # --- General tools also included ---
                        AskForHumanFeedbackTool(),
                        PlanModeResponseTool(),
                        AttemptCompletionTool(),
                        SearchKnowledgeBaseTool()
                    ]
                    
                    if env.code_server_group_id and env.region_id:
                        write_tool = WriteFileTool(
                            region_id=env.region_id,
                            container_group_id=env.code_server_group_id,
                            main_event_loop=loop
                        )
                        tools.append(write_tool)
                        print(f"Info: WriteFileTool added for env {env.id} with container_group_id {env.code_server_group_id}.")
                    else:
                        print(f"Warning: WriteFileTool not added for env {env.id} because code_server_group_id is missing.")
                else: # Database-agnostic mode
                    # Limited set of tools for RAG and general tasks
                    tools = [
                        AskForHumanFeedbackTool(),
                        PlanModeResponseTool(),
                        AttemptCompletionTool(),
                        SearchKnowledgeBaseTool()
                        # Note: No database tools are included here
                    ]

                tool_manager = ToolManager(tools)
                agent_executor = InteractiveSQLExecutor(tool_manager=tool_manager)
                
                user_input = request.conversation_history[-1]['content'] if request.conversation_history else ""
                
                task_generator = agent_executor.execute_task(
                    user_input=user_input,
                    conversation_history=request.conversation_history,
                    injected_context=request.injected_context
                )
                
                for update in task_generator:
                    q.put(update)

            except Exception as e:
                import traceback
                print(f"Error in agent thread: {e}\n{traceback.format_exc()}")
                q.put(e)
            finally:
                q.put(None) # End signal
                # If a database executor was created, ensure it's closed
                if request_specific_executor:
                    request_specific_executor.close()

        thread = threading.Thread(target=agent_thread_target, args=(main_loop,))
        thread.start()

        # Streaming loop (this part is identical to the original implementation)
        while True:
            if await fastapi_request.is_disconnected():
                print("Client disconnected, stopping agent task.")
                break
            
            try:
                update = q.get_nowait()
            except queue.Empty:
                await asyncio.sleep(0.05)
                continue

            if update is None:
                break
            
            if isinstance(update, Exception):
                yield f"data: {json.dumps({'type': 'error', 'content': str(update)})}\n\n"
                break
            
            if isinstance(update, BaseModel):
                json_payload = update.model_dump_json()
            elif isinstance(update, dict):
                json_payload = json.dumps(update, default=str)
            else:
                json_payload = json.dumps({"type": "unknown", "content": str(update)})
            
            yield f"data: {json_payload}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")
===== ./api/v1/endpoints/feedback.py =====
# FILE: ./api/v1/endpoints/feedback.py

from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from db.session import get_db
from db import crud
from schemas import FeedbackCreate, UserInDB
from core.security import get_current_user

router = APIRouter()

@router.post("/", status_code=status.HTTP_201_CREATED)
async def submit_feedback(
    feedback_in: FeedbackCreate,
    db: Session = Depends(get_db),
    current_user: UserInDB = Depends(get_current_user)
):
    """
    Receives and stores user feedback for a specific AI turn.
    """
    # 检查该 turn_id 是否已经提交过反馈，防止重复提交
    # 注意：这需要一个 get_feedback_by_turn_id 的 crud 函数
    # 为了简单起见，我们暂时依赖数据库的 unique 约束来处理重复
    try:
        await crud.create_feedback(db, feedback=feedback_in, owner_id=current_user.id)
        return {"status": "success", "message": "Feedback received successfully."}
    except Exception as e:
        # 这里的异常可能是因为 turn_id 已经存在（违反了 unique 约束）
        # 或者其他数据库错误
        print(f"Error saving feedback: {e}")
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail="Feedback for this turn may already exist or a database error occurred."
        )
===== ./core/__init__.py =====

===== ./core/config.py =====
from pydantic_settings import BaseSettings 

from typing import Optional

class Settings(BaseSettings):
    PROJECT_NAME: str = "DolphinDB Cloud Service"
    API_V1_STR: str = "/api/v1"

    # Security
    SECRET_KEY: str
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 30

    # Database
    DATABASE_URL: str

    # Redis for ARQ
    REDIS_HOST: str = "localhost"
    REDIS_PORT: int = 6379

    # Aliyun
    ALIYUN_ACCESS_KEY_ID: str
    ALIYUN_ACCESS_KEY_SECRET: str
    ALIYUN_REGION_ID: str
    ALIYUN_SECURITY_GROUP_ID: str
    ALIYUN_VSWITCH_ID: str
    DDB_CONTAINER_IMAGE_URL: str
    
    CODE_SERVER_CONTAINER_IMAGE_URL: str

    OPENAI_API_KEY: str
    OPENAI_API_BASE_URL: Optional[str] = None
    OPENAI_MODEL_NAME: Optional[str] = "gpt-3.5-turbo" # 提供一个默认模型

    DDB_HOST: str
    DDB_PORT: int
    DDB_USER: str
    DDB_PASSWORD: str

    LLM_API_KEY: str
    LLM_BASE_URL: Optional[str] = None
    LLM_MODEL: Optional[str] = "google/gemini-2.5-flash" # 提供一个默认模型

    class Config:
        case_sensitive = True
        env_file = ".env"

settings = Settings()
===== ./core/security.py =====
from datetime import datetime, timedelta
from typing import Optional

from jose import jwt, JWTError
from passlib.context import CryptContext
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer

from core.config import settings
from db.session import get_db
from db import crud
from schemas import UserInDB

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl=f"{settings.API_V1_STR}/auth/token")

def verify_password(plain_password: str, hashed_password: str) -> bool:
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    return pwd_context.hash(password)

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None) -> str:
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, settings.SECRET_KEY, algorithm="HS256")
    return encoded_jwt

async def get_current_user(token: str = Depends(oauth2_scheme), db = Depends(get_db)) -> UserInDB:
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(token, settings.SECRET_KEY, algorithms=["HS256"])
        username: str = payload.get("sub")
        if username is None:
            raise credentials_exception
    except JWTError:
        raise credentials_exception
    
    user = await crud.get_user_by_username(db, username=username)
    if user is None:
        raise credentials_exception
    return user
===== ./create_first_user.py =====
# FILE: create_first_user.py
import asyncio
from db.session import SessionLocal
from db import crud
from schemas import UserCreate

async def main():
    """
    A simple script to create the first user directly in the database.
    """
    print("--- Creating the first user ---")
    db = SessionLocal()
    
    username = "admin"
    password = "JZJZ112233"

    print(f"Checking if user '{username}' already exists...")
    user = await crud.get_user_by_username(db, username=username)
    
    if user:
        print(f"User '{username}' already exists. Skipping creation.")
    else:
        user_in = UserCreate(username=username, password=password)
        await crud.create_user(db, user=user_in)
        print(f"✅ User '{username}' created successfully!")
        print("You can now log in with these credentials.")
    
    await db.close()

if __name__ == "__main__":
    asyncio.run(main())
===== ./db/__init__.py =====

===== ./db/crud.py =====
from typing import List, Optional
from sqlalchemy.future import select
from sqlalchemy.orm import Session
from datetime import datetime, timedelta

from . import models
from schemas import UserCreate, EnvironmentCreate, FeedbackCreate 
from core.security import get_password_hash
from core.config import settings

# User CRUD
async def get_user_by_username(db: Session, username: str):
    result = await db.execute(select(models.User).filter(models.User.username == username))
    return result.scalars().first()

async def create_user(db: Session, user: UserCreate):
    hashed_password = get_password_hash(user.password)
    db_user = models.User(username=user.username, hashed_password=hashed_password)
    db.add(db_user)
    await db.commit()
    await db.refresh(db_user)
    return db_user

# Environment CRUD
async def get_environment(db: Session, env_id: str):
    result = await db.execute(select(models.Environment).filter(models.Environment.id == env_id))
    return result.scalars().first()

async def list_environments_by_owner(db: Session, owner_id: int):
    result = await db.execute(select(models.Environment).filter(models.Environment.owner_id == owner_id))
    return result.scalars().all()

async def create_environment(db: Session, env: EnvironmentCreate, owner_id: int):
    expires_at = datetime.utcnow() + timedelta(hours=env.lifetime_hours)
    db_env = models.Environment(
        owner_id=owner_id,
        spec_cpu=env.spec_cpu,
        spec_memory=env.spec_memory,
        expires_at=expires_at,
        region_id=settings.ALIYUN_REGION_ID,
    )
    db.add(db_env)
    await db.commit()
    await db.refresh(db_env)
    return db_env

async def update_environment_status(db: Session, env_id: str, status: str, message: str):
    env = await get_environment(db, env_id)
    if env:
        env.status = status
        env.message = message
        await db.commit()

async def update_environment_after_provisioning(db: Session, env_id: str, status: str, message: str, public_ip: str, container_group_id: str, code_server_public_ip: Optional[str] = None,
    code_server_group_id: Optional[str] = None):
    env = await get_environment(db, env_id)
    if env:
        env.status = status
        env.message = message
        env.public_ip = public_ip
        env.container_group_id = container_group_id
        if code_server_public_ip:
            env.code_server_public_ip = code_server_public_ip
        if code_server_group_id:
            env.code_server_group_id = code_server_group_id
        await db.commit()
        
async def get_expired_environments(db: Session):
    result = await db.execute(
        select(models.Environment)
        .filter(models.Environment.expires_at <= datetime.utcnow(), models.Environment.status == "RUNNING")
    )
    return result.scalars().all()

async def get_active_environments(db: Session) -> List[models.Environment]:
    """Fetches all environments that are supposed to be active on the cloud."""
    active_statuses = ["PROVISIONING", "RUNNING"]
    result = await db.execute(
        select(models.Environment)
        .filter(models.Environment.status.in_(active_statuses))
    )
    return result.scalars().all()

async def create_feedback(db: Session, feedback: FeedbackCreate, owner_id: int):
    db_feedback = models.Feedback(
        turn_id=feedback.turn_id,
        owner_id=owner_id,
        feedback_type=feedback.feedback,
        prompt=feedback.prompt,
        response=feedback.response,
        conversation_history=feedback.conversation_history
    )
    db.add(db_feedback)
    await db.commit()
    await db.refresh(db_feedback)
    return db_feedback
===== ./db/models.py =====
import uuid
from datetime import datetime
from sqlalchemy import Column, Integer, String, DateTime, Float, ForeignKey
from sqlalchemy.orm import relationship
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.types import JSON

Base = declarative_base()

class User(Base):
    __tablename__ = "users"
    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True, nullable=False)
    hashed_password = Column(String, nullable=False)
    environments = relationship("Environment", back_populates="owner", cascade="all, delete-orphan")

class Environment(Base):
    __tablename__ = "environments"
    id = Column(String, primary_key=True, default=lambda: f"ddb-env-{uuid.uuid4().hex[:8]}")
    owner_id = Column(Integer, ForeignKey("users.id"))
    
    status = Column(String, default="PENDING", index=True)
    message = Column(String, nullable=True)
    
    container_group_id = Column(String, nullable=True, unique=True)
    region_id = Column(String)
    public_ip = Column(String, nullable=True)
    port = Column(Integer, default=8848)
    
    code_server_group_id = Column(String, nullable=True, unique=True) # code-server ECI 的 ID
    code_server_public_ip = Column(String, nullable=True)            # code-server ECI 的公网 IP
    code_server_port = Column(Integer, default=8080)                 # code-server 服务的端口
    
    spec_cpu = Column(Float)
    spec_memory = Column(Float)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    expires_at = Column(DateTime, nullable=False)
    
    owner = relationship("User", back_populates="environments")
    
class Feedback(Base):
    __tablename__ = "feedback"
    
    id = Column(Integer, primary_key=True, index=True)
    turn_id = Column(String, index=True, nullable=False, unique=True)
    owner_id = Column(Integer, ForeignKey("users.id"))
    
    feedback_type = Column(String, nullable=False) # 'like' or 'dislike'
    prompt = Column(String, nullable=False)
    response = Column(String, nullable=False)
    
    # Use JSON type for conversation history. 
    # If using PostgreSQL, JSONB is more efficient.
    # For SQLite or MySQL, JSON is a good choice.
    conversation_history = Column(JSON, nullable=False)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    
    owner = relationship("User")
===== ./db/session.py =====
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from core.config import settings

engine = create_async_engine(settings.DATABASE_URL, pool_pre_ping=True)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine, class_=AsyncSession)

async def get_db():
    async with SessionLocal() as session:
        yield session
===== ./db/database_session.py =====
from typing import Any, Tuple
import dolphindb as ddb
class DatabaseSession:
    """数据库会话管理器"""
    def __init__(self, host: str, port: int, user: str, passwd: str, 
                keep_alive_time: int = 3600, reconnect: bool = True, logger=None):
        self.host = host 
        self.port = port
        self.user = user 
        self.passwd = passwd
        self.keep_alive_time = keep_alive_time
        self.reconnect = reconnect
        self.session = ddb.session()
        self.logger = logger
        self.isConnected = False

    def __enter__(self):
        self.session.connect(
            self.host, 
            int(self.port), 
            self.user, 
            self.passwd,
            keepAliveTime=self.keep_alive_time, 
            reconnect=self.reconnect
        )
        self.isConnected = True
        return self 

    def __exit__(self, exc_type, exc_value, traceback):
        self.session.close()
        self.isConnected = False

    
    def execute(self, script: str) -> Tuple[bool, Any]:
        """执行DolphinDB脚本并返回结果或错误"""
        try:
            result = self.session.run(script)
            return True, result
        except Exception as e:
            return False, str(e) 

    def connect(self):
        """显式建立连接（可多次调用，已连则跳过）"""
        if not self.isConnected:
            self.session.connect(
                self.host, 
                int(self.port), 
                self.user, 
                self.passwd,
                keepAliveTime=self.keep_alive_time, 
                reconnect=self.reconnect
            )

    def close(self):
        """显式关闭连接"""
        if self.isConnected:
            self.session.close()
            self.isConnected = False

===== ./main.py =====
from fastapi import FastAPI
from contextlib import asynccontextmanager
from arq import create_pool

from core.config import settings
from api.v1.api import api_router
from worker import WorkerSettings
from db.session import engine
from db.models import Base # Import Base
from fastapi.middleware.cors import CORSMiddleware

from agent.interactive_sql_executor import InteractiveSQLExecutor
from agent.tool_manager import ToolManager
from agent.tools.ddb_tools import RunDolphinDBScriptTool
from agent.tools.enhanced_ddb_tools import (
    InspectDatabaseTool, ListTablesTool, DescribeTableTool, QueryDataTool,
    CreateSampleDataTool, OptimizeQueryTool, GetFunctionDocumentationTool, SearchKnowledgeBaseTool,
)
from agent.tools.interactive_tools import AskForHumanFeedbackTool, PlanModeResponseTool
from agent.tools.completion_tool import AttemptCompletionTool
from agent.code_executor import CodeExecutor

@asynccontextmanager
async def lifespan(app: FastAPI):
    # --- Startup ---
    global arq_pool
    # Connect to Redis for ARQ
    app.state.arq_pool = await create_pool(WorkerSettings.redis_settings)
    
    # Create database tables
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    print("INFO:     Initializing Stateless Agent Components...")
    try:
        # 创建一个临时的 RAG 实例 (即使不用，某些工具构造函数也需要)
        # 我们可以创建一个模拟/空的RAG对象来避免初始化真实RAG的开销

        
        # 创建一个 "模板" CodeExecutor，它将在每次请求时被重新配置
        template_executor = CodeExecutor()

        # 初始化工具管理器
        # 注意：这里的executor是临时的，会在请求中被替换
        tool_manager = ToolManager([
            RunDolphinDBScriptTool(executor=template_executor),
            GetFunctionDocumentationTool(project_path="."),
            InspectDatabaseTool(executor=template_executor),
            ListTablesTool(executor=template_executor),
            DescribeTableTool(executor=template_executor),
            QueryDataTool(executor=template_executor),
            CreateSampleDataTool(executor=template_executor),
            OptimizeQueryTool(executor=template_executor),
            AskForHumanFeedbackTool(),
            PlanModeResponseTool(),
            AttemptCompletionTool(),
            SearchKnowledgeBaseTool()
        ])
        
        # 初始化交互式SQL执行器
        interactive_executor = InteractiveSQLExecutor(tool_manager)

        # 将这些组件存入 app.state
        app.state.interactive_executor_template = interactive_executor
        
        print("INFO:     Stateless Agent Components initialized.")
    except Exception as e:
        print(f"FATAL:    Failed to initialize Stateless Agent Components: {e}")
        import traceback
        traceback.print_exc()

    yield # The application runs here

    # --- Shutdown ---
    if getattr(app.state, "arq_pool", None):
        await app.state.arq_pool.close()

app = FastAPI(
    title=settings.PROJECT_NAME,
    openapi_url=f"{settings.API_V1_STR}/openapi.json",
    lifespan=lifespan
)

origins = [
    "*"
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  # 允许访问的源
    allow_credentials=True,  # 支持 cookie
    allow_methods=["*"],  # 允许所有方法
    allow_headers=["*"],  # 允许所有头
)
app.include_router(api_router, prefix=settings.API_V1_STR)

# Optional: Add a root endpoint
@app.get("/")
def read_root():
    return {"message": "Welcome to the DolphinDB Cloud Service"}
===== ./schemas.py =====
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime

# --- User Schemas ---
class UserBase(BaseModel):
    username: str

class UserCreate(UserBase):
    password: str

class UserInDB(UserBase):
    id: int
    class Config:
        from_attributes = True

# --- Token Schemas ---
class Token(BaseModel):
    access_token: str
    token_type: str

# --- Environment Schemas ---
class EnvironmentBase(BaseModel):
    spec_cpu: float = Field(..., gt=0, description="vCPU cores (e.g., 2.0)")
    spec_memory: float = Field(..., gt=0, description="Memory in GiB (e.g., 4.0)")

class EnvironmentCreate(EnvironmentBase):
    lifetime_hours: int = Field(default=24, le=48, description="How long the instance will live.")

class EnvironmentPublic(EnvironmentBase):
    id: str
    owner_id: int
    status: str
    message: Optional[str] = None
    public_ip: Optional[str] = None
    port: int
    code_server_public_ip: Optional[str] = None
    code_server_port: int
    region_id: str
    created_at: datetime
    expires_at: datetime

    class Config:
        from_attributes = True

class ChatQueryRequest(BaseModel):
    query: str = Field(..., description="User's natural language query.")
    # selected_tables_schema 是一个字典，键是表名，值是该表的schema描述
    # 这样后端就不需要重新查询schema，提高了效率
    selected_tables_schema: Dict[str, Any] = Field(..., description="Schema of the tables selected by the user.")
    # 历史对话可以帮助模型理解上下文，但我们先从简单的开始，设为可选
    history: Optional[List[Dict[str, str]]] = None

class ChatQueryResponse(BaseModel):
    response_type: str = Field(..., description="Type of the response, e.g., 'table', 'error', 'message'.")
    data: Any = Field(..., description="The content of the response, could be a list of records, an error message, etc.")
    generated_script: Optional[str] = Field(None, description="The DolphinDB script generated by the AI.")

class AgentTaskRequest(BaseModel):
    query: str # 只有在新任务时才需要
    mode: str = "sql" # 我们专注于sql模式
    task_id: Optional[str] = None # 用于恢复任务
    user_response: Optional[str] = None # 用户对交互请求的回应

class InteractiveSQLRequest(BaseModel):
    # 完整的对话历史，由前端管理和提供
    conversation_history: List[Dict[str, Any]]
    
    # 用户为本次任务注入的上下文（例如，选中的表）
    # 我们用一个字典来保持灵活性
    injected_context: Optional[Dict[str, Any]] = None
    env_id: Optional[str] = Field(None, description="The optional ID of the DolphinDB environment to connect to.")
    
class FeedbackCreate(BaseModel):
    turn_id: str = Field(..., description="The unique ID of the AI response node (turn).")
    feedback: str = Field(..., description="The user's feedback, e.g., 'like' or 'dislike'.")
    prompt: str = Field(..., description="The user's prompt that triggered this turn.")
    response: str = Field(..., description="The AI's final response for this turn.")
    conversation_history: List[Dict[str, Any]] = Field(..., description="The full conversation history up to this turn.")
===== ./services/__init__.py =====

===== ./services/aliyun_eci.py =====
# FILE: services/aliyun_eci.py

import asyncio
import time
from typing import Dict, List, Tuple

import aiohttp

from alibabacloud_eci20180808.client import Client as EciClient
from alibabacloud_tea_openapi import models as open_api_models
from alibabacloud_eci20180808 import models as eci_models
from alibabacloud_tea_util import models as util_models

from core.config import settings
from schemas import EnvironmentPublic

class AliyunECIService:
    """
    Service to manage DolphinDB instances on Alibaba Cloud ECI.
    This is the real implementation using the Alibaba Cloud SDK.
    """

    def _create_client(self, region_id: str) -> EciClient:
        """
        Initializes and returns an ECI client for a specific region.
        """
        config = open_api_models.Config(
            access_key_id=settings.ALIYUN_ACCESS_KEY_ID,
            access_key_secret=settings.ALIYUN_ACCESS_KEY_SECRET,
            region_id=region_id
        )
        # The endpoint is region-specific.
        config.endpoint = f'eci.{region_id}.aliyuncs.com'
        return EciClient(config)

    async def create_instance(self, env: EnvironmentPublic) -> Tuple[str, str]:
        """
        Provisions a complete DolphinDB instance on ECI.
        This includes creating the ECI container group with a public IP and a data volume.

        Returns:
            A tuple of (public_ip, container_group_id).
        """
        client = self._create_client(env.region_id)
        print(f"[{env.id}] Starting provisioning on Aliyun ECI in region {env.region_id}...")

        # NOTE: For ECI, we can directly assign an EIP and create a temporary data volume
        # without separate API calls for EIP and Disks, simplifying the process.
        
        container_group_name = f"dolphindb-env-{env.id}"

        # Define the container within the group
        container_definition = eci_models.CreateContainerGroupRequestContainer(
            name="dolphindb-container",
            image=settings.DDB_CONTAINER_IMAGE_URL,
            cpu=env.spec_cpu,
            memory=env.spec_memory,
            # Expose the DolphinDB port
            port=[eci_models.CreateContainerGroupRequestContainerPort(protocol="TCP", port=8848)]
        )

        # Define a temporary data volume. This volume's lifecycle is tied to the ECI.
        # For true persistence across restarts, you would use Aliyun Disks (ESSD).
        # For this on-demand use case, a temporary volume is often sufficient.
        volume_definition = eci_models.CreateContainerGroupRequestVolume(
            name="ddb-data-volume",
            type="EmptyDirVolume" # This creates a temporary directory
        )

        # Create the full container group request object
        create_request = eci_models.CreateContainerGroupRequest(
            region_id=env.region_id,
            container_group_name=container_group_name,
            security_group_id=settings.ALIYUN_SECURITY_GROUP_ID,
            v_switch_id=settings.ALIYUN_VSWITCH_ID,
            auto_create_eip=True,
            eip_bandwidth=200,
            container=[container_definition],
            # volumes=[volume_definition], # Uncomment if you use volumes
            # Add a tag for easy identification and cost tracking
            tag=[eci_models.CreateContainerGroupRequestTag(key="owner_id", value=str(env.owner_id))]
        )

        try:
            print(f"[{env.id}] Sending CreateContainerGroup request to Alibaba Cloud...")
            create_response = await client.create_container_group_with_options_async(create_request, util_models.RuntimeOptions())
            
            container_group_id = create_response.body.container_group_id
            print(f"[{env.id}] ECI instance created with ID: {container_group_id}. Now waiting for it to become 'Running'...")

            # Poll for the instance to be running and get its public IP
            public_ip = await self._wait_for_instance_running(client, container_group_id, env.region_id, env.id)

            print(f"[{env.id}] Provisioning complete. Public IP: {public_ip}")
            return public_ip, container_group_id

        except Exception as error:
            print(f"[{env.id}] FAILED to create ECI instance. Error: {error}")
            # The 'Recommend' field is very useful for debugging
            if hasattr(error, 'data') and error.data.get("Recommend"):
                print(f"[{env.id}] Aliyun Recommend: {error.data.get('Recommend')}")
            raise error # Re-raise the exception to be caught by the worker
        
    async def create_code_server_instance(
        self, 
        env: EnvironmentPublic, 
        dolphindb_host_ip: str
    ) -> Tuple[str, str]:
        """
        Provisions a standalone code-server instance on ECI, injecting the DolphinDB host IP.

        Args:
            env: The environment schema containing metadata.
            dolphindb_host_ip: The public IP of the already created DolphinDB instance.

        Returns:
            A tuple of (public_ip, container_group_id) for the new code-server instance.
        """
        client = self._create_client(env.region_id)
        print(f"[{env.id}] Starting provisioning for Code-Server instance...")

        # 名字要唯一，可以加上 '-cs' 后缀
        container_group_name = f"codeserver-env-{env.id}"

        # 定义 Code Server 容器
        code_server_container = eci_models.CreateContainerGroupRequestContainer(
            name="code-server-container",
            image=settings.CODE_SERVER_CONTAINER_IMAGE_URL, # 确保已在 config.py 中定义
            cpu=2.0,  # 可以给 code-server 分配更多资源
            memory=4.0,
            port=[eci_models.CreateContainerGroupRequestContainerPort(protocol="TCP", port=env.code_server_port)],

            environment_var=[
                eci_models.CreateContainerGroupRequestContainerEnvironmentVar(
                    key="DDB_HOST",
                    value=dolphindb_host_ip
                )
            ]
        )

        create_request = eci_models.CreateContainerGroupRequest(
            region_id=env.region_id,
            container_group_name=container_group_name,
            security_group_id=settings.ALIYUN_SECURITY_GROUP_ID,
            v_switch_id=settings.ALIYUN_VSWITCH_ID,
            auto_create_eip=True,
            eip_bandwidth=200,
            container=[code_server_container],
            tag=[eci_models.CreateContainerGroupRequestTag(key="owner_id", value=str(env.owner_id))]
        )

        try:
            print(f"[{env.id}] Sending CreateContainerGroup request for Code-Server...")
            create_response = await client.create_container_group_with_options_async(create_request, util_models.RuntimeOptions())
            
            container_group_id = create_response.body.container_group_id
            print(f"[{env.id}] Code-Server ECI created with ID: {container_group_id}. Waiting for 'Running' state...")

            public_ip = await self._wait_for_instance_running(client, container_group_id, env.region_id, env.id)

            print(f"[{env.id}] Code-Server provisioning complete. Public IP: {public_ip}")
            return public_ip, container_group_id

        except Exception as error:
            print(f"[{env.id}] FAILED to create Code-Server ECI instance. Error: {error}")
            if hasattr(error, 'data') and error.data.get("Recommend"):
                print(f"[{env.id}] Aliyun Recommend: {error.data.get('Recommend')}")
            raise error

    async def _wait_for_instance_running(self, client: EciClient, container_group_id: str, region_id: str, env_id: str, timeout_seconds: int = 300) -> str:
        """
        Polls the status of a container group until it is 'Running' or a timeout is reached.
        Returns the public IP address.
        """
        start_time = time.time()
        while time.time() - start_time < timeout_seconds:
            describe_request = eci_models.DescribeContainerGroupsRequest(
                region_id=region_id,
                container_group_ids=str([container_group_id]) # API expects a JSON string array
            )
            
            describe_response = await client.describe_container_groups_with_options_async(describe_request, util_models.RuntimeOptions())
            
            if describe_response.body.container_groups:
                instance = describe_response.body.container_groups[0]
                status = instance.status
                print(f"[{env_id}] Current ECI status: {status}")

                if status == "Running":
                    if not instance.internet_ip:
                        raise Exception("ECI is Running but has no public IP. Check EIP settings.")
                    return instance.internet_ip
                elif status in ["Failed", "Expired"]:
                    raise Exception(f"ECI instance entered a failed state: {status}. Events: {instance.events}")
            
            await asyncio.sleep(10) # Wait 10 seconds between checks

        raise TimeoutError(f"ECI instance {container_group_id} did not become 'Running' within {timeout_seconds} seconds.")
    
    async def describe_instances_batch(self, region_id: str, container_group_ids: List[str]) -> Dict[str, any]:
        """
        Describes a batch of ECI instances and returns a dictionary mapping
        instance_id to the cloud provider's instance object.
        """
        if not container_group_ids:
            return {}
            
        client = self._create_client(region_id)
        describe_request = eci_models.DescribeContainerGroupsRequest(
            region_id=region_id,
            container_group_ids=container_group_ids
        )
        
        try:
            response = await client.describe_container_groups_with_options_async(describe_request, util_models.RuntimeOptions())
            
            # Create a lookup dictionary for easy access
            live_instances = {
                cg.container_group_id: cg 
                for cg in response.body.container_groups
            }
            return live_instances
        except Exception as e:
            print(f"Error describing instances in batch: {e}")
            return {} # On error, return an empty dict


    async def delete_instance(self, container_group_id: str, region_id: str):
        """
        De-provisions a DolphinDB instance by deleting the ECI container group.
        """
        client = self._create_client(region_id)
        print(f"Starting de-provisioning for ECI instance: {container_group_id}...")

        delete_request = eci_models.DeleteContainerGroupRequest(
            region_id=region_id,
            container_group_id=container_group_id
        )

        try:
            await client.delete_container_group_with_options_async(delete_request, util_models.RuntimeOptions())
            print(f"Successfully deleted ECI instance {container_group_id}.")
        except Exception as error:
            # If the instance is already gone, that's okay.
            if "not exist" in str(error):
                print(f"ECI instance {container_group_id} was already deleted.")
                return
            print(f"FAILED to delete ECI instance {container_group_id}. Error: {error}")
            if hasattr(error, 'data') and error.data.get("Recommend"):
                print(f"Aliyun Recommend: {error.data.get('Recommend')}")
            raise error

    async def execute_command(
        self, 
        region_id: str,
        container_group_id: str,
        container_name: str, # 我们需要知道在哪个容器里执行
        command: str,
        timeout_seconds: int = 20
    ) -> Tuple[bool, str]:
        """
        Executes a command in a specified container of an ECI instance.

        Returns:
            A tuple of (success, output_string).
        """
        client = self._create_client(region_id)
        exec_request = eci_models.ExecContainerCommandRequest(
            region_id=region_id,
            container_group_id=container_group_id,
            container_name=container_name,
            command=["/bin/sh", "-c", command] # 使用 sh -c 来执行复杂命令
        )

        try:
            print(f"[{container_group_id}] Requesting exec WebSocket URL for command: {command[:50]}...")
            exec_response = await client.exec_container_command_with_options_async(exec_request, util_models.RuntimeOptions())
            
            websocket_uri = exec_response.body.web_socket_uri
            if not websocket_uri:
                raise Exception("Failed to get a WebSocket URI for exec.")

            print(f"[{container_group_id}] Got WebSocket URI, connecting...")
            
            output_buffer = []

            # 使用 aiohttp 连接到阿里云返回的 WebSocket URI
            async with aiohttp.ClientSession() as session:
                async with session.ws_connect(websocket_uri, timeout=timeout_seconds) as ws:
                    # 连接后，阿里云的 exec stream 是双向的
                    # 我们不需要发送任何 stdin，只需要接收 stdout/stderr
                    # 阿里云的流协议会在每条消息前加一个字节表示流类型 (0x01 for stdout, 0x02 for stderr)
                    
                    async for msg in ws:
                        if msg.type == aiohttp.WSMsgType.BINARY:
                            # 忽略第一个字节（流类型指示符）
                            data = msg.data[1:]
                            output_buffer.append(data.decode('utf-8', errors='ignore'))
                        elif msg.type == aiohttp.WSMsgType.ERROR:
                            raise Exception(f"Exec WebSocket connection error: {ws.exception()}")

            full_output = "".join(output_buffer)
            print(f"[{container_group_id}] Command executed. Output: {full_output[:100]}...")
            # 简单的成功判断：如果输出了 "error", "failed", "not found" 等词，则认为失败
            # 更可靠的方式是让命令自己输出成功/失败标记，例如 `... && echo "SUCCESS"`
            if any(err in full_output.lower() for err in ["error", "failed", "not found", "no such file"]):
                return False, f"Command execution likely failed. Output:\n{full_output}"
            else:
                return True, full_output

        except Exception as e:
            print(f"[{container_group_id}] FAILED to execute command. Error: {e}")
            return False, str(e)

# Create a single instance of the service to be used throughout the application
aliyun_service = AliyunECIService()
===== ./worker.py =====
import asyncio
from datetime import datetime
from arq.connections import RedisSettings, create_pool

from core.config import settings
from db.session import SessionLocal
from db import crud
from services.aliyun_eci import aliyun_service
from schemas import EnvironmentPublic
from arq.cron import cron

async def create_environment_task(ctx, env_id: str):
    """ARQ task to create a cloud environment."""
    db = SessionLocal()
    try:
        await crud.update_environment_status(db, env_id, "PROVISIONING", "Creating cloud resources...")
        
        env_from_db = await crud.get_environment(db, env_id)
        env_schema = EnvironmentPublic.from_orm(env_from_db)

        ddb_public_ip, ddb_container_group_id = await aliyun_service.create_instance(env_schema)
        
        await crud.update_environment_after_provisioning(
            db, env_id, "PROVISIONING", "DolphinDB is ready. Creating Code-Server...",
            public_ip=ddb_public_ip,
            container_group_id=ddb_container_group_id
        )
        
        # --- 阶段 2: 创建 Code-Server 实例 ---
        print(f"[{env_id}] DolphinDB IP is {ddb_public_ip}. Now creating Code-Server instance...")
        
        # 重新从数据库加载环境信息，以防万一
        env_from_db = await crud.get_environment(db, env_id)
        env_schema = EnvironmentPublic.from_orm(env_from_db)

        # 调用新的方法创建 Code-Server，并传入 DDB 的 IP
        cs_public_ip, cs_container_group_id = await aliyun_service.create_code_server_instance(
            env_schema, ddb_public_ip
        )
        
        # --- 最终更新: 保存所有信息，并将状态设为 RUNNING ---
        await crud.update_environment_after_provisioning(
            db, env_id, "RUNNING", "Environment is ready.",
            public_ip=ddb_public_ip,
            container_group_id=ddb_container_group_id,
            code_server_public_ip=cs_public_ip,
            code_server_group_id=cs_container_group_id
        )


    except Exception as e:
        await crud.update_environment_status(db, env_id, "ERROR", str(e))
    finally:
        await db.close()

async def delete_environment_task(ctx, env_id: str):
    """ARQ task to delete a cloud environment."""
    db = SessionLocal()
    try:
        env = await crud.get_environment(db, env_id)
        if env.container_group_id:
            await aliyun_service.delete_instance(env.container_group_id, env.region_id)
            
        # 2. 删除 Code-Server ECI (如果存在)
        if env.code_server_group_id:
            print(f"[{env.id}] Deleting Code-Server instance: {env.code_server_group_id}")
            await aliyun_service.delete_instance(env.code_server_group_id, env.region_id)
        await crud.update_environment_status(db, env_id, "DELETED", "Successfully deleted.")
    except Exception as e:
        await crud.update_environment_status(db, env_id, "ERROR", f"Deletion failed: {e}")
    finally:
        await db.close()

async def cleanup_expired_environments_task(ctx):
    """ARQ cron job to clean up expired environments."""
    
    arq_pool = await create_pool(WorkerSettings.redis_settings)
    db = SessionLocal()
    
    try:
        print("--- CRON JOB: Running cleanup for expired environments... ---") # 添加日志
        expired_envs = await crud.get_expired_environments(db)
        
        if not expired_envs:
            print("--- CRON JOB: No expired environments found. ---")
            return

        print(f"--- CRON JOB: Found {len(expired_envs)} expired environments to clean up. ---")
        for env in expired_envs:
            await crud.update_environment_status(db, env.id, "DELETING", "Environment expired. Cleaning up.")
            

            await arq_pool.enqueue_job("delete_environment_task", env.id)
            print(f"--- CRON JOB: Enqueued deletion task for env_id: {env.id} ---")

    except Exception as e:
        # 添加错误日志
        print(f"--- CRON JOB: An ERROR occurred during cleanup: {e} ---")
    finally:
        await db.close()
        if arq_pool:
            await arq_pool.close()

async def sync_cloud_state_task(ctx):
    """
    Periodically checks our database state against the actual state in Alibaba Cloud
    and marks any missing instances as deleted.
    """
    db = SessionLocal()
    try:
        print("--- CRON JOB: Running cloud state synchronization... ---")
        active_envs = await crud.get_active_environments(db)
        if not active_envs:
            print("--- CRON JOB: No active environments to sync. ---")
            return

        # Batch instance IDs by region for efficient API calls
        ids_by_region = {}
        for env in active_envs:
            if env.region_id not in ids_by_region:
                ids_by_region[env.region_id] = []
            if env.container_group_id:
                 ids_by_region[env.region_id].append(env.container_group_id)

        all_live_instances = {}
        for region_id, group_ids in ids_by_region.items():
            live_in_region = await aliyun_service.describe_instances_batch(region_id, group_ids)
            all_live_instances.update(live_in_region)
            
        # Reconcile: Find ghosts in our DB that are not live in the cloud
        ghost_count = 0
        for env in active_envs:
            if env.container_group_id and env.container_group_id not in all_live_instances:
                ghost_count += 1
                print(f"--- CRON JOB: Found ghost environment {env.id}. Marking as DELETED. ---")
                await crud.update_environment_status(
                    db, env.id, "DELETED", "Instance was deleted from the cloud provider externally."
                )
        
        print(f"--- CRON JOB: Sync complete. Found and marked {ghost_count} ghost environments. ---")

    except Exception as e:
        print(f"--- CRON JOB: An ERROR occurred during state sync: {e} ---")
    finally:
        await db.close()

# ARQ Worker Settings
class WorkerSettings:
    functions = [create_environment_task, delete_environment_task]
    cron_jobs = [
        # 使用 cron() 函数来创建 CronJob 对象
        cron(
            cleanup_expired_environments_task, 
            minute=0,
            run_at_startup=True
        ),
        cron(sync_cloud_state_task,  minute={0,5,10,15,20,25,30,35,40,45,50,55}, run_at_startup=True)
    ]
    redis_settings = RedisSettings(host=settings.REDIS_HOST, port=settings.REDIS_PORT)
===== ./utils/json_utils.py =====
# FILE: ./utils/json_utils.py

import pandas as pd
import numpy as np
from datetime import datetime, date

def custom_json_serializer(obj):
    """
    A custom JSON serializer to handle special data types from
    DolphinDB, Pandas, and Numpy.
    """
    # 1. 处理DolphinDB的Timestamp和相关时间类型
    #    DolphinDB的Timestamp对象通常可以被Pandas识别为pd.Timestamp
    if isinstance(obj, (datetime, date, pd.Timestamp)):
        # 将所有时间相关的对象统一转换为ISO 8601格式的字符串
        # 这是Web API中最标准的时间表示方式
        return obj.isoformat()

    # 2. 处理Numpy的特殊数值类型 (在Pandas中很常见)
    if isinstance(obj, (np.integer, np.floating, np.bool_)):
        # 将Numpy的数字类型转换为Python原生的int/float/bool
        return obj.item()

    # 3. 处理Numpy的数组 (如果需要的话)
    if isinstance(obj, np.ndarray):
        return obj.tolist()

    # 如果遇到其他不认识的类型，抛出原始错误
    raise TypeError(f"Object of type {type(obj).__name__} is not JSON serializable")
===== ./utils/__init__.py =====

===== ./utils/json_parser.py =====
import datetime
import json
import os

def parse_json_string(json_str):
    # 去掉 JSON 字符串的 ```json 和 ``` 标记部分
    json_str  = json_str.lstrip().rstrip()  # 去掉首尾空白字符
    if json_str.startswith('```json'):
        json_str = json_str[7:]  # 去掉开头的 ```json
    if json_str.endswith('```'):
        json_str = json_str[:-3]  # 去掉结尾的 ```

    # 解析 JSON 字符串
    try:
        import re
        cleaned_json_str = re.sub(r'[\x00-\x1F\x7F]', '', json_str)

        data = json.loads(cleaned_json_str)
        return data
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
        error_time = datetime.datetime.now()
        print(f"Error decoding JSON at {error_time.isoformat()}: {e}")

        try:
            error_log_dir = os.path.join(os.path.expanduser("~"), ".ddb_agent", "error_logs")
            # 确保错误日志目录存在
            os.makedirs(error_log_dir, exist_ok=True)

            # 创建一个带时间戳的唯一文件名
            # 格式: error_YYYYMMDD_HHMMSS_microseconds.log
            filename = f"error_{error_time.strftime('%Y%m%d_%H%M%S_%f')}.log"
            filepath = os.path.join(error_log_dir, filename)

            # 将详细信息写入文件
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("--- JSON Decode Error Log ---\n\n")
                f.write(f"Timestamp: {error_time.isoformat()}\n")
                f.write(f"Error Message: {e}\n")
                f.write("\n--- Problematic String (after cleaning markdown) ---\n")
                f.write(cleaned_json_str)

            print(f"The problematic string and error details have been saved to: {filepath}")

        except Exception as log_e:
            # 如果连写入日志文件都失败了，打印一个严重的警告
            print(f"CRITICAL: Failed to write error log file to '{error_log_dir}'. Error: {log_e}")
===== ./utils/tokenizer.py =====
# file: utils/tokenizer.py

import re
from typing import Set

# 尝试导入 jieba，如果失败则给出提示
try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    print("Warning: `jieba` library not found. Chinese tokenization will be suboptimal. "
          "Please install it with `pip install jieba`.")

def is_contains_chinese(text: str) -> bool:
    """
    Checks if a string contains any Chinese characters.
    """
    # \u4e00-\u9fa5 是中文字符的Unicode范围
    return bool(re.search(r'[\u4e00-\u9fa5]', text))

def smart_tokenize(text: str) -> Set[str]:
    """
    Tokenizes text intelligently based on its content.
    - Uses jieba for text containing Chinese characters.
    - Uses regex for English-only text.
    Returns a set of lowercased tokens.
    """
    text_lower = text.lower()
    
    if JIEBA_AVAILABLE and is_contains_chinese(text):
        # 使用 jieba 进行中文分词 (搜索引擎模式)
        # cut_for_search 会切分出更细粒度的词，适合搜索
        tokens = jieba.cut_for_search(text_lower)
        # 过滤掉单个字符和停用词（可选，但推荐）
        # 这里简单过滤掉长度为1的词
        return {token for token in tokens if len(token.strip()) > 1}
    else:
        # 对纯英文使用正则表达式
        return set(re.findall(r'\w+', text_lower))
===== ./utils/text_extractor.py =====
from abc import ABC, abstractmethod
import os
from typing import Optional

# --- 抽象基类 ---
class BaseTextExtractor(ABC):
    """
    Abstract base class for all text extraction strategies.
    """
    @abstractmethod
    def extract(self, file_path: str) -> Optional[str]:
        """
        Extracts plain text content from a given file.

        Args:
            file_path: The path to the file.

        Returns:
            The extracted text content as a string, or None if extraction fails.
        """
        pass

# --- 具体实现子类 ---

class PlainTextExtractor(BaseTextExtractor):
    """Extracts text from plain text files (.txt, .md, .py, .dos, etc.)."""
    def extract(self, file_path: str) -> Optional[str]:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"Error reading plain text file {file_path}: {e}")
            return None

class PDFExtractor(BaseTextExtractor):
    """Extracts text from PDF files using the PyMuPDF library."""
    def __init__(self):
        try:
            import fitz  # PyMuPDF
            self.fitz = fitz
        except ImportError:
            raise ImportError(
                "PyMuPDF is not installed. Please install it with `pip install PyMuPDF` to support PDF files."
            )

    def extract(self, file_path: str) -> Optional[str]:
        try:
            text_content = []
            with self.fitz.open(file_path) as doc:
                for page_num, page in enumerate(doc):
                    # 添加页码分隔符，为上下文提供更多信息
                    text_content.append(f"\n--- Page {page_num + 1} ---\n")
                    text_content.append(page.get_text())
            return "".join(text_content)
        except Exception as e:
            print(f"Error extracting text from PDF {file_path}: {e}")
            return None

class DOCXExtractor(BaseTextExtractor):
    """Extracts text from DOCX files using the python-docx library."""
    def __init__(self):
        try:
            import docx
            self.docx = docx
        except ImportError:
            raise ImportError(
                "python-docx is not installed. Please install it with `pip install python-docx` to support DOCX files."
            )

    def extract(self, file_path: str) -> Optional[str]:
        try:
            doc = self.docx.Document(file_path)
            full_text = [para.text for para in doc.paragraphs]
            return '\n'.join(full_text)
        except Exception as e:
            print(f"Error extracting text from DOCX {file_path}: {e}")
            return None

# --- 工厂函数 ---

# 缓存已创建的提取器实例，避免重复初始化
_EXTRACTOR_CACHE = {}

def get_extractor(file_path: str) -> Optional[BaseTextExtractor]:
    """
    Factory function that returns the appropriate text extractor based on the file extension.
    """
    _, extension = os.path.splitext(file_path)
    extension = extension.lower()

    # 首先检查缓存
    if extension in _EXTRACTOR_CACHE:
        return _EXTRACTOR_CACHE[extension]

    extractor: Optional[BaseTextExtractor] = None
    if extension in ['.txt', '.md', '.markdown', '.py', '.js', '.ts', '.html', '.css', '.dos', '.json', '.xml']:
        extractor = PlainTextExtractor()
    elif extension == '.pdf':
        try:
            extractor = PDFExtractor()
        except ImportError as e:
            print(e) # 打印安装提示
    elif extension == '.docx':
        try:
            extractor = DOCXExtractor()
        except ImportError as e:
            print(e)
    # 未来可以轻松扩展
    # elif extension in ['.ppt', '.pptx']:
    #     extractor = PPTExtractor()
    else:
        # 对于未知类型，可以默认使用纯文本方式尝试，或者直接返回None
        print(f"Warning: No specific extractor for '{extension}'. Falling back to PlainTextExtractor.")
        extractor = PlainTextExtractor()

    # 存入缓存
    if extractor:
        _EXTRACTOR_CACHE[extension] = extractor
    
    return extractor

# --- 统一的调用接口 ---

def extract_text_from_file(file_path: str) -> Optional[str]:
    """
    A single entry point to extract text from any supported file type.
    """
    if not os.path.exists(file_path):
        print(f"Error: File not found at {file_path}")
        return None
        
    extractor = get_extractor(file_path)
    if extractor:
        return extractor.extract(file_path)
    
    print(f"No suitable extractor found for file: {file_path}")
    return None
===== ./llm/llm_client.py =====
from dataclasses import dataclass
import json
from openai import OpenAI
from typing import Generator, List, Dict, Any, Literal, Optional, Union
import os
from loguru import logger

@dataclass
class StreamChunk:
    type: Literal["reasoning", "content", "metadata", "error"]
    data: any

@dataclass 
class LLMResponse:
    """通用LLM响应结果容器"""
    success: bool
    content: str = ""  # 原始响应内容
    reasoning_content: str = ""  # 推理内容
    error_message: str = ""
    error_type: str = ""
    metadata: Dict[str, Any] = None  # 可能的元数据

class LLMClient:
    """LLM客户端，处理与OpenAI API的交互"""
    
    def __init__(self, api_key: str, base_url: str, logger=None):
        """初始化LLM客户端
        
        Args:
            api_key: API密钥
            base_url: API基础URL
            logger: 日志记录器
        """
        if not api_key:
            raise ValueError("API key must be provided.")
        if not base_url:
            raise ValueError("Base URL must be provided.")
        
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.logger = logger

    def _log_request(self, conversation_history: List[Dict[str, str]], model: str):
        """Helper method to log the request payload."""
        try:
            # 使用 .bind(llm_request=True) 来标记这条日志
            # 这样我们的文件处理器就能通过 filter 捕获它
            request_logger = logger.bind(llm_request=True)
            
            # 格式化日志内容
            log_content = {
                "model": model,
                "messages": conversation_history
            }
            # 使用 pretty-printed JSON 格式，便于阅读
            request_logger.debug(f"\n{json.dumps(log_content, indent=2, ensure_ascii=False)}")
        except Exception as e:
            logger.warning(f"Failed to log LLM request: {e}")
            
    def generate_response(
        self, 
        conversation_history: List[Dict[str, str]],
        model: Optional[str] = None,
        log_requests: bool = False
    ) ->  Generator[StreamChunk, None, LLMResponse]:
        """从LLM获取响应
        
        Args:
            conversation_history: 对话历史
            
        Returns:
            LLMResponse: llm原始返回
        """
        try:
            target_model = model or os.getenv("LLM_MODEL")
            if not target_model:
                raise ValueError("No model specified and LLM_MODEL environment variable is not set.")
            
            if log_requests:
                self._log_request(conversation_history, target_model)
            
            stream = self.client.chat.completions.create(
                model=target_model,
                messages=conversation_history,
                max_completion_tokens=8000,
                stream=True
            )

            if self.logger:
                self.logger.info("Thinking...")
            
            reasoning_started = False
            reasoning_content = ""
            final_content = ""

            for chunk in stream:
                delta = chunk.choices[0].delta

                if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                    if self.logger and not reasoning_started:
                        self.logger.info("Reasoning:")
                        reasoning_started = True
                    reasoning_content += delta.reasoning_content
                    yield StreamChunk(type="reasoning", data=delta.reasoning_content)
                elif hasattr(delta, 'content') and delta.content:
                    final_content += delta.content
                    yield StreamChunk(type="content", data=delta.content)

            if self.logger:
                self.logger.info(f"Assistant> {final_content}")

            return LLMResponse(
                    success=True,
                    content=final_content,
                    reasoning_content=reasoning_content,
                    metadata={"model": target_model}
                )
            

        except Exception as e:
            error_msg = f"Model API error: {str(e)}"
            if self.logger:
                self.logger.error(error_msg)
            return LLMResponse(success=False,error_message=error_msg,error_type=type(e).__name__)
            
        
class LLMClientManager:
    """
    管理和缓存多个LLMClient实例。
    """
    _clients: Dict[str, LLMClient] = {}

    @classmethod
    def get_client(cls, api_key: Optional[str] = None, base_url: Optional[str] = None, logger=None) -> LLMClient:
        """
        获取一个LLMClient实例。如果已存在相同配置的实例，则从缓存返回。
        
        Args:
            api_key: API密钥。如果为None，则从环境变量 LLM_API_KEY 获取。
            base_url: API基础URL。如果为None，则从环境变量 LLM_BASE_URL 获取。
            logger: 日志记录器。
            
        Returns:
            LLMClient实例。
        """
        # 确定最终的配置
        final_api_key = api_key or os.getenv("LLM_API_KEY")
        final_base_url = base_url or os.getenv("LLM_BASE_URL")

        if not final_api_key or not final_base_url:
            raise ValueError("API key and Base URL must be provided either as arguments or environment variables.")

        # 使用base_url作为缓存的key，通常一个base_url对应一个服务商
        cache_key = final_base_url

        if cache_key not in cls._clients:
            print(f"Creating new LLMClient for: {final_base_url}")
            cls._clients[cache_key] = LLMClient(
                api_key=final_api_key,
                base_url=final_base_url,
                logger=logger
            )
        
        return cls._clients[cache_key]
===== ./llm/llm_prompt.py =====
import functools
import inspect
from typing import Any, Callable, Dict, Generator, Optional, Type, TypeVar, List
from jinja2 import Environment, BaseLoader

from context.context_manager import ContextManager
from llm.models import ModelManager
from .llm_client import LLMClientManager, LLMResponse, StreamChunk
from dotenv import load_dotenv
import datetime


load_dotenv()

T = TypeVar('T')

CONVERSATION_HISTORY_PARAM = "conversation_history"

def normalize_history_for_llm(history: List[Dict[str, Any]]) -> List[Dict[str, str]]:
        """
        Normalizes a conversation history by converting any multi-part 'content' lists
        into a single string. This makes the history safe for caching, hashing, and
        passing to LLM APIs that expect simple string content.

        Args:
            history: The conversation history, which may contain complex content.

        Returns:
            A new history list where all 'content' fields are guaranteed to be strings.
        """
        normalized = []
        for msg in history:
            content = msg.get("content")
            if isinstance(content, list):
                # This is our new multi-part format. Flatten it.
                # We join the 'text' fields of all parts.
                string_content = "\n".join(
                    part.get("text", "") for part in content if isinstance(part, dict)
                )
                normalized.append({"role": msg["role"], "content": string_content})
            elif content is not None:
                # It's already a string or something else convertible to string.
                normalized.append({"role": msg["role"], "content": str(content)})
            else:
                # Handle cases where content might be null (e.g., tool calls in some APIs)
                normalized.append({"role": msg["role"], "content": ""})
                
        return normalized

class PromptDecorator:
    """
    一个类似于 @llm.prompt() 的装饰器，用于管理LLM提示模板
    """
    def __init__(self, 
                 model: Optional[str] = None,
                 api_key: Optional[str] = None,
                 base_url: Optional[str] = None,
                 response_model: Optional[Type] = None, 
                 log_requests: Optional[bool] = None,
                 **kwargs):
        """
        初始化装饰器
        
        Args:
            response_model: 响应的数据模型类型
            stream: 是否启用流式响应
            **kwargs: 其他配置参数
        """
        self.model_name_alias = model
        self.override_api_key = api_key
        self.override_base_url = base_url
        self.response_model = response_model
        self.override_log_requests = log_requests
        self.kwargs = kwargs
        self.jinja_env = Environment(loader=BaseLoader())

        def get_current_time():
            return datetime.datetime.now().strftime("%-m/%-d/%Y, %-I:%M:%S %p (Asia/Shanghai, UTC+8:00)")

        # 2. 将这个函数注册为 Jinja2 环境的全局变量
        self.jinja_env.globals['now'] = get_current_time
        
    def __call__(self, func: Callable[..., Dict[str, Any]]) -> Callable:
        """
        应用装饰器到函数
        
        Args:
            func: 被装饰的函数
        
        Returns:
            包装后的函数
        """
        # 获取函数的文档字符串作为模板
        docstring = inspect.getdoc(func)
        if not docstring:
            raise ValueError(f"函数 {func.__name__} 缺少文档字符串作为提示模板")
        
        # 预编译模板
        template = self.jinja_env.from_string(docstring)

         # 提取模板中的变量
        template_variables = self._extract_variables(docstring)
        
        # 获取函数签名
        sig = inspect.signature(func)
        
        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            system_prompt_content = None
            user_prompt_template_str = docstring
            
            model_config = None
            if self.model_name_alias:
                model_config = ModelManager.get_model_config(self.model_name_alias)
                if not model_config:
                    raise ValueError(f"Model configuration '{self.model_name_alias}' not found in models.json.")

            # 决定最终的API配置
            # 优先级: 装饰器直接覆盖 > 配置文件 > 环境变量 (由LLMClientManager处理)
            final_api_key = self.override_api_key or (model_config.get_api_key() if model_config else None)
            final_base_url = self.override_base_url or (model_config.base_url if model_config else None)
            final_model_name = model_config.model_name if model_config else None # 这是要传给API的真正model name

            max_window = getattr(model_config, 'max_window_size', 50000)
            
            # 1. 绑定所有参数，包括默认值
            bound_args = sig.bind(*args, **kwargs)
            bound_args.apply_defaults()
            all_params = bound_args.arguments.copy()

            # 2. 提取对话历史 (如果存在)
            conversation_history: List[Dict[str, str]] = []
            if CONVERSATION_HISTORY_PARAM in all_params:
                history = all_params.pop(CONVERSATION_HISTORY_PARAM) # 从参数中移除，避免被用于模板填充
                if history and isinstance(history, list):
                    conversation_history.extend(history)

            # 3. 调用原函数，获取其返回的上下文
            func_result = func(*args, **kwargs)
            
            # 4. 准备模板变量
            template_vars = {}

            if isinstance(func_result, tuple) and len(func_result) == 2:
                system_prompt_content, user_prompt_context = func_result
                if isinstance(user_prompt_context, dict):
                    template_vars.update(user_prompt_context)
                # The user prompt is now just the docstring, which will be the last user message
                user_prompt_template_str = docstring
            # 如果函数返回字典，则使用其填充模板变量
            elif isinstance(func_result, dict):
                # Legacy behavior: no system prompt, docstring is the user prompt
                template_vars.update(func_result)

            # 检查是否还有未填充的模板变量
            missing_vars = [var for var in template_variables if var not in template_vars]
            
            if missing_vars:
                # 尝试从函数参数中获取变量
                # 首先绑定参数
                bound_args = sig.bind(*args, **kwargs)
                bound_args.apply_defaults()
                
                # 将参数添加到模板变量中
                for var in missing_vars:
                    if var in bound_args.arguments:
                        template_vars[var] = bound_args.arguments[var]
            
            # 检查是否仍有未填充的变量
            missing_vars = [var for var in template_variables if var not in template_vars]
            if missing_vars:
                raise ValueError(f"无法填充模板变量: {', '.join(missing_vars)}")
            
            # 渲染模板
            rendered_user_prompt = template.render(**template_vars)

            llm_messages = []
            if system_prompt_content:
                llm_messages.append({"role": "system", "content": system_prompt_content})
            
            llm_messages.extend(conversation_history)
            
            # The rendered template is now always the last user message
            llm_messages.append({"role": "user", "content": rendered_user_prompt})

            context_manager = ContextManager(
                model_name=final_model_name, 
                max_window_size=max_window
            )

            pruned_messages = context_manager.prune(llm_messages)
            
            # --- 决定最终的日志开关状态 ---
            # 优先级: 装饰器直接覆盖 > 配置文件 > 默认Fals
            final_log_requests = self.override_log_requests
            if final_log_requests is None and model_config:
                final_log_requests = model_config.log_requests
            final_log_requests = final_log_requests or True
            
            # 调用LLM API
            llm_result = self._call_llm_api(
                messages = pruned_messages, 
                model = final_model_name,
                api_key = final_api_key,
                base_url = final_base_url,
                log_requests = final_log_requests
            )
            
            return llm_result
        
        # 将原始模板和其他元数据附加到包装函数
        wrapper.prompt_template = docstring
        wrapper.response_model = self.response_model
        wrapper.template_variables = template_variables
        
        # 添加调试辅助方法
        def example_input():
            """返回使用示例输入的模板渲染结果"""
            example_vars = {k: f"example_{k}" for k in template_variables}
            return template.render(**example_vars)
        
        wrapper.example_input = example_input
        
        return wrapper
    
    def _call_llm_api(self, messages: List[Dict[str, str]], model: Optional[str] = None, api_key: Optional[str] = None, base_url: Optional[str] = None,  log_requests: bool = False) -> Generator[StreamChunk, None, LLMResponse]:
        """
        调用LLM API (这里是一个模拟实现)
        
        Args:
            prompt: 渲染后的提示文本
            
        Returns:
            LLM的响应文本
        """
        llm_client = LLMClientManager.get_client(api_key=api_key, base_url=base_url)

        response = llm_client.generate_response(
            conversation_history=messages,
            model=model,
            log_requests=log_requests
        )
        
        return response

    def _extract_variables(self, template_text: str) -> list:
        """
        从模板中提取变量名
        
        Args:
            template_text: 模板文本
            
        Returns:
            变量名列表
        """
        # 简单实现，实际应用可能需要更复杂的解析
        import re
        pattern = r"{{\s*(\w+)\s*}}"
        return re.findall(pattern, template_text)


class LLM:
    def prompt(self, *args, **kwargs):
        return PromptDecorator(*args, **kwargs)

llm = LLM()

===== ./llm/models.py =====
import os
import json
from typing import List, Dict, Optional, Any
from pydantic import BaseModel, Field

class ModelConfig(BaseModel):
    """
    Represents the configuration for a single LLM.
    """
    name: str = Field(description="A unique, user-friendly name for this configuration.")
    model_name: str = Field(description="The actual model name to be passed to the API.")
    base_url: str = Field(description="The base URL of the API provider.")
    api_key: Optional[str] = Field(None, description="Direct API key (less secure).")
    api_key_env_var: Optional[str] = Field(None, description="Environment variable name for the API key (recommended).")
    model_type: Optional[str] = Field(None, description="Type of the model provider (e.g., 'openai', 'deepseek').")
    description: Optional[str] = Field(None, description="A brief description of the model.")
    max_context_tokens: Optional[int] = Field(None, description="Maximum number of tokens the model can handle in a single request. If not set, defaults to 50000.")
    log_requests: Optional[bool] = Field(False, description="Whether to log requests made to this model. Defaults to False.")

    def get_api_key(self) -> str:
        """
        Retrieves the API key, prioritizing the direct key, then the environment variable.
        """
        if self.api_key:
            return self.api_key
        if self.api_key_env_var:
            key = os.getenv(self.api_key_env_var)
            if key:
                return key
        # 如果都没有，则返回空字符串，让上层处理错误
        return ""

class ModelManager:
    """
    Loads and manages all available model configurations from a JSON file.
    """
    _models: Dict[str, ModelConfig] = {}
    _is_loaded = False

    @classmethod
    def load_models(cls, file_path: str = "models.json"):
        """
        Loads model configurations from a JSON file.
        """
        if not os.path.exists(file_path):
            print(f"Warning: Model configuration file not found at '{file_path}'.")
            cls._is_loaded = True
            return

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                models_data = json.load(f)
            
            for config_data in models_data:
                model_config = ModelConfig(**config_data)
                if model_config.name in cls._models:
                    print(f"Warning: Duplicate model name '{model_config.name}' found. Overwriting.")
                cls._models[model_config.name] = model_config
            
            cls._is_loaded = True
            print(f"Successfully loaded {len(cls._models)} model configurations from '{file_path}'.")
        except (json.JSONDecodeError, Exception) as e:
            raise IOError(f"Failed to load or parse model configuration file '{file_path}': {e}")

    @classmethod
    def get_model_config(cls, name: str) -> Optional[ModelConfig]:
        """
        Retrieves a model configuration by its unique name.
        """
        if not cls._is_loaded:
            cls.load_models() # 自动加载
        
        return cls._models.get(name)

# 可以在模块加载时自动加载一次
ModelManager.load_models()
===== ./llm/__init__.py =====

===== ./context/budget.py =====
from typing import Dict, Literal

# CONTEXT_WEIGHTS 现在只定义 history 和 file_context 的相对权重
CONTEXT_WEIGHTS = {
    "default": {
        "conversation_history": 0.40,
        "file_context": 0.60,
    },
    "coding": {
        "conversation_history": 0.25,
        "file_context": 0.75, # 编码时，文件上下文权重更高
    },
    "chat": {
        "conversation_history": 0.70, # 聊天时，对话历史权重更高
        "file_context": 0.30,
    }
}

class ContextBudget:
    """
    Calculates the token budget for conversation history and file context,
    after reserving space for a fixed system prompt.
    """
    def __init__(
        self, 
        total_safe_zone: int, 
        system_prompt_tokens: int,
        task_type: Literal['default', 'coding', 'chat'] = 'default'
    ):
        if task_type not in CONTEXT_WEIGHTS:
            raise ValueError(f"Unknown task type: {task_type}")

        # 1. 首先为 system_prompt 预留空间
        self.system_prompt_tokens = system_prompt_tokens
        
        if self.system_prompt_tokens >= total_safe_zone:
            raise ValueError(
                f"System prompt alone ({self.system_prompt_tokens} tokens) "
                f"exceeds or equals the total safe zone ({total_safe_zone} tokens)."
            )

        # 2. 计算剩余的可用预算
        remaining_budget = total_safe_zone - self.system_prompt_tokens
        
        # 3. 根据权重分配剩余预算
        self.weights = CONTEXT_WEIGHTS[task_type]
        self.history_budget = int(remaining_budget * self.weights["conversation_history"])
        self.file_context_budget = int(remaining_budget * self.weights["file_context"])

        # 4. 调整以确保总和精确
        self._adjust_budgets(remaining_budget)
        
    def _adjust_budgets(self, remaining_budget: int):
        """Adjusts budgets to ensure they sum up to the remaining budget."""
        current_sum = self.history_budget + self.file_context_budget
        diff = remaining_budget - current_sum
        # 将误差加到权重最大的部分上
        if self.weights["file_context"] > self.weights["conversation_history"]:
            self.file_context_budget += diff
        else:
            self.history_budget += diff
===== ./context/code_extractor_pruner.py =====
# file: ddb_agent/context/code_extractor_pruner.py

import json
from typing import List, Dict, Any, Tuple
from llm.llm_prompt import llm  # 假设您使用之前设计的llm.prompt
from token_counter import count_tokens # 引入我们之前创建的token计数器

class Document:
    """A simple container for source code or md doc data."""
    def __init__(self, file_path: str, source_code: str, tokens: int = -1):
        self.file_path = file_path
        self.source_code = source_code
        self.tokens = tokens if tokens != -1 else count_tokens(source_code)

class CodeExtractorPruner:
    """
    Implements the 'extract' context pruning strategy.
    It extracts relevant code snippets from large files based on conversation history.
    """
    def __init__(self, max_tokens: int, llm_model_name: str = "deepseek-default"):
        self.max_tokens = max_tokens
        self.llm_model_name = llm_model_name
        # 设置一个阈值，小于此阈值的文件将被完整保留，以提高效率
        self.full_file_threshold = int(max_tokens * 0.8)

    @llm.prompt(response_model=List[Dict[str, int]])
    def _extract_snippets_prompt(self, conversations: List[Dict[str, str]], content_with_lines: str) -> dict:
        """
        Based on the provided code file and conversation history, extract relevant code snippets.

        The code file content is provided below with line numbers.
        <CODE_FILE>
        {{ content_with_lines }}
        </CODE_FILE>

        Here is the conversation history leading to the current task.
        <CONVERSATION_HISTORY>
        {% for msg in conversations %}
        <{{ msg.role }}>: {{ msg.content }}
        {% endfor %}
        </CONVERSATION_HISTORY>

        Your Task:
        1. Analyze the last user request in the conversation history.
        2. Identify one or more important code sections in the code file that are relevant to this request.
        3. For each relevant section, determine its start and end line numbers.
        4. You can return up to 4 snippets.

        Output Requirements:
        - Return a JSON array of objects, where each object contains "start_line" and "end_line".
        - Line numbers must be integers and correspond to the numbers in the provided code file.
        - If no code sections are relevant, return an empty array [].
        - Your response MUST be a valid JSON array and nothing else.

        Example output:
        ```json
        [
            {"start_line": 10, "end_line": 25},
            {"start_line": 88, "end_line": 95}
        ]
        ```
        """
        # 这个函数将使用 llm.prompt 装饰器，自动填充模板
        return {
            "model": self.llm_model_name,
            # conversations 和 content_with_lines 会从函数参数中获取
        }
    
    def _merge_overlapping_snippets(self, snippets: List[Dict[str, int]]) -> List[Dict[str, int]]:
        """Merges overlapping or adjacent line number ranges."""
        if not snippets:
            return []

        # 按起始行排序
        sorted_snippets = sorted(snippets, key=lambda x: x["start_line"])

        merged = [sorted_snippets[0]]
        for current in sorted_snippets[1:]:
            last = merged[-1]
            # 如果当前区间的开始在前一个区间的结束行+1的范围内，则合并
            if current["start_line"] <= last["end_line"] + 1:
                last["end_line"] = max(last["end_line"], current["end_line"])
            else:
                merged.append(current)
        return merged

    def _build_snippet_content(self, original_code: str, snippets: List[Dict[str, int]]) -> str:
        """Constructs the final content string from the extracted snippets."""
        lines = original_code.splitlines()
        content_parts = ["# Snippets from the original file:\n"]
        
        for snippet in snippets:
            start = max(0, snippet["start_line"] - 1)
            end = min(len(lines), snippet["end_line"])
            content_parts.append(f"\n# ... (lines {start + 1}-{end}) ...\n")
            content_parts.extend(lines[start:end])
        
        return "\n".join(content_parts)

    def prune(self, file_sources: List[Document], conversations: List[Dict[str, str]]) -> List[Document]:
        """
        Prunes the context by extracting relevant snippets from large files.
        """
        print("Starting 'extract' pruning strategy...")
        selected_files: List[Document] = []
        total_tokens = 0
        
        for file_source in file_sources:
            if total_tokens + file_source.tokens <= self.full_file_threshold:
                # 1. 完整保留小文件
                selected_files.append(file_source)
                total_tokens += file_source.tokens
                print(f"✅ Kept file completely: {file_source.file_path} ({file_source.tokens} tokens)")
                continue

            if total_tokens >= self.max_tokens:
                print("Token limit reached. Stopping further processing.")
                break

            # 2. 对大文件进行片段抽取
            print(f"🔍 Processing large file for snippets: {file_source.file_path} ({file_source.tokens} tokens)")
            
            try:
                # 为文件内容添加行号
                lines_with_numbers = "\n".join(
                    f"{i+1} {line}" for i, line in enumerate(file_source.source_code.splitlines())
                )
                
                # 调用 LLM 抽取片段
                # 注意：这里我们假设单个文件添加行号后不会超过模型窗口，
                # 如果会，则需要引入 `_split_content_with_sliding_window` 逻辑
                raw_snippets = self._extract_snippets_prompt(
                    conversations=conversations,
                    content_with_lines=lines_with_numbers
                )
                
                if not raw_snippets:
                    print(f"  - No relevant snippets found in {file_source.file_path}.")
                    continue
                
                # 合并重叠片段
                merged_snippets = self._merge_overlapping_snippets(raw_snippets)
                
                # 构建新内容并计算token
                new_content = self._build_snippet_content(file_source.source_code, merged_snippets)
                new_tokens = count_tokens(new_content, model_name=self.llm_model_name)
                
                if total_tokens + new_tokens <= self.max_tokens:
                    selected_files.append(Document(
                        file_path=file_source.file_path,
                        source_code=new_content,
                        tokens=new_tokens
                    ))
                    total_tokens += new_tokens
                    print(f"  - Extracted snippets from {file_source.file_path}. "
                          f"Original: {file_source.tokens} tokens -> New: {new_tokens} tokens.")
                else:
                    print(f"  - Snippets from {file_source.file_path} are too large to fit. Skipping.")
                    break # 如果添加片段后超限，则停止处理后续文件

            except Exception as e:
                print(f"Error processing snippets for {file_source.file_path}: {e}")
                continue # 出错则跳过此文件

        print(f"Pruning complete. Final context has {len(selected_files)} files with {total_tokens} tokens.")
        return selected_files
===== ./context/context_builder.py =====
# file: ddb_agent/context/context_builder.py (重构后)

from typing import List, Dict, Any, Literal

from .pruner import get_pruner, Document
from .budget import ContextBudget
from token_counter import count_tokens

class ContextBuilder:
    """
    Orchestrates building the LLM context with dynamic budget allocation
    and a reserved space for the system prompt.
    """
    def __init__(self, model_name: str, max_window_size: int):
        self.model_name = model_name
        self.max_window_size = max_window_size
        self.safe_zone = int(max_window_size * 0.9)

    def build(
        self,
        system_prompt: str,
        conversations: List[Dict[str, Any]],
        file_sources: List[Document],
        task_type: Literal['default', 'coding', 'chat'] = 'default',
        file_pruning_strategy: str = 'extract'
    ) -> List[Dict[str, Any]]:
        
        # 1. 首先计算不可动摇的 system_prompt 的 token 数
        system_prompt_tokens = count_tokens(system_prompt, self.model_name)

        # 2. 基于 system_prompt 的开销，创建预算分配器
        try:
            budget = ContextBudget(
                total_safe_zone=self.safe_zone,
                system_prompt_tokens=system_prompt_tokens,
                task_type=task_type
            )
        except ValueError as e:
            # 如果 system_prompt 本身就超限了，这是一个严重错误
            print(f"Error: {e}")
            # 我们可以选择返回一个只包含截断后的系统提示的最小化上下文
            return [{"role": "system", "content": self._prune_system_prompt(system_prompt, self.safe_zone)}]

        # 3. 剪枝对话历史 (使用分配好的预算)
        pruned_conversations = self._prune_conversation_history(conversations, budget.history_budget)
        
        # 4. 剪枝文件上下文 (使用分配好的预算)
        file_pruner = get_pruner(
            strategy=file_pruning_strategy, 
            max_tokens=budget.file_context_budget,
            llm_model_name=self.model_name
        )
        pruned_file_sources = file_pruner.prune(file_sources, conversations)

        # 5. 组合最终上下文
        final_messages = [{"role": "system", "content": system_prompt}]
        
        if pruned_file_sources:
            file_context_str = "\n---\n".join(
                f"File: {f.file_path}\n\n{f.source_code}" for f in pruned_file_sources
            )
            final_messages.append({"role": "assistant", "content": f"<CONTEXT_FILES>\n{file_context_str}\n</CONTEXT_FILES>"})
           
        final_messages.extend(pruned_conversations)

        return final_messages

    def _prune_system_prompt(self, prompt: str, budget: int) -> str:
        # 这个方法现在主要用于极端情况下的报错和截断
        if count_tokens(prompt, self.model_name) > budget:
            print(f"CRITICAL WARNING: System prompt is too long ({count_tokens(prompt, self.model_name)} tokens) "
                  f"and exceeds the total budget ({budget} tokens). It will be severely truncated.")
            avg_chars_per_token = len(prompt) / count_tokens(prompt, self.model_name) if count_tokens(prompt, self.model_name) > 0 else 4
            safe_chars = int(budget * avg_chars_per_token * 0.95)
            return prompt[:safe_chars]
        return prompt

    def _prune_conversation_history(self, conversations: List[Dict[str, Any]], budget: int) -> List[Dict[str, Any]]:
        """Prunes conversation history using a sliding window approach."""
        pruned_history = []
        current_tokens = 0
        
        # 从最新（末尾）的对话开始保留
        for msg in reversed(conversations):
            msg_tokens = count_tokens(msg.get('content', ''), self.model_name)
            if current_tokens + msg_tokens <= budget:
                pruned_history.insert(0, msg)
                current_tokens += msg_tokens
            else:
                break
        
        return pruned_history
===== ./context/context_manager.py =====
# file: ddb_agent/context/context_manager.py (处理超长单条消息)

from typing import List, Dict, Any
from token_counter import count_tokens

class ContextManager:
    """
    Manages the context window for LLM calls by dynamically pruning content,
    including handling single oversized messages.
    """
    def __init__(self, model_name: str, max_window_size: int = 50000):
        self.model_name = model_name
        self.max_window_size = max_window_size
        self.safe_zone_size = int(max_window_size * 0.9)

    def _truncate_single_message(self, message: Dict[str, str]) -> Dict[str, str]:
        """
        Truncates a single message if it exceeds the safe zone size.
        A warning is added to the content indicating it has been truncated.
        """
        content = message.get('content', '')
        message_tokens = count_tokens(content, model_name=self.model_name)

        if message_tokens > self.safe_zone_size:
            print(f"Warning: A single message (role: {message['role']}) with {message_tokens} tokens "
                  f"exceeds the safe zone of {self.safe_zone_size}. It will be truncated.")
            
            # --- 截断策略 ---
            # 更智能的策略可能是保留开头和结尾，但这里先实现一个简单的从头截断
            # 我们需要估算能保留多少文本
            avg_chars_per_token = len(content) / message_tokens if message_tokens > 0 else 4
            safe_char_count = int(self.safe_zone_size * avg_chars_per_token * 0.95) # 再留5%余量

            truncated_content = content[:safe_char_count]
            
            # 在被截断的内容末尾添加一个警告
            truncation_warning = "\n\n[---SYSTEM WARNING: This content has been truncated due to context window limitations.---]"
            
            # 创建一个新的消息字典，而不是修改原始的
            return {
                'role': message['role'],
                'content': truncated_content + truncation_warning
            }
        
        return message # 如果消息不大，原样返回

    def prune(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        Prunes the list of messages to fit within the safe zone of the context window.
        This now includes a pre-processing step to handle oversized single messages.
        """
        if not messages:
            return []

        # --- 第1步：单消息预处理层 ---
        # 遍历所有消息，对可能超长的消息进行截断
        pre_processed_messages = [self._truncate_single_message(msg) for msg in messages]

        # --- 第2步：多消息整体剪枝层 (逻辑与之前类似) ---
        total_tokens = self._count_total_tokens(pre_processed_messages)

        if total_tokens <= self.safe_zone_size:
            print(f"Total tokens within safe zone: {total_tokens}. No pruning needed.")
            return pre_processed_messages

        print(f"Context window overflow detected. Total tokens: {total_tokens}, "
              f"Safe zone: {self.safe_zone_size}. Pruning conversation history...")

        # 剪枝策略：保留系统提示，移除旧的对话
        system_prompt = None
        if pre_processed_messages[0]['role'] == 'system':
            system_prompt = pre_processed_messages[0]
            workable_messages = pre_processed_messages[1:]
        else:
            workable_messages = pre_processed_messages

        while self._count_total_tokens([system_prompt] + workable_messages if system_prompt else workable_messages) > self.safe_zone_size:
            if not workable_messages:
                # 经过单条消息截断后，这里几乎不可能再出现系统提示单独超长的情况
                # 但保留这个检查以防万一
                raise ValueError("System prompt alone exceeds the context window safe zone even after potential truncation.")
            
            removed_message = workable_messages.pop(0)
            print(f"  - Pruned historical message (role: {removed_message['role']}): '{removed_message['content'][:50]}...'")

        final_messages = [system_prompt] + workable_messages if system_prompt else workable_messages
        
        final_tokens = self._count_total_tokens(final_messages)
        print(f"Pruning complete. Final token count: {final_tokens}")

        return final_messages

    def _count_total_tokens(self, messages: List[Dict[str, str]]) -> int:
        if not messages:
            return 0
        full_text = "".join(msg.get('content', '') for msg in messages if msg) # 增加 if msg 保护
        return count_tokens(full_text, model_name=self.model_name)
===== ./context/pruner.py =====
# file: ddb_agent/context/pruner.py

from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any
import json

from pydantic import BaseModel, Field, field_validator
from llm.llm_prompt import llm
from token_counter import count_tokens
from utils.json_parser import parse_json_string 
class Document:
    """A simple container for source code data."""
    def __init__(self, file_path: str, source_code: str, tokens: int = -1):
        self.file_path = file_path
        self.source_code = source_code
        # 懒加载token计数，如果未提供
        from token_counter import count_tokens # 局部导入避免循环依赖
        self.tokens = tokens if tokens != None else count_tokens(source_code)

class BasePruner(ABC):
    """
    Abstract base class for all context pruning strategies.
    """
    def __init__(self, max_tokens: int):
        if max_tokens <= 0:
            raise ValueError("max_tokens must be a positive integer.")
        self.max_tokens = max_tokens

    @abstractmethod
    def prune(
        self, 
        file_sources: List[Document], 
        conversations: List[Dict[str, Any]]
    ) -> List[Document]:
        """
        Applies a specific pruning strategy to the list of file sources.

        Args:
            file_sources: A list of Document objects to be pruned.
            conversations: The conversation history, which may be used by the strategy.

        Returns:
            A pruned list of Document objects that fits within max_tokens.
        """
        pass

    def _count_total_tokens(self, sources: List[Document]) -> int:
        """Helper method to count total tokens of a list of sources."""
        return sum(source.tokens for source in sources)
    

class DeletePruner(BasePruner):
    """
    A simple pruner that discards files from the end of the list 
    if the total token count exceeds the limit.
    It assumes the input file_sources list is already sorted by importance.
    """
    def prune(
        self, 
        file_sources: List[Document], 
        conversations: List[Dict[str, Any]]
    ) -> List[Document]:
        
        print("Applying 'delete' pruning strategy...")
        
        total_tokens = self._count_total_tokens(file_sources)
        if total_tokens <= self.max_tokens:
            return file_sources

        pruned_sources: List[Document] = []
        current_tokens = 0

        for source in file_sources:
            if current_tokens + source.tokens <= self.max_tokens:
                pruned_sources.append(source)
                current_tokens += source.tokens
            else:
                print(f"Token limit reached. Discarding remaining files starting from {source.file_path}.")
                break
        
        print(f"Pruning complete. Kept {len(pruned_sources)} files with {current_tokens} tokens.")
        return pruned_sources

class ExtractedSnippet(BaseModel):
    """
    Represents a text snippet extracted by the LLM, along with its relevance score.
    """
    score: int = Field(description="The relevance score of the snippet to the user's query, from 0 (not relevant) to 10 (highly relevant).")
    snippet: str = Field(description="The actual extracted text or code snippet.")

    # @field_validator('score')
    # @classmethod
    # def score_must_be_in_range(cls, v: int) -> int:
    #     """Ensures the score is within the valid range of 0-10."""
    #     if not 0 <= v <= 10:
    #         raise ValueError('score must be between 0 and 10')
    #     return v

class ExtractPruner(BasePruner):
    """
    An intelligent pruner that extracts relevant code snippets from large files.
    """
    def __init__(self, max_tokens: int, llm_model_name: str = "deepseek-default", max_workers = 8):
        super().__init__(max_tokens)
        self.llm_model_name = llm_model_name
        self.full_file_threshold = int(max_tokens * 0.8)
        self.max_workers = max_workers

    @llm.prompt()
    def _extract_snippets_prompt(self, conversations: List[Dict[str, str]], content_with_lines: str) -> dict:
        """
        Based on the provided code file and conversation history, extract relevant code snippets.

        The code file content is provided below with line numbers.
        <CODE_FILE>
        {{ content_with_lines }}
        </CODE_FILE>

        Here is the conversation history leading to the current task.
        <CONVERSATION_HISTORY>
        {% for msg in conversations %}
        <{{ msg.role }}>: {{ msg.content }}
        {% endfor %}
        </CONVERSATION_HISTORY>

        Your Task:
        1. Analyze the last user request in the conversation history.
        2. Identify one or more important code sections in the code file that are relevant to this request.
        3. For each relevant section, determine its start and end line numbers.
        4. You can return up to 4 snippets.

        Output Requirements:
        - Return a JSON array of objects, where each object contains "start_line" and "end_line".
        - Line numbers must be integers and correspond to the numbers in the provided code file.
        - If no code sections are relevant, return an empty array [].
        - Your response MUST be a valid JSON array and nothing else.

        Example output:
        ```json
        [
            {"start_line": 10, "end_line": 25},
            {"start_line": 88, "end_line": 95}
        ]
        ```
        """''

        return {
            "conversations": conversations,
            "content_with_lines": content_with_lines
        }
      

    @llm.prompt()
    def _extract_content_prompt(self, conversations: List[Dict[str, str]], full_content: str) -> dict:
        """
        You are an expert content analyst. Your task is to extract the most relevant text snippets from a source document and score their relevance to a user's query.

        Here is the source document:
        <DOCUMENT>
        {{ full_content }}
        </DOCUMENT>

        Here is the conversation history. The last message is the user's primary request.
        <CONVERSATION_HISTORY>
        {% for msg in conversations %}
        <{{ msg.role }}>: {{ msg.content }}
        {% endfor %}
        </CONVERSATION_HISTORY>

        Your Task:
        1. Analyze the user's request in the conversation.
        2. Identify and extract the most relevant continuous blocks of text/code from the document.
        3. For each extracted snippet, assign a relevance score from 0 to 10, where 10 is most relevant and 0 is not relevant at all.
        4. Keep the snippets concise but complete. You can return up to 4 snippets.

        Output Requirements:
        - Your response MUST be a valid JSON array of objects.
        - Each object must have two keys: "score" (an integer from 0-10) and "snippet" (a string).
        - If no parts of the document are relevant, return an empty array [].
        - Do not include any text or explanations outside of the JSON array.

        Example output:
        ```json
        [
          {
            "score": 9,
            "snippet": "def calculate_pnl(trades, prices):\\n    # ... implementation ...\\n    return pnl"
          },
          {
            "score": 7,
            "snippet": "pnl_result = calculate_pnl(my_trades, daily_prices)"
          }
        ]
        ```
        """
        return {
            "conversations": conversations,
            "full_content": full_content
        }
    
    def _merge_overlapping_snippets(self, snippets: List[Dict[str, int]]) -> List[Dict[str, int]]:
        """...""" # 实现不变
        if not snippets: return []
        sorted_snippets = sorted(snippets, key=lambda x: x["start_line"])
        merged = [sorted_snippets[0]]
        for current in sorted_snippets[1:]:
            last = merged[-1]
            if current["start_line"] <= last["end_line"] + 1:
                last["end_line"] = max(last["end_line"], current["end_line"])
            else:
                merged.append(current)
        return merged

    def _build_snippet_content(self, original_code: str, snippets: List[Dict[str, int]]) -> str:
        """...""" # 实现不变
        lines = original_code.splitlines()
        content_parts = ["# Snippets from the original file:\n"]
        for snippet in snippets:
            start = max(0, snippet["start_line"] - 1)
            end = min(len(lines), snippet["end_line"])
            content_parts.append(f"\n# ... (lines {start + 1}-{end}) ...\n")
            content_parts.extend(lines[start:end])
        return "\n".join(content_parts)

    def _process_single_large_file(self, file_source: Document, conversations: List[Dict[str, Any]]) -> Document:
        """
        Processes a single large file to extract snippets. This is the target for our threads.
        Returns a new Document object with pruned content, or the original if it fails.
        """
        print(f"  - Starting snippet extraction for: {file_source.file_path}")
        try: 
        
            response_generator = self._extract_content_prompt(
                conversations=conversations,
                full_content=file_source.source_code
            )

            response_str = None
            try:
                while True:
                    next(response_generator)
            except StopIteration as e:
                response_str = e.value.content

            import re
            # 将所有非法反斜杠转义为合法形式，例如 \* -> \\*
            def escape_invalid_json_backslashes(s):
                return re.sub(r'\\([^"\\/bfnrtu])', r'\\\\\1', s)

            clean_json_str = escape_invalid_json_backslashes(response_str)

            json_items = parse_json_string(clean_json_str)
            extracted_items = [ExtractedSnippet(**item) for item in json_items if isinstance(item, dict)]
            
            # --- 关键：过滤掉低分数的片段 ---
            # 我们可以设定一个阈值，比如只保留分数大于等于5的片段
            score_threshold = 5
            high_score_snippets = [item for item in extracted_items if item.score >= score_threshold]

            if not high_score_snippets:
                print(f"  - No snippets with score >= {score_threshold} found in {file_source.file_path}.")
                return Document(file_source.file_path, "")
            
            # (可选) 可以按分数从高到低排序，让最重要的内容出现在前面
            high_score_snippets.sort(key=lambda x: x.score, reverse=True)

            # --- 构建新的内容 ---
            new_content_parts = [
                f"# Highly relevant snippets from {file_source.file_path} (filtered by score >= {score_threshold}):\n"
            ]
            for item in high_score_snippets:
                # 在注释中包含分数，便于调试
                new_content_parts.append(f"\n# Relevance Score: {item.score}\n---\n{item.snippet}\n")
            
            new_content = "".join(new_content_parts)

            return Document(file_source.file_path, new_content)

        except Exception as e:
            print(f"  - Error extracting content from {file_source.file_path}: {e}")
            return Document(file_source.file_path, "")


    def prune(
        self, 
        file_sources: List[Document], 
        conversations: List[Dict[str, Any]]
    ) -> List[Document]:
        
        print(f"Applying concurrent 'extract' pruning strategy with {self.max_workers} workers...")
        
        # 1. 分组：小文件 vs 大文件
        small_files: List[Document] = []
        large_files: List[Document] = []

        try:
        
            current_tokens = 0
            for source in file_sources:
                if source is None: 
                    print("Warning: Encountered a None source in file_sources. Skipping.", file_sources)
                if current_tokens + source.tokens <= self.full_file_threshold:
                    small_files.append(source)
                    current_tokens += source.tokens
                else:
                    large_files.append(source)
        
        # print(f"Split files: {len(small_files)} small files (kept whole), {len(large_files)} large files (to be processed).")
        
       

            # 测试
            large_files += small_files  # 确保小文件也在后续处理列表中
            # 2. 并发处理大文件
            processed_large_files: List[Document] = []
            if large_files:
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    future_to_source = {
                        executor.submit(self._process_single_large_file, source, conversations): source 
                        for source in large_files
                    }
                    
                    for future in as_completed(future_to_source):
                        try:
                            processed_source = future.result()
                            #if processed_source.tokens > 0: # 只保留有内容的
                            processed_large_files.append(processed_source)
                        except Exception as exc:
                            original_source = future_to_source[future]
                            print(f"Exception processing {original_source.file_path}: {exc}")

            # 3. 合并和最终剪枝
            # 将完整保留的小文件和处理后的大文件片段合并
            # 我们优先保留小文件，然后尝试添加处理后的大文件片段
            print("Merging results and performing final token check...")
            final_sources: List[Document] = []
            # final_tokens = self._count_total_tokens(small_files)
            # final_sources.extend(small_files)

            final_tokens = 0

            # 对处理后的大文件按（新）token数从小到大排序，优先添加小的
            processed_large_files.sort(key=lambda x: x.tokens)

            for source in processed_large_files:
                if final_tokens + source.tokens <= self.max_tokens:
                    final_sources.append(source)
                    final_tokens += count_tokens(source.source_code)
                    print(f"  - Added snippets from {source.file_path} ({source.tokens} tokens)")
                else:
                    print(f"  - Snippets from {source.file_path} ({source.tokens} tokens) too large to fit. Discarding.")
            
            print(f"Pruning complete. Final context has {len(final_sources)} files with {final_tokens} tokens.")
            return final_sources
        except Exception as e:
            import traceback    
            traceback.print_exc()
            print(f"Error during pruning: {e}. Returning original file sources.")


def get_pruner(strategy: str, max_tokens: int, **kwargs) -> BasePruner:
    """
    Factory function to get a pruner instance based on the strategy name.

    Args:
        strategy: The name of the strategy ('delete', 'extract').
        max_tokens: The maximum number of tokens allowed.
        **kwargs: Additional arguments for specific pruners (e.g., llm_model_name).

    Returns:
        An instance of a BasePruner subclass.
    """
    if strategy == "delete":
        return DeletePruner(max_tokens=max_tokens)
    elif strategy == "extract":
        return ExtractPruner(max_tokens=max_tokens, **kwargs)
    # 未来可以扩展
    # elif strategy == "summarize":
    #     return SummarizePruner(max_tokens=max_tokens, **kwargs)
    else:
        raise ValueError(f"Unknown pruning strategy: {strategy}. "
                         "Available strategies: 'delete', 'extract'.")
===== ./token_counter.py =====

import os
from typing import Dict, Optional, Callable
from functools import lru_cache
import transformers

# --- Tokenizer 注册表和加载器 ---

# 1. 注册表：存储已加载的Tokenizer实例
_tokenizer_cache: Dict[str, transformers.PreTrainedTokenizer] = {}

# 2. Tokenizer加载函数定义
#    这是一个可扩展的设计，我们可以为不同类型的模型定义不同的加载函数
def _load_deepseek_tokenizer(model_path: str) -> transformers.PreTrainedTokenizer:
    """
    加载 DeepSeek 系列模型的 Tokenizer。
    """
    print(f"Loading DeepSeek tokenizer from: {model_path}...")
    try:
        # trust_remote_code=True 是因为 DeepSeek 的 tokenizer 可能包含自定义代码
        tokenizer = transformers.AutoTokenizer.from_pretrained(
            model_path, trust_remote_code=True
        )
        print("Tokenizer loaded successfully.")
        return tokenizer
    except Exception as e:
        raise IOError(f"Failed to load tokenizer from {model_path}. "
                      f"Please ensure the tokenizer files are correctly placed. Error: {e}")

# 3. 模型名称到加载器和路径的映射
#    这个字典让我们可以为不同的模型别名配置不同的加载方式和路径
TOKENIZER_CONFIGS = {
    # 默认的 DeepSeek 模型别名
    "deepseek": {
        "loader": _load_deepseek_tokenizer,
        "path": "./tokenizers/deepseek" # 假设的路径
    },
    # 未来可以轻松扩展，例如：
    # "llama3": {
    #     "loader": _load_llama_tokenizer,
    #     "path": "./tokenizers/llama3"
    # }
}

def get_tokenizer(model_name: str) -> Optional[transformers.PreTrainedTokenizer]:
    """
    根据模型名称获取（加载并缓存）一个Tokenizer实例。

    Args:
        model_name: 在 TOKENIZER_CONFIGS 中定义的模型别名。

    Returns:
        一个 transformers Tokenizer 实例，如果找不到则返回 None。
    """
    if model_name in _tokenizer_cache:
        return _tokenizer_cache[model_name]

    config = TOKENIZER_CONFIGS.get(model_name)
    if not config:
        #print(f"Warning: No tokenizer configuration found for model '{model_name}'. "
             # f"Token counting may be inaccurate.")
        return None

    loader_func = config["loader"]
    path = config["path"]

    if not os.path.isdir(path):
        print(f"Warning: Tokenizer directory not found for model '{model_name}' at path '{path}'.")
        return None

    try:
        tokenizer = loader_func(path)
        _tokenizer_cache[model_name] = tokenizer
        return tokenizer
    except Exception as e:
        print(f"Error getting tokenizer for model '{model_name}': {e}")
        return None


# --- 统一的 Token 计数接口 ---

# 使用 lru_cache 进一步对计数结果进行缓存，提高对相同文本计数的性能
@lru_cache(maxsize=1024)
def count_tokens(text: str, model_name: str = "deepseek-default") -> int:
    """
    计算给定文本的 token 数量。

    Args:
        text: 要计算 token 的文本。
        model_name: 要使用的模型名称（别名），默认为 'deepseek-default'。
                    这个名称应在 TOKENIZER_CONFIGS 中有定义。

    Returns:
        token 的数量。如果找不到对应的 tokenizer，则会基于字符数进行粗略估算。
    """
    tokenizer = get_tokenizer(model_name)

    if tokenizer:
        try:
            # 使用 tokenizer.encode，因为它通常比 __call__ 更直接地返回 token ID 列表
            return len(tokenizer.encode(text))
        except Exception as e:
            print(f"Error encoding text with tokenizer for '{model_name}': {e}")
            # fall back to estimation
            return _estimate_tokens(text)
    else:
        # 如果没有找到 tokenizer，提供一个回退的估算方法
        return _estimate_tokens(text)

def _estimate_tokens(text: str) -> int:
    """
    一个简单的 token 估算方法，当没有可用 tokenizer 时的后备方案。
    对于英文，一个token约等于4个字符。对于中文，一个token约等于1.5-2个字符。
    我们取一个粗略的平均值。
    """
    return len(text) // 3
===== ./agent/__init__.py =====

===== ./agent/interactive_sql_executor.py =====
import json
import os
import re
from typing import Generator, Dict, Any, List, Optional
from agent.prompts import interactive_sql_agent_prompt
from agent.tool_manager import ToolManager
from agent.task_status import TaskStart, TaskEnd, TaskError, ReactThought, ReactAction, ReactObservation
from utils.json_parser import parse_json_string
from llm.llm_prompt import normalize_history_for_llm

class InteractiveSQLExecutor:
    """
    Implements the PLAN/ACT loop for interactive SQL tasks.
    """
    def __init__(self, tool_manager: ToolManager):
        self.tool_manager = tool_manager
        self.max_turns = 150
        self.injected_context: Optional[str] = None
        
    def _parse_xml_response(self, text: str, known_tools: List[str]) -> Dict[str, Any]:
        """
        A robust regex-based parser that handles transitional text and specifically
        looks for known tool names.

        Args:
            text: The raw text response from the LLM.
            known_tools: A list of valid tool names that the parser should look for.

        Returns:
            A dictionary with 'thought' and 'action' (or None).
        """
        # 1. Extract and remove the thinking block first.
        thought_match = re.search(r"<thinking>(.*?)</thinking>", text, re.DOTALL)
        thought = thought_match.group(1).strip() if thought_match else "No thought provided."
        
        # Create a "clean" string with the thinking block removed for tool searching
        text_without_thought = text[thought_match.end():] if thought_match else text

        # 2. Search for any of the known tool names in the remaining text.
        tool_name = None
        tool_match = None
        
        # We create a regex pattern like "(tool1|tool2|tool3)"
        tool_pattern = "|".join(re.escape(t) for t in known_tools)
        
        # Find the first occurrence of any known tool tag
        found_tool_match = re.search(f"<({tool_pattern})>", text_without_thought)

        if not found_tool_match:
            return {"thought": thought, "action": None}

        tool_name = found_tool_match.group(1)

        # 3. Once a valid tool is found, extract its full content block.
        action_content_match = re.search(f"<{re.escape(tool_name)}>(.*?)</{re.escape(tool_name)}>", text_without_thought, re.DOTALL)
        
        if not action_content_match:
            # This can happen if the closing tag is missing or malformed.
            return {"thought": thought, "action": None}

        action_content = action_content_match.group(1).strip()
        
        # 4. Parse parameters within the action block.
        params = {}
        param_pattern = r"<([a-zA-Z0-9_]+)>(.*?)</\1>"
        param_matches = re.findall(param_pattern, action_content, re.DOTALL)
        
        for name, value_str in param_matches:
            if name == 'options':
                # The LLM gives us a string that looks like a list. We need to parse it.
                # Using json.loads is safer than eval().
                try:
                    # First, try parsing it as a JSON array
                    parsed_value = json.loads(value_str.strip())
                    if isinstance(parsed_value, list):
                        params[name] = parsed_value
                    else:
                        # If it parses but isn't a list, treat as a single-item list
                        params[name] = [str(parsed_value)]
                except (json.JSONDecodeError, TypeError):
                    # If JSON parsing fails, fall back to a simple string split
                    # This handles cases like: "Option 1, Option 2"
                    params[name] = [opt.strip() for opt in value_str.split(',') if opt.strip()]
            else:
                params[name] = value_str.strip()
        
        return {
            "thought": thought,
            "action": {
                "tool_name": tool_name,
                "arguments": params
            }
        }
    
    def _format_tools_for_prompt(self, tool_defs: List[Dict[str, Any]]) -> str:
        prompt_lines = ["# Tools"]
        reverse_aliases = {v: k for k, v in self.tool_manager.tools.items()}
        for tool in tool_defs:
            name = tool.get("name")
            description = tool.get("description", "")
            prompt_tool_name = reverse_aliases.get(name, name)
            
            prompt_lines.append(f"## {prompt_tool_name}")
            prompt_lines.append(f"Description: {description}")
            
            params = tool.get("parameters", {}).get("properties", {})
            if params:
                prompt_lines.append("Parameters:")
                for param_name, schema in params.items():
                    # Map param names back for the prompt if needed (here we assume they match)
                    prompt_lines.append(f"- {param_name}: ({schema.get('type', 'any')}) {schema.get('description', '')}")
            prompt_lines.append("")
        return "\n".join(prompt_lines)
    
    

    def execute_task(self, user_input: str, conversation_history: List[Dict], injected_context: Optional[Dict] = None) -> Generator[Dict, None, List[Dict]]:

        context_parts = []
        if injected_context:
            # 1. 格式化 Schema 上下文
            schema_markdown = injected_context.get('schemas', {}).get('markdown')
            if schema_markdown:
                context_parts.append(schema_markdown)

            # 2. 格式化文件上下文
            files_dict = injected_context.get('files', {})
            if files_dict:
                file_context_str = "<INJECTED_FILES>\n"
                file_context_str += "The user has also injected the content of the following files. Prioritize this content when the query mentions them.\n"
                for path, data in files_dict.items():
                    if data['type'] == 'full_content':
                        lang = os.path.splitext(path)[1].lstrip('.')
                        file_context_str += f"\n--- Content of file: `{path}` ---\n"
                        # 使用我们提取的语言标识符
                        file_context_str += f"```{lang}\n"
                        file_context_str += data['content']
                        file_context_str += "\n```\n"
                file_context_str += "</INJECTED_FILES>"
                context_parts.append(file_context_str)
        
        self.injected_context = "\n\n".join(context_parts) if context_parts else None
        
        yield TaskStart(task_description=user_input, message="🚀 Starting Interactive Analyst task...")
        
        try:
            if not conversation_history or conversation_history[-1].get("content") != user_input:
                conversation_history.append({"role": "user", "content": user_input})
            
            history = conversation_history

            consecutive_errors = 0
            max_consecutive_errors = 5 
            turn_count = 0 

            while True:
                turn_count += 1
                if turn_count > self.max_turns: # 保留一个总轮次上限作为最终保险
                    yield TaskEnd(success=False, final_message=f"Task aborted: Exceeded maximum total turns ({self.max_turns}).", message="❌ Task failed: reached maximum total turns.")
                    return
                
                # 1. Build environment details
                current_mode = "ACT"
                environment_details = f"""In each user message, the environment_details will specify the current mode. There are two modes:
    - ACT MODE: In this mode, you have access to all tools EXCEPT the plan_mode_response tool.
    - PLAN MODE: In this special mode, you have access to the plan_mode_response tool.
    Current Mode: {current_mode}"""
                
                # 2. Get and format available tools
                tool_defs_list = self.tool_manager.get_tool_definitions(mode=current_mode)
                tools_for_prompt = self._format_tools_for_prompt(tool_defs_list)

                reverse_aliases = {v: k for k, v in self.tool_manager.tools.items()}
                known_tool_names_for_prompt = [reverse_aliases.get(t['name'], t['name']) for t in tool_defs_list]

                # 3. inject context 
                just_in_time_context = None
                if self.injected_context:
                    just_in_time_context = (
                        "====\n\n"
                        "# User-Provided Data Context (Highest Priority)\n\n"
                        "The user has just provided the following data context for this specific task. "
                        "You MUST use this schema information as the primary source of truth for database structures.\n\n"
                        f"{self.injected_context}\n\n"
                        "====\n"
                    )

                normalized_history = normalize_history_for_llm(history)

                # 4. Call LLM with the complete, structured context
                response_generator = interactive_sql_agent_prompt(
                    conversation_history=normalized_history,
                    available_tools=tools_for_prompt,
                    environment_details=environment_details,
                    just_in_time_context=just_in_time_context
                )
                
                llm_response = ""
                try:
                    while True: 
                        ck = next(response_generator)
                        yield {
                            "type": "task_status", # 保持和其他事件一致的顶级类型
                            "subtype": "llm_chunk", # 这是我们的新 subtype
                            "content": ck.data,
                            "message": "AI is thinking..." # 可以附带一条消息
                        }
                except StopIteration as e:
                    llm_response = e.value.content

                # 5. Parse, yield thought, and update history
                parsed_response = self._parse_xml_response(llm_response, known_tool_names_for_prompt)
                thought = parsed_response["thought"]
                action = parsed_response.get("action")
                
                yield ReactThought(thought=thought, message=f"🤔 Thinking... (Turn {turn_count}, Mode: {current_mode})")
                history.append({"role": "assistant", "content": llm_response})

                if not action:
                    consecutive_errors += 1
                    observation = "Agent did not produce a valid action. Please analyze the history and decide the next step."
                    yield ReactObservation(observation=observation, is_error=True, message="🔍 Observing result...")
                    history.append({"role": "tool_result", "content": observation})
                    if consecutive_errors >= max_consecutive_errors:
                        yield TaskEnd(success=False, final_message=f"Task aborted after {max_consecutive_errors} consecutive errors. The agent was unable to proceed.", message="❌ Task failed: too many consecutive errors.")
                        return
                    continue

                tool_name = action["tool_name"]
                arguments = action["arguments"]

                if (tool_name == "search_knowledge_base"): arguments["conversation_history"] = conversation_history

                # 6. Execute action and process result (logic remains the same as before)
                yield ReactAction(tool_name=tool_name, tool_args=arguments, message=f"🎬 Calling tool: {tool_name}")
                exec_result = yield from self.tool_manager.call_tool(tool_name, arguments)
                
                print(f"[Agent]: Tool '{tool_name}' executed with result: {exec_result}")
                is_error = not exec_result or not exec_result.success
                observation_content = ""
                
                if is_error:
                    consecutive_errors += 1
                    if consecutive_errors >= max_consecutive_errors:
                        error_message = exec_result.error_message if exec_result else "Unknown error."
                        yield TaskEnd(success=False, final_message=f"Task aborted after {max_consecutive_errors} consecutive errors. Last error:\n\n{error_message}", message="❌ Task failed: too many consecutive errors.")
                        return
                    error_message = exec_result.error_message if exec_result else "An unknown error occurred."

                    # 不再向用户求助，而是将错误信息直接作为观察结果
                    # 这会迫使 LLM 在下一轮思考如何处理这个错误
                    observation_content = (
                        f"Action '{tool_name}' failed with the following error:\n\n"
                        f"```\n{error_message}\n```\n\n"
                        "I need to analyze this error and decide how to fix it. "
                        "I should use tools like `search_knowledge_base` or `get_function_documentation` to get more information before trying again."
                    )
                elif isinstance(exec_result.data, dict) and exec_result.data.get("_is_completion_signal"):
                    consecutive_errors = 0
                    final_payload = exec_result.data
                    yield ReactObservation(observation="Task completion signaled.", is_error=False, message="🔍 Observing result...")
                    yield TaskEnd(success=True, final_message=final_payload['result'], message="✅ Task completed successfully.")
                    return history
                elif isinstance(exec_result.data, dict) and exec_result.data.get("_is_interactive_request"):
                    consecutive_errors = 0
                    interaction_data = exec_result.data
                    user_choice = yield interaction_data
                    if user_choice: # Check if user_choice is not None
                        observation_content = f"User responded with: '{user_choice}'"
                        history.append({"role": "user", "content": user_choice})
                    else:
                        # This branch is now reachable if resumed by next() instead of send()
                        observation_content = "Resumed without user input. Continuing with current plan."
                else:
                    consecutive_errors = 0
                    try:
                        observation_content = json.dumps(exec_result.data, indent=2, ensure_ascii=False) if isinstance(exec_result.data, (dict, list)) else str(exec_result.data)
                    except TypeError:
                        observation_content = str(exec_result.data)

                yield ReactObservation(observation=observation_content, is_error=is_error, message="🔍 Observing result...")

                # 1. 创建上下文前缀，明确告诉模型这是哪个工具的结果
                tool_result_prefix = f"[{tool_name}] Result:"

                # 2. 准备环境详情
                current_mode = "ACT"
                environment_details_content = f"""<environment_details>
                In each user message, the environment_details will specify the current mode. There are two modes:
                - ACT MODE: In this mode, you have access to all tools EXCEPT the plan_mode_response tool.
                - PLAN MODE: In this special mode, you have access to the plan_mode_response tool.
                Current Mode: {current_mode}
                </environment_details>"""

                # 3. 构建一个和成功日志完全一致的多部分 user 消息
                tool_result_message = {
                    "role": "user",
                    "content": [
                        # 部分 1: 上下文前缀
                        {"type": "text", "text": tool_result_prefix},
                        # 部分 2: 真实的工具输出
                        {"type": "text", "text": observation_content},
                        # 部分 3: 每次都附带的环境详情
                        {"type": "text", "text": environment_details_content}
                    ]
                }

                # 4. 将这个结构化的 user 消息添加到历史记录中
                history.append(tool_result_message)

            yield TaskEnd(success=False, final_message="Task failed to complete within the maximum number of turns.", message="❌ Task failed: reached maximum turns.")
        except Exception as e:
            import traceback
            traceback.print_stack()
            print(str(e))
===== ./agent/prompts.py =====
# file: agent/prompts.py

from typing import List, Dict, Any, Tuple
from llm.llm_prompt import llm


@llm.prompt() # 可以选用一个擅长总结的模型
def generate_final_user_answer(
    task_description: str,
    execution_history: List[Dict[str, str]],
    final_thought: str
) -> str:
    """
    You are a meticulous and factual report generator. Your sole purpose is to transform a raw execution log into a clear, structured, and user-friendly final report. You MUST NOT invent information or summarize creatively. Your report must be based exclusively on the successful `Observation` data provided.

    ## 1. User's Original Request
    {{ task_description }}

    ## 2. Raw Execution Log
    This is the sequence of thoughts, actions, and observations performed by an agent. The `Observation` sections contain the factual results of each action.
    ---
    {% for turn in execution_history %}
    Thought: {{ turn.thought }}
    Action: {{ turn.action_str }}
    Observation: {{ turn.observation }}
    ---
    {% endfor %}

    ## 3. Agent's Final Thought (For Context Only)
    {{ final_thought }}

    ## YOUR CRUCIAL TASK: Generate the Final Report

    ### Core Objective:
    Extract all successful, tangible results from the `Observation` sections of the log and present them clearly to the user.

    ### Strict Rules:
    1.  **Fact-Based Reporting**: Your answer MUST be built directly from the content within the `Observation` blocks. If an observation contains a list of files, you must list those files. If it contains data, you must present that data.
    2.  **Ignore Failures**: Do NOT mention failed steps or errors in the final report. Only report on successful outcomes.
    3.  **No New Actions**: Do not suggest new steps or actions. The task is complete. Your only job is to report the results.
    4.  **Handle No Results**: If the execution log contains no successful observations with concrete data, you MUST state that clearly. For example: "The process completed, but no specific data or files were found that match your request."

    ### Mandatory Output Structure:
    You MUST follow this Markdown structure precisely.

    **Summary:**
    [A brief, one-sentence summary of the final outcome based on the results.]

    **Detailed Results:**
    [Use bullet points (`*`) to list each key finding. Each bullet point MUST correspond to a piece of data from a successful `Observation`.]

    **Final Output (if applicable):**
    [If a final script, file content, or structured data block was produced in the last successful step, present it here in a proper Markdown code block.]

    ---
    ### Example of correct behavior:
    **If an `Observation` is:**
    ```json
    [
      {"name": "davis_speech_draft.md", "path": "/docs/davis_speech_draft.md"},
      {"name": "davis_community_plan.docx", "path": "/docs/davis_community_plan.docx"}
    ]
    ```
    **Your report's "Detailed Results" section MUST contain:**
    *   Found file: `davis_speech_draft.md`
    *   Found file: `davis_community_plan.docx`
    ---

    Now, based on the provided log, generate the final report for the user.
    """
    pass


@llm.prompt()
def react_agent_prompt(
    task_description: str,
    history: List[Dict[str, str]],
    available_tools: str,
    rag_context: str
) -> str:
    """
    You are an autonomous DolphinDB expert. Your primary goal is to achieve the user's task by thinking step-by-step and using the available tools. You must respond in a specific JSON format.

    ## 1. Primary Goal
    **The user wants you to: {{ task_description }}**

    ## 2. Available Tools
    You have access to the following tools to interact with the environment.
    ```json
    {{ available_tools }}
    ```

    ## 3. Relevant Context from Knowledge Base
    This information from the knowledge base might be useful.
    <CONTEXT>
    {{ rag_context }}
    </CONTEXT>

    ## 4. Conversation History (Your Previous Steps)
    This is the history of your previous thoughts, actions, and their observed outcomes.
    ---
    {% for turn in history %}
    Thought: {{ turn.thought }}
    Action:
    ```json
    {{ turn.action_str }}
    ```
    Observation: {{ turn.observation }}
    ---
    {% endfor %}

    ## 5. Your Crucial Task
    Your task is to analyze the history and the user's goal, then decide on the single next step. This could be using another tool OR finishing the task.

    ### **Decision-Making Process:**

    1.  **Analyze the last `Observation`:** What new information did I get? Was it what I expected? Did it contain an error?
    2.  **Check against the `Primary Goal`:** Do I now have enough information to fully and completely answer the user's request?
    3.  **Decide:**
        *   **If YES, I am done:** I must stop using tools. My next action is to give the final answer.
        *   **If NO, I need more information:** I must choose ONE tool from the list to get closer to the goal.

    ### **Output Format:**
    Your response MUST be a single, valid JSON object with the following structure. Do not add any text before or after the JSON object.

    ```json
    {
      "thought": "Your detailed reasoning. First, explicitly state whether you have enough information to answer the user's goal (e.g., 'Based on the file content from the last step, I now have everything I need.'). Then, formulate your response or explain which tool you will use next and why.",
      "action": {
        "tool_name": "name_of_the_tool_to_use",
        "arguments": {
          "arg1": "value1"
        }
      }
    }
    ```

    ### !! CRITICAL INSTRUCTION !!
    **To finish the task and give the final answer to the user, you MUST set the `action` field to `null`.** For example:
    ```json
    {
      "thought": "I have successfully read the file and calculated the average. The final answer is 42.7. I have completed the user's request.",
      "action": null
    }
    ```
    """

    return {
        "history": history,
        "task_description": task_description,
        "available_tools": available_tools,
        "rag_context": rag_context
    }

@llm.prompt() # 我们可以为代码任务指定一个更擅长编码的模型
def generate_initial_script(user_query: str, rag_context: str) -> str:
    """
    You are a world-class DolphinDB expert developer. Your task is to write a DolphinDB script to solve the user's request.

    ## User Request
    {{ user_query }}

    ## Relevant Context from Documentation
    Based on my knowledge base, here is some context that might be helpful:
    <CONTEXT>
    {{ rag_context }}
    </CONTEXT>

    sql中，不要top和limit一起使用，如果要筛选前几条数据，使用select top n * from xx 这样的语句，后面不要再带limit了
    
    ## Your Task
    - Write a complete, executable DolphinDB script that directly addresses the user's request.
    - **Do not** add any explanations, comments, or markdown formatting around the code.
    - Your output must be **only the raw script code**.
    - ensure the output not wrappered in any code block or markdown formatting.
    """
    pass


@llm.prompt() # 同样使用编码模型
def fix_script_from_error(
    original_query: str,
    failed_code: str,
    error_message: str,
    rag_context: str,
    # (可选) 增加一个 full_history 字段，提供完整的尝试历史
    # full_history: str 
) -> str:
    """
    You are an elite DolphinDB debugging expert. You previously wrote a script that failed to execute. Your task is to analyze the error and provide a corrected version of the script.

    ## Original User Request
    {{ original_query }}

    ## Context from Documentation
    <CONTEXT>
    {{ rag_context }}
    </CONTEXT>

    ## The Code That Failed
    The following script was executed:
    ```dolphiindb
    {{ failed_code }}
    ```

    ## Execution Error
    It failed with the following error message:
    ```
    {{ error_message }}
    ```

    ## Your Task
    1.  Carefully analyze the error message in the context of the code and the original request.
    2.  Identify the root cause of the error.
    3.  Provide a new, complete, and corrected version of the script.
    4.  **Do not** add any explanations or markdown. Your output must be **only the raw, fixed script code**.
    """
    pass

@llm.prompt() # Planner需要最强的模型
def debugging_planner(
    original_query: str,
    failed_code: str,
    error_message: str,
    tool_definitions: str,
    # 也可以加入对话历史、RAG上下文等
) -> str:
    """
    You are an autonomous debugging expert for DolphinDB.
    A script you wrote has failed. Your goal is to create a step-by-step plan to identify the cause of the error and fix the script.

    ## Initial Goal
    The user wants to: {{ original_query }}

    ## The Code that Failed
    ```dolphiindb
    {{ failed_code }}
    ```

    ## The Error Message
    ```
    {{ error_message }}
    ```

    ## Available Tools
    You have access to the following tools to help you diagnose the problem.
    {{ tool_definitions }}

    ## Your Task
    Based on the error, create a JSON plan of actions to take.
    - Think step-by-step.
    - The plan should lead to a final, corrected script.
    - The available actions are the names of the tools provided.
    - The final step in your plan should ALWAYS be `run_dolphindb_script` with the fully corrected code.

    Example Plan for a function error:
    ```json
    [
      {
        "step": 1,
        "thought": "The error message 'wavg function needs 2 argument(s)' suggests I used the wavg function incorrectly. I need to check its correct signature and documentation.",
        "action": "get_function_signature",
        "args": {"function_name": "wavg"}
      },
      {
        "step": 2,
        "thought": "The documentation shows wavg requires two arguments: a value column and a weight column. The original code only provided one. I need to add the 'qty' column as the weight. I will now construct the corrected script and run it.",
        "action": "run_dolphindb_script",
        "args": {"script": "trades = stocks::create_mock_trades_table()\nselect wavg(price, qty) from trades"}
      }
    ]
    ```
    """
    pass


@llm.prompt() # Or your preferred powerful model for reasoning
def interactive_sql_agent_prompt(
    conversation_history: List[Dict[str, str]],
    available_tools: str,
    environment_details: str,
    just_in_time_context: str
) -> Tuple[str, Dict[str, Any]]:
    """
    {# This is the new USER PROMPT template. It's clean and focused on dynamic context. #}
    <environment_details>
    {{ environment_details }}
    </environment_details>

    <current_time>
    {{ now() }} 
    </current_time>

    """
    
    # This is the SYSTEM PROMPT. It contains all the static rules and instructions.
    system_prompt = f"""
    You are  a highly skilled Data analysis with extensive knowledge in many programming languages, data processing, analysis, and visualization.

    Core Principle: Question Classification and Tool Usage
    Before deciding whether to use a tool, you MUST first classify the user's query.
    1. Technical / DolphinDB-Related Questions:
    These questions require leveraging your specialized knowledge and tools to answer.
    Examples: "Show me the first 5 rows of the 'trades' table.", "How do I calculate VWAP in DolphinDB?"
    Action: For this category, proceed with the standard process of using analytical tools (run_dolphindb_script, search_knowledge_base, etc.) and then deliver the final result using the <attempt_completion> tool.
    2. General / Conversational Questions:
    These questions do not involve data analysis or specific DolphinDB knowledge.
    Examples: "Who are you?", "Hello", "What is your name?"
    Action: For these questions, you must NEVER use analytical tools. Instead, formulate a direct and concise answer within your <thinking> block, and then immediately use the <attempt_completion> tool to deliver that answer.
    
    ====

    TOOL USE

    You have access to a set of tools that are executed upon the user's approval. You can use one tool per message, and will receive the result of that tool use in the user's response. You use tools step-by-step to accomplish a given task, with each tool use informed by the result of the previous tool use.

    # Tool Use Formatting

    Tool use is formatted using XML-style tags. The tool name is enclosed in opening and closing tags, and each parameter is similarly enclosed within its own set of tags. Here's the structure:

    <tool_name>
    <parameter1_name>value1</parameter1_name>
    <parameter2_name>value2</parameter2_name>
    ...
    </tool_name>

    Always adhere to this format for the tool use to ensure proper parsing and execution.

    # Tools
    {available_tools}

    # Tool Use Guidelines

    1. In <thinking> tags, assess what information you already have and what information you need to proceed with the task.
    2. Choose the most appropriate tool based on the task and the tool descriptions provided.
    3. If multiple actions are needed, use one tool at a time per message.
    4. Formulate your tool use using the XML format specified for each tool.
    5. After you call a tool, the system will provide its output in the next message with the role tool_result. You must use the content of this tool_result message to inform your next step and continue the task.

    ====

    AVAILABLE DATA CONTEXT

    
    {"# User-Provided Data Context (Highest Priority)" if just_in_time_context else ""}
    {just_in_time_context if just_in_time_context else ""}
        
    ====

    ACT MODE V.S. PLAN MODE

    {environment_details}

    ## What is PLAN MODE?
    - You start in PLAN MODE to gather information and create a detailed plan.
    - Use tools like `list_tables` and `describe_table` to get context about the task.
    - Use the `plan_mode_response` tool to ask clarifying questions or present your plan.
    - Once the user is satisfied with the plan, they will ask you to switch to ACT MODE to implement the solution.

    ====

    RULES

    - Your goal is to accomplish the user's task, NOT engage in a back and forth conversation.
    - NEVER end `attempt_completion` result with a question. Formulate the end of your result in a way that is final.
    - You are STRICTLY FORBIDDEN from starting your messages with "Great", "Certainly", "Okay", "Sure". Be direct and technical.
    - At the end of each user message, you will automatically receive `environment_details`. Use this to inform your actions.
    - It is CRITICALLY IMPORTANT to follow the guidelines after successful execution of `run_dolphindb_script`: Analyze the schema and evaluate if the data is suitable for visualization.
    - Do not base summaries or insights on partial data samples. Use SQL to analyze the complete dataset for metrics like max, min, avg, etc.
    - In your <thinking> block, you MUST first summarize the result from the previous tool_result before deciding your next action. This proves you have processed the new information. For example: "The describe_table tool succeeded and showed me the table has columns X, Y, Z. Now that I know the structure, my next step is to query the first 5 rows to see the data."

    **CRITICAL RULE: KNOWLEDGE BASE USAGE**
    If you encounter a question that is not about querying specific data from a table, but is a general question about DolphinDB, finance, or requires external knowledge, you MUST use the `search_knowledge_base` tool first.
    
    **Examples of WHEN to use `search_knowledge_base`:**
    - "What is the difference between a partitioned table and a dimension table?"
    - "How do I calculate VWAP in DolphinDB?"
    - "What were the main market trends last week?" (If you have news/docs in your knowledge base)

    **Examples of WHEN NOT to use `search_knowledge_base`:**
    - "Show me the first 5 rows of the 'trades' table." (Use `run_dolphindb_script` or `query_data`)
    - "What is the average price of AAPL?" (Use `run_dolphindb_script`)

    
    **CRITICAL RULE: ERROR HANDLING AND SELF-CORRECTION**
    If a tool action results in an error, you MUST NOT ask the user for help. Your primary objective is to solve the problem autonomously. Follow this process:
    1.  **Analyze the Error**: Carefully read the error message provided in the observation.
    2.  **Formulate a Debugging Plan**: Think about the cause of the error.
        - If it's a function usage error (e.g., wrong arguments), use the `get_function_documentation` tool.
        - If it's a general or unexpected error, **your first step should be to use the `search_knowledge_base` tool**. Pass the core part of the error message as the `query`.
        - If it's a data-related error (e.g., table not found), use `list_tables` or `describe_table` to investigate.
    3.  **Act on the Plan**: Execute the chosen debugging tool.
    4.  **Synthesize and Retry**: After gathering information from the debugging tool, formulate a corrected action and try again. Only after multiple failed attempts should you consider reporting a failure with `attempt_completion`.

    ====

    OBJECTIVE

    You accomplish a given task iteratively, breaking it down into clear steps.

    1. Analyze the user's task and set clear, achievable goals.
    2. Work through these goals sequentially, utilizing available tools one at a time.
    3. Before calling a tool, do some analysis within <thinking></thinking> tags. If a required parameter is missing, DO NOT invoke the tool; instead, ask the user to provide the missing parameters using `plan_mode_response`.

    ====

    !! CRITICAL: HOW TO COMPLETE THE TASK !!
    When you have successfully gathered all the necessary information and have a final, complete answer that directly addresses the user's original request, you MUST use the `attempt_completion` tool. This is the final step of any task.

    Your thought process for completion MUST be:
    1.  **Final Thought**: In the `<thinking>` block, explicitly state that the task is complete. Summarize the final answer you are about to provide. For example: "I have successfully retrieved the top 4 stocks for August. The task is complete and I will now present the final answer."
    2.  **Final Action**: Call the `<attempt_completion>` tool.
    3.  **Final Parameter**: The `<final_answer>` parameter inside the tool call MUST contain the complete, well-formatted, user-facing response.

    **Example of Finishing:**
    <thinking>
    I have successfully found the table schema and the first 5 rows of the 'kline' table. This fulfills the user's request to 'view the kline data'. I will now present this information as the final answer.
    </thinking>
    <attempt_completion>
    <final_answer>
    The schema for the 'kline' table has been retrieved successfully. It contains the following columns: ts_code, trade_date, open, high, low, close, etc.

    USER'S CUSTOM INSTRUCTIONS

    # Preferred Language

    Speak in zh-CN.
    """
    
    # The function now returns a tuple: (system_prompt, context_for_user_prompt)
    return system_prompt, {"conversation_history": conversation_history}
===== ./agent/task_status.py =====
from pydantic import BaseModel, Field
from typing import Optional, Literal, Union, List, Dict, Any
from typing import Annotated

# --- 基类 ---
class BaseTaskStatus(BaseModel):
    """所有任务状态更新的基类。"""
    type: Literal["task_status"] = "task_status"
    message: str = Field(description="向用户展示的友好信息。")

# --- 通用状态子类 ---

class TaskStart(BaseTaskStatus):
    """表示任务开始。"""
    subtype: Literal["start"] = "start"
    task_description: str

class TaskEnd(BaseTaskStatus):
    """表示任务结束。"""
    subtype: Literal["end"] = "end"
    success: bool
    final_message: str
    final_script: Optional[str] = Field(default=None, description="如果任务生成了脚本，则包含该脚本。")

class TaskError(BaseTaskStatus):
    """表示任务中发生严重错误。"""
    subtype: Literal["error"] = "error"
    error_details: str



class PlanGenerationStart(BaseTaskStatus):
    """表示开始生成执行计划。"""
    subtype: Literal["plan_gen_start"] = "plan_gen_start"
    reason: str = Field(description="生成计划的原因（如 'initial', 'debug_fix'）。")

class PlanGenerationEnd(BaseTaskStatus):
    """表示计划生成结束。"""
    subtype: Literal["plan_gen_end"] = "plan_gen_end"
    plan: List[Dict[str, Any]] = Field(description="生成的计划步骤列表。")

class StepExecutionStart(BaseTaskStatus):
    """表示开始执行计划中的一个步骤。"""
    subtype: Literal["step_exec_start"] = "step_exec_start"
    step_index: int
    total_steps: int
    step_info: Dict[str, Any]

class StepExecutionEnd(BaseTaskStatus):
    """表示一个步骤执行结束。"""
    subtype: Literal["step_exec_end"] = "step_exec_end"
    step_index: int
    observation: str
    is_success: bool
    script: Optional[str] = Field(default=None, description="如果步骤执行生成了脚本，则包含该脚本。")

class ReactThought(BaseTaskStatus):
    """Represents the agent's thought process for a step."""
    subtype: Literal["react_thought"] = "react_thought"
    thought: str

class ReactAction(BaseTaskStatus):
    """Represents the agent's chosen action (tool call)."""
    subtype: Literal["react_action"] = "react_action"
    tool_name: str
    tool_args: Dict[str, Any]

class ReactObservation(BaseTaskStatus):
    """Represents the result of an action (observation)."""
    subtype: Literal["react_observation"] = "react_observation"
    observation: str
    is_error: bool



AnyTaskStatus = Annotated[
    Union[
        TaskStart,
        TaskEnd,
        TaskError,
        PlanGenerationStart,
        PlanGenerationEnd,
        StepExecutionStart,
        StepExecutionEnd,
        ReactThought,
        ReactAction,
        ReactObservation,
    ],
    Field(discriminator="subtype"),
]
===== ./agent/tools/completion_tool.py =====
from pydantic import Field
from agent.execution_result import ExecutionResult
from .tool_interface import BaseTool, ToolInput, ensure_generator

class AttemptCompletionInput(ToolInput):
    final_answer: str = Field(description="The final, complete answer to the user's request. This should be a comprehensive summary of the results, ready to be presented to the user.")

class AttemptCompletionTool(BaseTool):
    name = "attempt_completion"
    description = (
        "Use this tool ONLY when you have successfully completed all necessary steps and have the final answer for the user. "
        "This is the very last tool you should call in your plan to conclude the task."
    )
    args_schema = AttemptCompletionInput

    @ensure_generator
    def run(self, args: AttemptCompletionInput) -> ExecutionResult:
        """
        Signals that the task is complete. The data payload contains a special
        flag that the executor can use to terminate the loop gracefully.
        """
        completion_data = {
            "_is_completion_signal": True,
            "result": args.final_answer
        }
        
        return ExecutionResult(
            success=True,
            data=completion_data
        )
===== ./agent/tools/ddb_tools.py =====
import dolphindb as ddb
from pydantic import Field

from agent.execution_result import ExecutionResult 
from .tool_interface import BaseTool, ToolInput, ensure_generator
from agent.code_executor import CodeExecutor 
 
class RunDolphinDBScriptInput(ToolInput):
    script: str = Field(description="The DolphinDB script to execute.")

class RunDolphinDBScriptTool(BaseTool):
    name = "run_dolphindb_script"
    description = "Executes a given DolphinDB script. Returns the data output on success or an error message on failure."
    args_schema = RunDolphinDBScriptInput
    
    def __init__(self, executor: CodeExecutor):
        self.executor = executor

    @ensure_generator
    def run(self, args: RunDolphinDBScriptInput) -> ExecutionResult:
        return self.executor.run(args.script)
===== ./agent/tools/enhanced_ddb_tools.py =====
# file: agent/tools/enhanced_ddb_tools.py

import dolphindb as ddb
from pydantic import Field, field_validator
from typing import Generator, List, Dict, Any, Optional
import json
import os

from agent.execution_result import ExecutionResult
from context.context_builder import ContextBuilder
from context.pruner import Document
from llm.llm_prompt import llm
from rag.candidate_selector import LLMCandidateSelector
from rag.rag_status import AnyRagStatus, RagEnd, RagIndexLoaded, RagRerankStart, RagSelectionStart, RagStart
from rag.text_index_manager import TextIndexManager
from rag.types import BaseIndexModel 
from .tool_interface import BaseTool, ToolInput, ensure_generator
from agent.code_executor import CodeExecutor


class InspectDatabaseInput(ToolInput):
    """检查数据库连接和基本信息"""
    pass


class InspectDatabaseTool(BaseTool):
    name = "inspect_database"
    description = "Check database connection status and get basic system information like version, memory usage, and available databases."
    args_schema = InspectDatabaseInput
    
    def __init__(self, executor: CodeExecutor):
        self.executor = executor
    
    @ensure_generator
    def run(self, args: InspectDatabaseInput) -> ExecutionResult:
        """检查数据库状态"""
        inspection_script = """
        // Database inspection script
        info = dict(STRING, ANY)
        info["version"] = version()
        info["license"] = license()
        info["node_count"] = getClusterPerf().size()
        info["databases"] =  getClusterDFSDatabases()
      
        info
        """
        
        result = self.executor.run(inspection_script)
        return result

class ListTablesInput(ToolInput):
    database_name: Optional[str] = Field(default=None, description="Database name to list tables from. If not provided, lists tables from current session.")
    pattern: Optional[str] = Field(default=None, description="Pattern to filter table names (SQL LIKE pattern)")


class ListTablesTool(BaseTool):
    name = "list_tables"
    description = "List all tables in a database or current session, optionally filtered by pattern."
    args_schema = ListTablesInput
    
    def __init__(self, executor: CodeExecutor):
        self.executor = executor
    
    @ensure_generator
    def run(self, args: ListTablesInput) -> ExecutionResult:
        if args.database_name:
            script = f"""
            getTables(database("{args.database_name}"))
            """
        else:
            script = """
            // List tables in current session
            objs = objs(true)
            tables = select name, type, form from objs where type="TABLE"
            tables
            """
        
        result = self.executor.run(script)
        return result


class DescribeTableInput(ToolInput):
    table_name: str = Field(description="Name of the table to describe")
    database_name: Optional[str] = Field(default=None, description="Database name if table is in a specific database")


class DescribeTableTool(BaseTool):
    name = "describe_table"
    description = "Get detailed schema information about a table including column names, types, and sample data."
    args_schema = DescribeTableInput
    
    def __init__(self, executor: CodeExecutor):
        self.executor = executor
    
    @ensure_generator
    def run(self, args: DescribeTableInput) -> ExecutionResult:
        if args.database_name:
            table_ref = f'database("{args.database_name}").loadTable("{args.table_name}")'
        else:
            table_ref = args.table_name
        
        script = f"""
        // Describe table structure and sample data
        t = {table_ref}
        
        result = dict(STRING, ANY)
        result["table_name"] = "{args.table_name}"
        result["schema"] = schema(t)
        result["column_count"] = t.columns().size()
        result["row_count"] = t.size()
        
        // Get sample data (first 5 rows)
        try {{
            result["sample_data"] = select top 5 * from t
        }} catch(ex) {{
            result["sample_data"] = "Unable to fetch sample data: " + ex
        }}
        
        result
        """
        
        result = self.executor.run(script)
        return result


class ValidateScriptInput(ToolInput):
    script: str = Field(description="DolphinDB script to validate for syntax errors")


class ValidateScriptTool(BaseTool):
    name = "validate_script"
    description = "Check a DolphinDB script for syntax errors without executing it."
    args_schema = ValidateScriptInput
    
    def __init__(self, executor: CodeExecutor):
        self.executor = executor
    
    @ensure_generator
    def run(self, args: ValidateScriptInput) -> ExecutionResult:
        # DolphinDB doesn't have a built-in syntax validator, so we'll try to parse it
        validation_script = f"""
        try {{
            parseExpr(`{args.script.replace('`', '``')})
            "Script syntax is valid"
        }} catch(ex) {{
            "Syntax error: " + ex
        }}
        """
        
        result = self.executor.run(validation_script)
        return result


class QueryDataInput(ToolInput):
    query: str = Field(description="SQL query to execute")
    limit: Optional[int] = Field(default=100, description="Maximum number of rows to return")


class QueryDataTool(BaseTool):
    name = "query_data"
    description = "Execute a SELECT query and return results with optional row limit."
    args_schema = QueryDataInput
    
    def __init__(self, executor: CodeExecutor):
        self.executor = executor
    
    @ensure_generator
    def run(self, args: QueryDataInput) -> ExecutionResult:
        # Add limit to query if not already present
        query = args.query.strip()
        # if args.limit and not query.lower().startswith('select top'):
        #     if query.lower().startswith('select'):
        #         query = query.replace('select', f'select top {args.limit}', 1)
        
        result = self.executor.run(query)
        return result


class CreateSampleDataInput(ToolInput):
    data_type: str = Field(description="Type of sample data to create (e.g., 'trades', 'quotes', 'timeseries')")
    row_count: Optional[int] = Field(default=1000, description="Number of rows to generate")
    table_name: Optional[str] = Field(default=None, description="Name for the created table")


class CreateSampleDataTool(BaseTool):
    name = "create_sample_data"
    description = "Create sample data for testing purposes. Supports common financial data types."
    args_schema = CreateSampleDataInput
    
    def __init__(self, executor: CodeExecutor):
        self.executor = executor
    
    @ensure_generator
    def run(self, args: CreateSampleDataInput) -> ExecutionResult:
        table_name = args.table_name or f"sample_{args.data_type}"
        
        if args.data_type.lower() == "trades":
            script = f"""
            // Create sample trades data
            n = {args.row_count}
            symbols = `AAPL`MSFT`GOOGL`AMZN`TSLA
            {table_name} = table(
                take(symbols, n) as symbol,
                2023.01.01T09:30:00.000 + rand(6.5*60*60*1000, n) as timestamp,
                20.0 + rand(100.0, n) as price,
                100 + rand(1000, n) as qty,
                rand(`B`S, n) as side
            )
            select count(*) as row_count from {table_name}
            """
        elif args.data_type.lower() == "quotes":
            script = f"""
            // Create sample quotes data
            n = {args.row_count}
            symbols = `AAPL`MSFT`GOOGL`AMZN`TSLA
            {table_name} = table(
                take(symbols, n) as symbol,
                2023.01.01T09:30:00.000 + rand(6.5*60*60*1000, n) as timestamp,
                20.0 + rand(100.0, n) as bid_price,
                20.1 + rand(100.0, n) as ask_price,
                100 + rand(1000, n) as bid_size,
                100 + rand(1000, n) as ask_size
            )
            select count(*) as row_count from {table_name}
            """
        elif args.data_type.lower() == "timeseries":
            script = f"""
            // Create sample time series data
            n = {args.row_count}
            {table_name} = table(
                2023.01.01T00:00:00.000 + (0..(n-1)) * 60000 as timestamp,
                100.0 + cumsum(rand(2.0, n) - 1.0) as value,
                rand(10.0, n) as volume
            )
            select count(*) as row_count from {table_name}
            """
        else:
            return f"Unsupported data type: {args.data_type}. Supported types: trades, quotes, timeseries"
        
        result = self.executor.run(script)
        return result


class OptimizeQueryInput(ToolInput):
    query: str = Field(description="Query to analyze and optimize")


class OptimizeQueryTool(BaseTool):
    name = "optimize_query"
    description = "Analyze a query and suggest optimizations for better performance."
    args_schema = OptimizeQueryInput
    
    def __init__(self, executor: CodeExecutor):
        self.executor = executor
    
    @ensure_generator
    def run(self, args: OptimizeQueryInput) -> ExecutionResult:
        # This is a simplified optimization analyzer
        # In practice, you might want to use DolphinDB's query plan analysis
        
        analysis_script = f"""
        // Basic query analysis
        query_text = `{args.query.replace('`', '``')}`
        
        analysis = dict(STRING, ANY)
        analysis["original_query"] = query_text
        
        // Check for common optimization opportunities
        suggestions = string[]
        
        if(query_text.regexFind("select \\\\* from") != -1)
            suggestions.append!("Consider selecting only needed columns instead of SELECT *")
        
        if(query_text.regexFind("where.*=.*or.*=") != -1)
            suggestions.append!("Consider using IN clause instead of multiple OR conditions")
        
        if(query_text.regexFind("order by") != -1 && query_text.regexFind("limit|top") == -1)
            suggestions.append!("Consider adding LIMIT/TOP clause when using ORDER BY")
        
        analysis["suggestions"] = suggestions
        analysis["query_length"] = query_text.size()
        
        analysis
        """
        
        result = self.executor.run(analysis_script)
        return result
        

class GetFunctionDocumentationInput(ToolInput):
    """Input model for the function documentation tool."""
    function_name: str = Field(description="The name of the DolphinDB function to look up documentation for. Should not be empty.")

    # Pydantic v2 的写法
    @field_validator('function_name')
    @classmethod
    def function_name_must_not_be_empty(cls, v: str) -> str:
        if not v or not v.strip():
            raise ValueError('function_name must not be empty')
        return v

class GetFunctionDocumentationTool(BaseTool):
    """
    A tool to retrieve detailed documentation for a specific DolphinDB function.
    """
    name = "get_function_documentation"
    description = (
        "Retrieves the full documentation for a specific DolphinDB function from the knowledge base. "
        "Use this when you are unsure about a function's arguments, behavior, or see an error message "
        "related to a specific function call (e.g., 'wrong number of arguments')."
    )
    args_schema = GetFunctionDocumentationInput

    def __init__(self, project_path: str):
        """
        Initializes the tool.
        
        Args:
            project_path: The root path of the project, used to locate the 'documentation/funcs' folder.
        """
        # 路径现在指向 funcs 子目录
        self.base_doc_path = os.path.join(project_path, "documentation", "funcs")

    @ensure_generator
    def run(self, args: GetFunctionDocumentationInput) -> ExecutionResult:
        """
        Reads and returns the content of a function's documentation file 
        from the structured directory: documentation/funcs/{first_char}/{function_name}.md
        """
        function_name = args.function_name.strip()

        # 1. 获取函数名的首字母
        first_char = function_name[0].lower()
        
        # 2. 检查首字母是否是合法的目录名 (例如，a-z)
        if not 'a' <= first_char <= 'z':
            return f"Error: Invalid function name '{function_name}'. It must start with a letter."

        # 3. 构造完整的文件路径
        #    函数名本身也统一转为小写，以匹配文件名
        doc_file_path = os.path.join(self.base_doc_path, first_char, f"{function_name.lower()}.md")

        # 4. 检查文件是否存在
        if not os.path.exists(doc_file_path):
            return ExecutionResult(
                success=False,
                error_message=f"Documentation for function '{function_name}' not found in '{doc_file_path}'."
            )

        try:
            # 5. 读取文件内容
            with open(doc_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # 6. 检查文件内容是否为空
            if not content or not content.strip():
                return (
                    f"Warning: Documentation for function '{function_name}' was found, "
                    "but the file is empty. No details are available."
                )

            return ExecutionResult(
                success=True,
                executed_script=f"Documentation for function '{function_name}' retrieved successfully.",
                data=(
                f"--- Documentation for {function_name} ---\n\n"
                f"{content}\n\n"
                f"--- End of Documentation ---"
            )
            )
        except Exception as e:
            return ExecutionResult(
                success=False,
                executed_script=f"Failed to read documentation for function '{function_name}'.",
                error_message=str(e)
            )
        

class SearchKnowledgeBaseInput(ToolInput):
    """Input model for the knowledge base search tool."""
    query: str = Field(description="The specific error message, function name, or concept to search for in the documentation and code snippets.")
    conversation_history: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="The recent conversation history as a list of message objects, to provide context for the search."
    )
class SearchKnowledgeBaseTool(BaseTool):
    """
    A tool to search the project's knowledge base (RAG system) for relevant information.
    Use this tool when you encounter an error, are unsure about a function's usage,
    or need more context to solve a problem. It provides context from documentation and code examples.
    """
    name = "search_knowledge_base"
    description = (
        "Searches the knowledge base for documentation and code examples related to a query. "
        "This is the primary tool for debugging and self-correction."
    )
    args_schema = SearchKnowledgeBaseInput

    def __init__(self):
        self.project_path = "/home/jzchen/ddb_agent"
        self.index_file = "/home/jzchen/ddb_agent/.ddb_agent/file_index.json"
        self.index_manager = TextIndexManager("/home/jzchen/ddb_agent", self.index_file)
        self.context_builder = ContextBuilder(model_name=os.getenv("LLM_MODEL"), max_window_size=128000)


        @llm.prompt()
        def _default_chat_prompt(conversation_history: List[Dict[str, str]]):
            """"
            You are a helpful DolphinDB assistant. Continue the conversation naturally.
            The user's latest message is the last one in the history.
            请严格按照相关资料来回答用户问题，如果没有搜到相关资料，请回答我不清楚,千万不要臆造"
            """
        
        self.chat_prompt_func = _default_chat_prompt

        pass

    def _get_files_content(self, file_paths: List[str]) -> List[Document]:
        """Reads file contents and creates Document objects."""
        sources = []
        for file_path in file_paths:
            full_path = os.path.join(self.project_path, file_path)
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                # 从索引中获取预先计算好的token数
                index_info = self.index_manager.get_index_by_filepath(file_path)
                tokens = index_info.tokens if index_info else -1 # 如果找不到索引，则让Document自己计算
                sources.append(Document(file_path, content, tokens))
            except Exception as e:
                print(f"Warning: Could not read file {file_path}: {e}")
        return sources

    def retrieve(self, query: str, top_k: int = 5) -> Generator[AnyRagStatus, None, List[Document]]:
        """
        Retrieves the most relevant documents using a two-step process.
        """
        yield RagStart(message="🚀 Starting retrieval process...")
        
        # 1. 从所有索引源获取全部索引数据
        all_text_indices = self.index_manager.get_all_indices()
        all_indices = all_text_indices

        if not all_indices:
            print("No indices found to search from.")
            return []

        yield RagIndexLoaded(message=f"🔍 Found {len(all_indices)} total index items.", total_items=len(all_indices))
        yield RagSelectionStart(
            message=f"Phase 1: Selecting candidates using llm strategy...",
            strategy="llm"
        )
        
        # 2. 阶段一：粗筛 (Candidate Selection)
        candidates: List[BaseIndexModel]
        
        selector = LLMCandidateSelector(all_indices, self.index_manager)
        candidates = yield from selector.select(query, max_workers=10) # 并发LLM筛选
        

        if not candidates:
            print("No relevant candidates found after initial selection.")
            return []
        
        yield RagRerankStart(
            message="Phase 2: Using LLM to re-rank candidates...",
            candidate_count=len(candidates)
        )

        # 我们直接从已排序的候选中选取前 top_k 个
        final_candidates = candidates[:top_k]
        final_identifiers = [c.file_path for c in final_candidates]
        
        yield RagEnd(
            message=f"Retrieval process completed. Selected top {len(final_identifiers)} documents.",
            final_document_count=len(final_identifiers)
        )

        # 4. 根据最终的标识符列表，获取并返回文件/文本块内容
        return self._get_files_content(final_identifiers)

    @ensure_generator
    def run(self, args: SearchKnowledgeBaseInput) -> Generator[Any, None, ExecutionResult]:
        """
        Executes the RAG retrieval process.
        """
        try:
            # DDBRAG.retrieve 是一个生成器，我们需要消耗它来获取最终结果
            relevant_files = yield from  self.retrieve(args.query, top_k=3)
            
            #print("Relevant files:", relevant_files)

            # 2. 上下文构建
            system_prompt = "You are a helpful assistant. Your task is to answer the user's question strictly based on the information found in the provided official DolphinDB documentation links. If you cannot find a direct answer in the provided links, you must state that you cannot find a built-in function for this purpose based on the documentation. Do not use any prior knowledge."

            final_messages = self.context_builder.build(
                system_prompt=system_prompt,
                conversations=args.conversation_history,
                file_sources=relevant_files,
                task_type='chat',
                file_pruning_strategy='extract'
            )

            # 3. 调用 LLM 并流式传输结果 (yields StreamChunk)
            assistant_response_gen = self.chat_prompt_func(
                conversation_history=final_messages
            )

            final_llm_response = None
            try:
                while True:
                    chunk = next(assistant_response_gen)
                    yield chunk # 将 StreamChunk 直接冒泡给调用者
            except StopIteration as e:
                final_llm_response = e.value

            print(final_llm_response)

            # 4. 任务结束，yield 最终消息对象
            if final_llm_response and getattr(final_llm_response, 'success', False):
                final_message_obj = {
                    "role": "assistant",
                    "content": final_llm_response.content
                }

                return ExecutionResult(
                    success=True,
                    data=f"Found the following relevant information:\n\n{str(final_message_obj)}"
                )
            elif final_llm_response: # 如果失败
                print("failed:", final_llm_response)
                return ExecutionResult(
                    success=False,
                    error_message=f"Failed to search knowledge base: {getattr(final_llm_response, 'error_message', 'Unknown error')}"
                )
                
        except Exception as e:

            return ExecutionResult(
                success=False,
                error_message=f"Failed to search knowledge base: {str(e)}"
            )
===== ./agent/tools/interactive_tools.py =====

from .tool_interface import BaseTool, ToolInput, ExecutionResult, ensure_generator
from pydantic import Field
from typing import List, Dict, Any, Optional

class AskForHumanFeedbackInput(ToolInput):
    message: str = Field(description="An informative message explaining the error or the situation requiring feedback.")
    options: Optional[List[str]] = Field(None, description="A list of suggested actions for the user to choose from, e.g., ['Retry', 'Abort'].")

class AskForHumanFeedbackTool(BaseTool):
    name = "ask_for_human_feedback"
    description = (
        "Presents an error or an unexpected situation to the user and asks for guidance on how to proceed. "
        "Use this tool ONLY when an action has failed and you need help deciding the next step."
    )
    args_schema = AskForHumanFeedbackInput

    @ensure_generator
    def run(self, args: AskForHumanFeedbackInput) -> ExecutionResult:
        """
        Signals the executor to pause and wait for human input regarding an error.
        """
        interactive_data = {
            "_is_interactive_request": True,
            "type": "USER_INTERACTION",
            "message": args.message,
            "options": args.options or [],
            "_is_error_feedback": True 
        }
        
        return ExecutionResult(
            success=True,
            data=interactive_data
        )

class PlanModeResponseInput(ToolInput):
    response: str = Field(description="The response to provide to the user, typically a plan or a clarifying question.")
    options: Optional[List[str]] = Field(None, description="A list of 2-5 options for the user to choose from, simplifying their response.")

class PlanModeResponseTool(BaseTool):
    name = "plan_mode_response"
    description = "Presents a plan or asks a clarifying question to the user and waits for their input. This is the primary tool for communication in PLAN_MODE."
    args_schema = PlanModeResponseInput

    @ensure_generator
    def run(self, args: PlanModeResponseInput) -> ExecutionResult:
        """
        This tool's execution is special. It doesn't perform an action but signals
        the executor to pause and wait for user interaction.
        """
        # The result data is a structured dictionary that the executor will interpret
        # as an interactive prompt for the user.
        interactive_data = {
            "_is_interactive_request": True, # A special flag to identify this tool's purpose
            "type": "USER_INTERACTION",
            "message": args.response,
            "options": args.options or []
        }
        
        return ExecutionResult(
            success=True,
            data=interactive_data
        )
===== ./agent/tools/tool_interface.py =====

from abc import ABC, abstractmethod
from typing import List, Any, Dict
from pydantic import BaseModel, Field

from agent.execution_result import ExecutionResult

import types
def ensure_generator(func):
    def wrapper(*args, **kwargs):
        result = func(*args, **kwargs)
        # 如果是生成器，直接返回
        if isinstance(result, types.GeneratorType):
            return result
        # 否则包装成生成器，并通过 return 把结果返回
        def gen():
            return result
            yield  # 这里永远不会执行，仅为了语法合法
        return gen()
    return wrapper

class ToolInput(BaseModel):
    pass

class BaseTool(ABC):
    name: str
    description: str
    args_schema: type[BaseModel]

    @abstractmethod
    def run(self, args: BaseModel) -> ExecutionResult:
        """Executes the tool and returns a string representation of the result."""
        pass

    def get_definition(self) -> dict:
        """Returns a JSON-serializable definition of the tool for the LLM."""
        return {
            "name": self.name,
            "description": self.description,
            "parameters": self.args_schema.model_json_schema()
        }
    
# class PresentPlanInput(ToolInput):
#     plan: List[Dict[str, Any]] = Field(description="A step-by-step plan to accomplish the task. Each step should be a dictionary with 'step', 'thought', and 'action' details.")
#     summary: str = Field(description="A brief summary of the plan and a question to the user asking for approval.")

# # 新增一个特殊的工具类
# class PresentPlanTool(BaseTool):
#     """
#     这个工具比较特殊，它不执行外部操作，
#     而是作为AI在PLAN模式下与用户沟通的唯一渠道。
#     它的“执行结果”实际上是触发一个等待用户输入的流程。
#     """
#     name = "present_plan_and_ask_for_approval"
#     description = "Presents a detailed, step-by-step plan to the user and asks for their approval to proceed. This should be the ONLY tool used when in PLAN_MODE."
#     args_schema = PresentPlanInput

#     def run(self, args: PresentPlanInput) -> str:
#         # 这个工具的 run 方法在服务器端实际上不会返回什么有意义的东西。
#         # 它只是一个信号，告诉调用方（Agent逻辑）：“现在应该把这个计划展示给用户，并等待他们的'yes'或'no'”。
#         # 我们返回一个结构化的字符串，方便上层逻辑解析。
#         import json
#         return json.dumps({
#             "plan_presented": True,
#             "plan": args.plan,
#             "summary": args.summary
#         })
===== ./agent/tools/web_tools.py =====
import os
import requests
import json
from pydantic import Field

from agent.execution_result import ExecutionResult 
from .tool_interface import BaseTool, ToolInput, ensure_generator

# 1. 定义工具的输入参数
class BaiduSearchInput(ToolInput):
    """Input model for the Baidu AI Search tool."""
    query: str = Field(description="The search query, e.g., '顺络电子基本面' or 'NVIDIA financial report'.")

# 2. 创建工具类
class BaiduSearchTool(BaseTool):
    """
    A tool to search the web using Baidu Qianfan AI Search.
    """
    name = "baidu_ai_search"
    description = (
        "Searches the web using Baidu Qianfan AI Search. "
        "Ideal for finding financial data, news, and fundamental analysis, especially for the Chinese market. "
        "Use this to get up-to-date information that is not in the local database."
    )
    args_schema = BaiduSearchInput

    @ensure_generator
    def run(self, args: BaiduSearchInput) -> ExecutionResult:
        """
        Executes the Baidu AI Search and returns the result.
        """
        # 从环境变量中安全地获取 Bearer Token
        bearer_token = os.getenv("BAIDU_QIANFAN_TOKEN")
        if not bearer_token:
            return ExecutionResult(
                success=False,
                error_message="BAIDU_QIANFAN_TOKEN environment variable is not set. The administrator needs to configure it."
            )

        url = "https://qianfan.baidubce.com/v2/ai_search/chat/completions"

        payload = json.dumps({
            "messages": [{"role": "user", "content": args.query}],
            "search_source": "baidu_search_v2",
            "search_recency_filter": "week"
        }, ensure_ascii=False)

        headers = {
            'Authorization': f'Bearer {bearer_token}',
            'Content-Type': 'application/json'
        }

        try:
            response = requests.request("POST", url, headers=headers, data=payload.encode("utf-8"))
            response.raise_for_status()  # 检查HTTP错误 (e.g., 401, 403, 500)
            
            response_data = response.json()
            
            # 提取关键结果给 Agent
            # 您可以根据需要调整返回给Agent的数据结构
            processed_data = {
                "summary": response_data.get("result", "No summary found."),
                "raw_response": response_data # 也可以包含完整响应供Agent参考
            }
            
            return ExecutionResult(
                success=True,
                data=processed_data
            )
        except requests.exceptions.HTTPError as http_err:
             return ExecutionResult(success=False, error_message=f"HTTP error occurred: {http_err} - Response: {response.text}")
        except requests.exceptions.RequestException as e:
            return ExecutionResult(success=False, error_message=f"API request failed: {e}")
        except json.JSONDecodeError:
            return ExecutionResult(success=False, error_message=f"Failed to decode API response. Raw response: {response.text}")
===== ./agent/tools/file_tools.py =====
# FILE: ./agent/tools/file_tools.py

import os
import shlex
import asyncio
import json # 引入 json
from concurrent.futures import Future
from typing import Tuple, List

from pydantic import Field

from agent.tools.tool_interface import BaseTool, ToolInput, ensure_generator
from agent.execution_result import ExecutionResult
from core.config import settings

from alibabacloud_tea_openapi import models as open_api_models
from alibabacloud_eci20180808.client import Client as EciClient
from alibabacloud_tea_util import models as util_models
from alibabacloud_openapi_util.client import Client as OpenApiUtilClient
import aiohttp


class WriteFileInput(ToolInput):
    file_path: str = Field(description="The relative path...")
    content: str = Field(description="The full content...")

class WriteFileTool(BaseTool):
    name = "write_file"
    description = "Creates or overwrites a file in the workspace..."
    args_schema = WriteFileInput

    def __init__(
        self, 
        region_id: str, 
        container_group_id: str, 
        main_event_loop, 
        container_name: str = "code-server-container"
    ):
        self.region_id = region_id
        self.container_group_id = container_group_id
        self.container_name = container_name
        self.main_loop = main_event_loop
        # 在工具实例化时创建一次 client，而不是每次调用都创建
        self.aliyun_client = self._create_aliyun_client(region_id)

    def _create_aliyun_client(self, region_id: str) -> EciClient:
        config = open_api_models.Config(
            access_key_id=settings.ALIYUN_ACCESS_KEY_ID,
            access_key_secret=settings.ALIYUN_ACCESS_KEY_SECRET,
            region_id=region_id
        )
        config.endpoint = f'eci.{region_id}.aliyuncs.com'
        return EciClient(config)

    async def _execute_command_in_container(self, command_list: List[str]) -> Tuple[bool, str]:
        """
        An async method that uses the direct dictionary method to execute a command.
        """
        # 1. 将命令列表序列化为 JSON 字符串
        command_str = json.dumps(command_list)
        
        
        print(f"\n[ASYNC TASK] Preparing to execute command list: {command_list}")
        
        # 2. 手动构建 query 字典
        query = {
            "RegionId": self.region_id,
            "ContainerGroupId": self.container_group_id,
            "ContainerName": self.container_name,
            "Command": command_str
        }

        # 3. 构建 OpenApiRequest
        req = open_api_models.OpenApiRequest(
            query=OpenApiUtilClient.query(query)
        )
        params = open_api_models.Params(
            action='ExecContainerCommand',
            version='2018-08-08',
            protocol='HTTPS',
            pathname='/',
            method='POST',
            auth_type='AK',
            style='RPC',
            req_body_type='formData',
            body_type='json'
        )
        runtime = util_models.RuntimeOptions()

        try:
            # 4. 使用 call_api_async 调用
            response_dict = await self.aliyun_client.call_api_async(params, req, runtime)
            
            websocket_uri = response_dict.get('body', {}).get('WebSocketUri')
            if not websocket_uri:
                return False, "API response did not contain a WebSocket URI."
            
            # 5. WebSocket 连接逻辑 
            output_buffer = []
            async with aiohttp.ClientSession() as session:
                async with session.ws_connect(websocket_uri, timeout=30) as ws:
                    async for msg in ws:
                        if msg.type == aiohttp.WSMsgType.BINARY:
                            output_buffer.append(msg.data[1:].decode('utf-8', errors='ignore'))
                        elif msg.type == aiohttp.WSMsgType.ERROR:
                            return False, f"Exec WebSocket connection error: {ws.exception()}"
            
            full_output = "".join(output_buffer)
            return True, full_output

        except Exception as e:
            return False, str(e)

    
    def run(self, args: WriteFileInput) -> ExecutionResult:
        dir_path = os.path.dirname(args.file_path)
        command_str = ""
        if dir_path:
            command_str += f"mkdir -p {shlex.quote(dir_path)} && "
        command_str += f"echo {shlex.quote(args.content)} > {shlex.quote(args.file_path)} && echo WRITE_SUCCESS"
        
        command_list_for_api = ["/bin/sh", "-c", command_str]

        try:
            coro = self._execute_command_in_container(command_list_for_api)
            future: Future = asyncio.run_coroutine_threadsafe(coro, self.main_loop)
            success, output = future.result(timeout=60)
            
            print(f"[FT]:{success},{output}")
            
            if success and "WRITE_SUCCESS" in output:
                return ExecutionResult(success=True, data=f"Successfully wrote to {args.file_path}.")
            elif success:
                return ExecutionResult(success=False, error_message=f"Command may have failed. Output: {repr(output)}")
            else:
                return ExecutionResult(success=False, error_message=f"API call failed. Details: {output}")

        except Exception as e:
            print(f"[FT]:{e}")
            return ExecutionResult(success=False, error_message=f"Error in run_coroutine_threadsafe: {type(e).__name__}: {str(e)}")
===== ./agent/execution_result.py =====
# file: agent/execution_result.py

from pydantic import BaseModel
from typing import Any, Optional

class ExecutionResult(BaseModel):
    """
    A structured container for the result of executing a code script.
    """
    success: bool
    executed_script: Optional[str] = None 
    data: Optional[Any] = None
    error_message: Optional[str] = None
    
    # 我们甚至可以预留一些元数据字段，比如执行耗时等
    metadata: Optional[dict] = None
===== ./agent/tool_manager.py =====
from typing import Any, Dict, Generator, Optional, List
from agent.execution_result import ExecutionResult
from agent.tools.tool_interface import BaseTool

class ToolNotFoundError(Exception):
    pass

class ToolArgumentValidationError(Exception):
    pass

class ToolManager:
    """增强的工具管理器"""
    
    def __init__(self, tools: list[BaseTool]):
        self.tools = {tool.name: tool for tool in tools}
       

    def get_tool_definitions(self, mode: str = "ACT") -> list[dict]:
        """
        根据模式返回不同的工具定义列表。
        PLAN 模式下，只暴露 'plan_mode_response'。
        """
        all_tools = list(self.tools.values())

        if mode == 'PLAN': 
            return [tool.get_definition() for tool in all_tools ]
        else: # ACT mode
            # 在ACT模式下，暴露所有实际操作的工具
            base_tools = [tool.get_definition() for tool in all_tools if tool.name != 'plan_mode_response']
            return base_tools

    def call_tool(self, tool_name: str, args: dict) ->  Generator[Any, None, ExecutionResult]:
        # 调用常规工具
        if tool_name not in self.tools:
            raise ToolNotFoundError(f"Tool '{tool_name}' not found.")
    
        tool = self.tools[tool_name]
        try:
            # Pydantic v2 用 model_validate
            validated_args = tool.args_schema.model_validate(args)
            z = yield from  tool.run(validated_args)
            return z
        except Exception as e:
            raise ToolArgumentValidationError(f"Error validating arguments for tool '{tool_name}': {e}") from e
     
    def get_all_tool_names(self) -> List[str]:
        """获取所有可用工具名称"""
        tool_names = list(self.tools.keys())
        
        if self.enable_mcp and self.mcp_tool_adapter:
            try:
                mcp_tools = self.mcp_tool_adapter.get_available_tools()
                for tool in mcp_tools:
                    tool_names.append(tool["name"].replace(".", "_"))
            except Exception:
                pass
        
        return tool_names
    
    def get_tool_help(self, tool_name: str) -> Optional[str]:
        """获取工具帮助信息"""
        # 检查常规工具
        if tool_name in self.tools:
            tool = self.tools[tool_name]
            return f"**{tool.name}**\n\n{tool.description}"
        
        # 检查MCP工具
        if self.enable_mcp and self.mcp_tool_adapter:
            try:
                mcp_tool_name = tool_name.replace("_", ".")
                return self.mcp_tool_adapter.get_tool_help(mcp_tool_name)
            except Exception:
                pass
        
        return None
    
    async def cleanup(self):
        """清理资源"""
        if self.enable_mcp and self.mcp_server_manager:
            try:
                await self.mcp_server_manager.stop_all_servers()
            except Exception as e:
                print(f"Warning: Failed to stop MCP servers during cleanup: {e}")
===== ./agent/code_executor.py =====
import os
import time
from typing import Optional
from dotenv import load_dotenv

from agent.execution_result import ExecutionResult
from db.database_session import DatabaseSession 

load_dotenv()

class CodeExecutor:
    """
    Safely executes DolphinDB scripts and returns structured results.
    It encapsulates the database session management.
    """
    def __init__(self, 
                 host: Optional[str] = None, 
                 port: Optional[int] = None, 
                 user: Optional[str] = None, 
                 password: Optional[str] = None, 
                 logger=None):
        """
        Initializes the CodeExecutor. Credentials can be passed directly or
        loaded from environment variables (DDB_HOST, DDB_PORT, DDB_USER, DDB_PASSWORD).
        """
        self.host = host or os.getenv("DDB_HOST")
        self.port = port or int(os.getenv("DDB_PORT", "8848")) # Provide a default port
        self.user = user or os.getenv("DDB_USER", "admin")
        self.password = password or os.getenv("DDB_PASSWORD", "123456") #
        self.logger = logger
        self.session = None

        if not all([self.host, self.port, self.user, self.password]):
            raise ValueError(
                "DolphinDB connection details are missing. "
                "Please provide them as arguments or set environment variables."
            )

        if self.logger:
            self.logger.info(f"CodeExecutor initialized for DolphinDB at {self.host}:{self.port}")

    def _get_session(self):
        if self.session is None:
            self.session = DatabaseSession(self.host, self.port, self.user, self.password, logger=self.logger)
            self.session.connect()
        return self.session

    def close(self):
        """关闭持久化的 session。"""
        if self.session:
            self.session.close()
            self.session = None

    def run(self, script: str) -> ExecutionResult:
        """
        Executes a DolphinDB script and captures its output or error.

        Args:
            script: The DolphinDB script to execute.

        Returns:
            An ExecutionResult object containing the outcome.
        """
        if not script or not script.strip():
            return ExecutionResult(
                success=False,
                executed_script=script,
                error_message="Error: Empty script provided."
            )

        if self.logger:
            self.logger.info("Executing DolphinDB script...")
            # For security, you might want to log only a snippet of the script
            self.logger.debug(f"Script to run:\n---\n{script[:500]}...\n---")

        start_time = time.time()
        
        try:
            db_session = self._get_session()
            success, result = db_session.execute(script)
            end_time = time.time()
            duration = end_time - start_time

            if success:
                if self.logger:
                    self.logger.info(f"Script executed successfully in {duration:.2f} seconds.")
                return ExecutionResult(
                    success=True,
                    data=result,
                    executed_script=script,
                    metadata={"execution_duration_seconds": duration}
                )
            else:
                # `db_session.execute` 已经将异常转换为字符串
                error_str = str(result)
                if self.logger:
                    self.logger.warning(f"Script execution failed after {duration:.2f} seconds. Error: {error_str}")
                return ExecutionResult(
                    success=False,
                    error_message=error_str,
                    executed_script=script,
                    metadata={"execution_duration_seconds": duration}
                )

        except Exception as e:
            # 这是一个兜底的异常捕获，以防 DatabaseSession 本身出现问题（比如连接失败）
            end_time = time.time()
            duration = end_time - start_time
            error_msg = f"A critical error occurred in CodeExecutor: {str(e)}"
            if self.logger:
                self.logger.error(error_msg, exc_info=True)
            return ExecutionResult(
                success=False,
                error_message=error_msg,
                executed_script=script,
                metadata={"execution_duration_seconds": duration}
            )
===== ./rag/__init__.py =====

===== ./rag/base_manager.py =====
# file: ddb_agent/rag/base_manager.py
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
import json
import os
import threading
from typing import Any, List, Optional, Union

import pydantic

from rag.types import BaseIndexModel
from rag.types import CodeIndex, ProjectIndex
from .retrieval_result import RetrievalResult

class BaseIndexManager(ABC):
    """
    Abstract base class for all index managers (for code, text, etc.).
    """
    def __init__(self, project_path: str, index_file: str):
        self.project_path = project_path
        self.index_path = os.path.join(project_path, index_file)
        self.project_index: ProjectIndex = self._load_index()
        self._index_lock = threading.Lock()

    def get_all_indices(self) -> List[BaseIndexModel]:
        return self.project_index.files

    @abstractmethod
    def build_index(
        self, 
        file_extensions: Optional[Union[str, List[str]]] = None,
        max_workers: int = 4
    ):
        """
        Builds the index for a specific type of content.
        """
        pass

    # @abstractmethod
    # def retrieve(self, query: str, top_k: int = 5) -> List[RetrievalResult]:
    #     """
    #     Retrieves the most relevant content for a given query.
    #     """
    #     pass

    from typing import List, Union

    def _add_or_update_and_save(self, new_item: Union[BaseIndexModel, List[BaseIndexModel]]):
        """
        A thread-safe template method to add/update an item or a list of items in the index and save.
        This method contains the algorithm skeleton.
        """
        with self._index_lock:
            # If new_item is a single BaseIndexModel, convert it to a list
            if isinstance(new_item, BaseIndexModel):
                new_item = [new_item]
            
            # Iterate over the list and update the index
            for item in new_item:
                self._update_internal_index(item)
            
            # Save the index
            self._save_index()

    @abstractmethod
    def _update_internal_index(self, new_item: BaseIndexModel):
        """
        (Abstract) Hook for subclasses to implement their specific index update logic.
        This method will be called within a thread-safe context.
        """
        pass

    def _load_index(self) -> ProjectIndex:
        if os.path.exists(self.index_path):
            try:
                return ProjectIndex.model_validate_json(open(self.index_path, 'r', encoding='utf-8').read())
            
            # 捕获 Pydantic 的 ValidationError
            except (json.JSONDecodeError, pydantic.ValidationError) as e:
                print(f"Warning: Could not load or validate index file. Starting fresh. Error: {e}")
                return ProjectIndex(files=[])
                
        # 如果文件不存在，返回一个空的 ProjectIndex
        return ProjectIndex(files=[])
    
    @abstractmethod
    def _process_single_file(self, file_path: str) -> Optional[BaseIndexModel]:
        """"""
        pass

    @abstractmethod
    def get_relevant_files(self, query: str, top_k: int = 5) -> List[str]:
        pass

    def _save_index(self):
        """Saves the current project index to the file."""
        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)

        with open(self.index_path, 'w', encoding='utf-8') as f:
            f.write(self.project_index.model_dump_json(indent=2))
    
    def get_index_by_filepath(self, file_path: str) -> Optional[Any]:
        """
        Retrieves the CodeIndex object for a given file path.

        Args:
            file_path: The relative path of the file.

        Returns:
            The CodeIndex object if found, otherwise None.
        """
        # 为了提高查找效率，我们可以构建一个临时的字典映射
        # 如果频繁调用，可以将这个映射作为 DDBIndexManager 的一个属性
        index_map = {f.file_path: f for f in self.project_index.files}
        return index_map.get(file_path)

    def _discover_files(self, file_extensions: Optional[Union[str, List[str]]]) -> List[str]:
        """
        A common utility to discover files, shared by subclasses.
        """
        import os
        discovered_files = []
        ignore_dirs = {'.git', 'node_modules', 'dist', 'build', '__pycache__', '.idea', '.vscode', '.ddb_agent'}
        
        if file_extensions:
            if isinstance(file_extensions, str):
                extensions = [file_extensions]
            else:
                extensions = file_extensions
            extensions = [ext if ext.startswith('.') else '.' + ext for ext in extensions]
        else:
            extensions = None

        for root, dirs, files in os.walk(self.project_path, topdown=True):
            dirs[:] = [d for d in dirs if d not in ignore_dirs]
            
            for file in files:
                if extensions is None or any(file.endswith(ext) for ext in extensions):
                    full_path = os.path.join(root, file)
                    #relative_path = os.path.relpath(full_path, self.project_path)
                    discovered_files.append(full_path)
        
        return discovered_files
    
    def _calculate_md5(self, file_path: str) -> str:
        """Calculates the MD5 hash of a file."""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                # Read file in chunks to handle large files efficiently
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except FileNotFoundError:
            return "" # Return empty string if file not found

    
    def build_index(self, file_extensions: Optional[Union[str, List[str]]] = None, max_workers: int = 4):
        """
        Automatically discovers files and builds or updates the index.

        Args:
            file_extensions: A list of file extensions to include (e.g., ['.dos', '.txt']).
                             If None, all files will be processed.
        """
        # 1. 自动发现文件
        print(f"Discovering files with extensions: {file_extensions or 'All'} in '{self.project_path}'...")
        file_paths_to_index = self._discover_files(file_extensions)

        if not file_paths_to_index:
            print("No matching files found to index.")
            return
    

        print(f"Found {len(file_paths_to_index)} files to build index...")

        # 2. 使用 ThreadPoolExecutor 并发处理文件
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_filepath = {executor.submit(self._process_single_file, fp): fp for fp in file_paths_to_index}
            
            processed_count = 0
            for future in as_completed(future_to_filepath):
                file_path = future_to_filepath[future]
                processed_count += 1
                try:
                    # 获取单个文件的处理结果
                    result_index = future.result()
                    
                    if result_index:
                        # --- 核心修改在这里 ---
                        # 调用线程安全的更新和保存方法
                        self._add_or_update_and_save(result_index)
                        print(f"[{processed_count}/{len(file_paths_to_index)}] Indexed and saved: {file_path}")
                    else:
                        print(f"[{processed_count}/{len(file_paths_to_index)}] Failed to index (skipped): {file_path}")
                except Exception as exc:
                    print(f"[{processed_count}/{len(file_paths_to_index)}] Exception for {file_path}: {exc}")
        
        
        print("Index building complete. All processed files have been saved incrementally.")
    
===== ./rag/candidate_selector.py =====
# file: ddb_agent/rag/candidate_selector.py

from concurrent.futures import ThreadPoolExecutor, as_completed
import json
from typing import Generator, List, Dict, Any
import re
from llm.llm_client import StreamChunk
from rag.base_manager import BaseIndexManager
from rag.rag_status import AnyRagStatus, RagError, RagSelectionEnd, RagSelectionProgress
from rag.types import BaseIndexModel
from token_counter import count_tokens
from utils.json_parser import parse_json_string
from utils.tokenizer import smart_tokenize 
from llm.llm_prompt import llm


class CandidateSelector:
    """
    Selects a subset of candidate index items based on simple, fast matching algorithms.
    """
    def __init__(self, all_index_items: List[Dict[str, Any]], index_manager: BaseIndexManager):
        """
        Args:
            all_index_items: A list of all index items (dicts, not Pydantic models for speed).
        """
        self.all_items = all_index_items
        self.index_manager = index_manager

    def select_by_keyword(self, query: str, top_n: int = 50) -> List[Dict[str, Any]]:
        """
        Selects candidates by scoring them based on keyword matches in their metadata.
        """
        query_keywords = smart_tokenize(query)

        print("query_keywords:",query_keywords)

        if not query_keywords:
            return []

        scored_items = []
        for item in self.all_items:
            score = 0
            
            # 1. 检查文件名
            item_name = item.file_path
            for keyword in query_keywords:
                if keyword in item_name.lower():
                    score += 5 # 文件名匹配权重更高

            # 2. 检查摘要
            summary = item.summary
            summary_lower = summary.lower()
            for keyword in query_keywords:
                if keyword in summary_lower:
                    score += 2
            
            # 3. 检查符号
            searchable_terms =  item.keywords
            searchable_terms_lower = {term.lower() for term in searchable_terms}

            for keyword in query_keywords:
                if keyword in searchable_terms_lower:
                    score += 3 # 符号/关键词匹配权重较高
            
            if score > 0:
                scored_items.append({'item': item, 'score': score})
        
        # 按分数排序并返回 top_n
        scored_items.sort(key=lambda x: x['score'], reverse=True)
        
        return [scored['item'] for scored in scored_items[:top_n]]
    

class LLMCandidateSelector:
    """
    Selects candidates by using an LLM to screen chunks of the index in parallel.
    """
    # 每个发给LLM的块，其token上限
    MAX_TOKENS_PER_CHUNK = 128000 # 假设使用一个大的窗口模型进行筛选

    def __init__(self, all_index_items: List[BaseIndexModel],  index_manager: BaseIndexManager):
        self.all_items = all_index_items
        self.index_manager = index_manager

    # 指定使用chat模型快一些
    @llm.prompt()
    def _select_from_chunk_prompt(self, user_query: str, index_chunk_json: str) -> str:
        """
        You are an expert retrieval assistant. Your task is to analyze a CHUNK of a project's index, 
        identify items relevant to the user's query, and assign a relevance score.

        User Query:
        {{ user_query }}

        Index Chunk (a subset of the project's total index):
        <INDEX_CHUNK>
        {{ index_chunk_json }}
        </INDEX_CHUNK>

        Instructions:
        1.  Review each item in the index chunk.
        2.  Compare the user's query against each item's metadata.
        3.  For each relevant item, assign a relevance score from 1 (least relevant) to 10 (most relevant).
        4.  If an item is not relevant, do not include it in the output.

        Your response MUST be a JSON array of objects. Each object must contain "file_path" and "score".
        If no items in this chunk are relevant, return an empty list [].
        Do not add any other text or explanations.

        Example Response:
        ```json
        [
            { "file_path": "path/to/code_file.dos", "score": 9 },
            { "file_path": "document.md-chunk_5", "score": 7 }
        ]
        ```
        """
   

    def _split_index_into_chunks(self) -> List[List[Dict]]:
        """Splits the list of all index items into manageable chunks."""
        chunks = []
        current_chunk = []
        current_tokens = 0

        for item in self.all_items:
            # 将Pydantic对象转为字典
            item_dict = {
                "file_path": item.file_path,
                "summary": item.summary,
                "keywords": item.keywords
            }   
            item_str = json.dumps(item_dict, ensure_ascii=False)
            item_tokens = count_tokens(item_str)

            if current_tokens + item_tokens > self.MAX_TOKENS_PER_CHUNK and current_chunk:
                chunks.append(current_chunk)
                current_chunk = []
                current_tokens = 0
            
            current_chunk.append(item_dict)
            current_tokens += item_tokens
        
        if current_chunk:
            chunks.append(current_chunk)
            
        return chunks

    def _select_candidates_from_chunk(self, query: str, index_chunk: List[Dict]) -> List[Dict]:
        """The target function for each thread, processing one chunk."""
        try:
            chunk_json_str = json.dumps(index_chunk, indent=2, ensure_ascii=False)

            prompt_generator = self._select_from_chunk_prompt(
                user_query=query,
                index_chunk_json=chunk_json_str
            )

            # RAG阶段，忽略筛选相关的LLM请求处理的中间过程
            try:
                while True:
                    chunk = next(prompt_generator)
                    pass 
            except StopIteration as e:
                response_str = e.value

            # 解析LLM返回的JSON列表
            relevant_items_in_chunk = parse_json_string(response_str.content)
            return relevant_items_in_chunk  if isinstance(relevant_items_in_chunk, list) else []
        except Exception as e:
            print(f"Error processing an index chunk with LLM: {e}")
            return []

    def select(self, query: str, max_workers: int = 4) -> Generator[AnyRagStatus, None, List[BaseIndexModel]]:
        """
        Performs the parallel selection process.
        """
        # 1. 分块
        index_chunks = self._split_index_into_chunks()
        if not index_chunks:
            return []
        
        yield RagSelectionProgress(
            message=f"Phase 1: Split index into {len(index_chunks)} chunks for parallel filter.",
            processed_count=0,
            total_count=len(index_chunks),
            found_count=0
        )
        # 2. 并行筛选
        all_candidates_with_scores = []
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            processed_chunks = 0
            future_to_chunk_index = {
                executor.submit(self._select_candidates_from_chunk, query, chunk): i
                for i, chunk in enumerate(index_chunks)
            }
            
            total_count = len(index_chunks)
            for future in as_completed(future_to_chunk_index):
                chunk_index = future_to_chunk_index[future]
                try:
                    result = future.result()
                    processed_chunks += 1
                    if result:
                        all_candidates_with_scores.extend(result)
                        yield RagSelectionProgress(
                            message=f"Phase 1: filter index  {processed_chunks}/{total_count} chunks...",
                            processed_count=processed_chunks,
                            total_count=total_count,
                            found_count=len(result)
                        )
                except Exception as exc:
                    yield RagError(
                        message=f"Chunk {chunk_index + 1} processing failed.",
                        step="selection",
                        error_details=str(exc)
                    )

        best_candidates = {}
        for cand in all_candidates_with_scores:
            path = cand.get("file_path")
            score = cand.get("score", 0)
            if path:
                if path not in best_candidates or score > best_candidates[path].get("score", 0):
                    best_candidates[path] = cand
        
        # 3. 排序：根据分数从高到低排序
        sorted_candidates_list = sorted(best_candidates.values(), key=lambda x: x.get("score", 0), reverse=True)

        # 4. 转换：将排序后的字典列表转换回 BaseIndexModel 对象列表
        final_candidates = []
        for item_dict in sorted_candidates_list:
            # get_index_by_filepath 依然可以复用
            index_item = self.index_manager.get_index_by_filepath(item_dict["file_path"])
            if index_item:
                final_candidates.append(index_item)

        yield RagSelectionEnd(
            message=f"Phase 1: Found and ranked {len(final_candidates)} unique candidates.",
            candidate_count=len(final_candidates)
        )
        return final_candidates

===== ./rag/code_index_manager.py =====
# file: ddb_agent/rag/index_manager.py (重构后)

import os
import json
import pydantic
from typing import List, Dict, Optional, Union
from token_counter import count_tokens

from utils.json_parser import parse_json_string # 引入 token 计数器
from .types import CodeIndex, ProjectIndex, Symbol
from llm.llm_prompt import llm

import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

from .base_manager import BaseIndexManager

class CodeIndexManager(BaseIndexManager):
    """
    Manages the creation, loading, and querying of the project index for DolphinDB.
    Handles large files by splitting them into chunks and using a map-reduce approach.
    """
    # 为每个块设置一个合理的 token 上限，留出余量给 prompt
    MAX_TOKENS_PER_CHUNK = 60*1000

    def __init__(self, project_path: str, index_file: str = ".ddb_agent/index.json"):
        super().__init__(project_path, index_file)


    def _save_index(self):
        """Saves the code project index to disk."""
        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
        with open(self.index_path, 'w', encoding='utf-8') as f:
            f.write(self.project_index.model_dump_json(indent=2))

    def _update_internal_index(self, new_item: CodeIndex):
        """Updates the in-memory ProjectIndex with a new CodeIndex."""
        if not isinstance(new_item, CodeIndex):
            print(f"Warning: CodeIndexManager received an item of wrong type: {type(new_item)}")
            return
            
        found = False
        for i, existing_file in enumerate(self.project_index.files):
            if existing_file.file_path == new_item.file_path:
                self.project_index.files[i] = new_item
                found = True
                break
        if not found:
            self.project_index.files.append(new_item)

    # _process_single_file, _discover_files 和所有 @llm.prompt 方法保持不变
    def _process_single_file(self, file_path: str) -> Optional[CodeIndex]:
        # ... (原有实现不变) ...
        full_path = os.path.join(self.project_path, file_path)
        
        try:
            with open(full_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            total_tokens = count_tokens(content)
            final_file_index = None

            if total_tokens <= self.MAX_TOKENS_PER_CHUNK:
                response_str = self._create_file_index_prompt_for_small_file(
                    file_path=file_path, file_content=content
                )
                json_content = parse_json_string(response_str)
                json_content["tokens"] = total_tokens
                final_file_index = CodeIndex(**json_content)
            else:
                chunks = self._split_code_into_chunks(content)
                
                chunk_summaries, all_symbols = [], []
                for i, chunk in enumerate(chunks):
                    response_str = self._summarize_chunk_prompt(
                        file_path=file_path, code_chunk=chunk
                    )
                    json_content = parse_json_string(response_str)
                    chunk_result = json.loads(json_content)
                    chunk_summaries.append(chunk_result["file_summary"])
                    all_symbols.extend([Symbol(**s) for s in chunk_result["symbols"]])

                summaries_str = "\n".join(f"- {s}" for s in chunk_summaries)
                response_str = self._reduce_summaries_prompt(
                    file_path=file_path, chunk_summaries=summaries_str
                )
                json_content = parse_json_string(response_str)
                final_summary = json.loads(json_content)["final_summary"]

                final_file_index = CodeIndex(
                    file_path=file_path,
                    file_summary=final_summary,
                    symbols=all_symbols,
                    is_aggregated=True,
                    chunk_count=len(chunks),
                    tokens = total_tokens
                )

            return final_file_index

        except Exception as e:
            print(f"Error processing file {file_path}: {e}")
            return None

    # 新增：代码切分辅助函数
    def _split_code_into_chunks(self, code: str) -> List[str]:
        """
        Splits code into chunks that are smaller than MAX_TOKENS_PER_CHUNK,
        trying to split at logical boundaries (e.g., function definitions).
        """
        chunks = []
        current_chunk_lines = []
        lines = code.splitlines(keepends=True)
        
        for line in lines:
            current_chunk_lines.append(line)
            # 简单地通过行数或token数来切分，更复杂的可以基于AST
            # 这里我们使用token计数，并在达到阈值时切分
            current_content = "".join(current_chunk_lines)
            if count_tokens(current_content) > self.MAX_TOKENS_PER_CHUNK:
                # 当超出时，我们将当前块（除了最后一行）作为一个chunk
                chunk_to_add = "".join(current_chunk_lines[:-1])
                if chunk_to_add: # 避免添加空块
                   chunks.append(chunk_to_add)
                # 新的块从最后一行开始
                current_chunk_lines = [line]

        # 添加最后一个剩余的块
        if current_chunk_lines:
            chunks.append("".join(current_chunk_lines))
        
        return chunks

    @llm.prompt()
    def _summarize_chunk_prompt(self, file_path: str, code_chunk: str) -> str:
        """
        You are an expert DolphinDB code analyst.
        The following code is a CHUNK from a larger file: {{ file_path }}.

        Code Chunk:
        ```dolphiindb
        {{ code_chunk }}
        ```

        Please provide a concise summary of THIS CHUNK's main purpose and functionality.
        Also, list all key symbols (functions, modules, etc.) defined in THIS CHUNK.

        Your response MUST be in the following JSON format.
        ```json
        {
          "file_summary": "A brief summary of this specific code chunk.",
          "symbols": [
            {"name": "symbol_name", "type": "function"}
          ]
        }
        ```
        """
        pass

    @llm.prompt()
    def _reduce_summaries_prompt(self, file_path: str, chunk_summaries: str) -> str:
        """
        You are an expert DolphinDB code analyst.
        I have analyzed a large file, '{{ file_path }}', by splitting it into several chunks.
        Here are the summaries for each chunk:

        <CHUNK_SUMMARIES>
        {{ chunk_summaries }}
        </CHUNK_SUMMARIES>

        Your task is to synthesize these individual chunk summaries into a single, cohesive, high-level summary for the entire file.
        The final summary should describe the overall purpose and functionality of '{{ file_path }}'.

        Your response should be a single JSON string with one key "final_summary".
        Example:
        ```json
        {
            "final_summary": "A comprehensive summary of the entire file."
        }
        ```
        """
        pass
    
    # 原有的_create_file_index_prompt保持不变，但我们可以称之为处理小文件的方法
    @llm.prompt()
    def _create_file_index_prompt_for_small_file(self, file_path: str, file_content: str) -> str:
        # ... (和上一版中的 _create_file_index_prompt 完全相同) ...
        """
        You are an expert DolphinDB code analyst.
        Your task is to analyze the following DolphinDB script file and extract its metadata.

        File Path: {{ file_path }}

        File Content:
        ```dolphiindb
        {{ file_content }}
        ```

        Please provide a concise summary of this file's main purpose and functionality.
        Also, list all key symbols defined in this script, including functions, modules, and any important global variables.

        Your response MUST be in the following JSON format. Do not add any extra text or explanations.
        ```json
        {
          "file_path": "{{ file_path }}",
          "file_summary": "A brief, one-sentence summary of the file's purpose.",
          "symbols": [
            {"name": "symbol_name_1", "type": "function"},
            {"name": "symbol_name_2", "type": "module"},
            ...
          ]
        }
        ```
        """
        pass

    # get_relevant_files 方法保持不变
    @llm.prompt()
    def _get_relevant_files_prompt(self, user_query: str, index_content: str) -> str:
        # ... (和上一版中的 _get_relevant_files_prompt 完全相同) ...
        """
        You are a smart file retrieval assistant for a DolphinDB project.
        Based on the user's query, your task is to identify the most relevant files from the project index.

        User Query:
        {{ user_query }}

        Project Index (contains a list of all files with their summaries and defined symbols):
        ```json
        {{ index_content }}
        ```

        Analyze the project index and determine which files are most relevant to answering the user's query.
        Consider file summaries and the symbols they contain.

        Your response MUST be a JSON list of strings, containing only the file paths of the relevant files.
        Example:
        ```json
        [
            "path/to/relevant_file1.dos",
            "path/to/relevant_file2.dos"
        ]
        ```
        Return an empty list if no files seem relevant. Do not add any other text.
        """
        pass

    def get_relevant_files(self, query: str, top_k: int = 5) -> List[str]:
        """
        Uses LLM to find the most relevant files for a given query based on the index.
        """
        if not self.project_index.files:
            return []

        index_json_str = self.project_index.model_dump_json()

        response_str = self._get_relevant_files_prompt(
            user_query=query,
            index_content=index_json_str
        )
        
        try:
            relevant_files = parse_json_string(response_str)
            print("relevant_files:", relevant_files)
            # You might want to respect top_k here if the LLM returns too many files
            return relevant_files[:top_k]
        except (json.JSONDecodeError, IndexError) as e:
            print(f"Error parsing LLM response for relevant files: {e}")
            return []
===== ./rag/rag_entry.py =====
# file: ddb_agent/rag/rag_entry.py

import json
import os

from context.pruner import Document, get_pruner
from llm.llm_prompt import llm
from typing import Any, Dict, Generator, List

from llm.models import ModelManager
from rag.rag_status import AnyRagStatus, RagEnd, RagError, RagIndexLoaded, RagRerankEnd, RagRerankStart, RagSelectionStart, RagStart
from rag.types import BaseIndexModel
from utils.json_parser import parse_json_string
from .code_index_manager import CodeIndexManager
from .text_index_manager import TextIndexManager
from .candidate_selector import CandidateSelector, LLMCandidateSelector 

class DDBRAG:
    """
    A simple RAG implementation for DolphinDB agent.
    """
    def __init__(self, project_path: str, index_file: str = None, selection_strategy: str = 'llm' ):
        """ 
            selection_strategy: 处理索引过大场景
        """
        self.project_path = project_path
        self.index_file = index_file or os.path.join(project_path, ".ddb_agent", "file_index.json")
        self.index_manager = TextIndexManager(project_path=project_path, index_file = self.index_file)
        self.selection_strategy = selection_strategy

    @llm.prompt()
    def _chat_prompt(self, user_query: str, context_files: str) -> str:
        """
        You are a world-class DolphinDB expert.
        Answer the user's query based on the provided file contexts.
        Be concise, accurate, and provide code examples where appropriate.

        Here are the relevant files and their content:
        <CONTEXT>
        {{ context_files }}
        </CONTEXT>

        User Query:
        {{ user_query }}

        Your Answer:
        """
        pass

    @llm.prompt()
    def _chat_without_context(self, user_query: str) -> str:

        """
        You are a world-class DolphinDB expert.
        Answer the user's query without any file context.
        Be concise, accurate, and provide code examples where appropriate.

        User Query:
        {{ user_query }}

        Your Answer:
        """
        pass

    @llm.prompt()
    def _chat_with_context(self, user_query: str, context_files: str) -> str:
        """
        You are a world-class DolphinDB expert.
        Answer the user's query based on the provided file contexts.
        Be concise, accurate, and provide code examples where appropriate.

        Here are the relevant files and their content:
        <CONTEXT>
        {{ context_files }}
        </CONTEXT>

        User Query:
        {{ user_query }}

        Your Answer:
        """
        pass

    @llm.prompt()
    def _rerank_candidates_prompt(self, user_query: str, candidates_json: str) -> str:
        """
        You are an expert re-ranking system. Your task is to analyze a list of candidate documents
        and select the most relevant ones for the given user query.

        User Query:
        {{ user_query }}

        Candidate Documents (metadata only):
        <CANDIDATES>
        {{ candidates_json }}
        </CANDIDATES>

        Please review the candidates and return a JSON list of the file paths (`module_name` or `source_document`) 
        or chunk IDs (`chunk_id`) of the TOP 5 most relevant items. Order them from most to least relevant.

        Your response MUST be a valid JSON list of strings.
        Example:
        ```json
        [
            "path/to/code_file.dos",
            "document.md-chunk_5",
            "utils/another_file.dos"
        ]
        ```
        """
        pass

    def _get_files_content(self, file_paths: List[str]) -> List[Document]:
        """Reads file contents and creates Document objects."""
        sources = []
        for file_path in file_paths:
            full_path = os.path.join(self.project_path, file_path)
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                # 从索引中获取预先计算好的token数
                index_info = self.index_manager.get_index_by_filepath(file_path)
                tokens = index_info.tokens if index_info else -1 # 如果找不到索引，则让Document自己计算
                sources.append(Document(file_path, content, tokens))
            except Exception as e:
                print(f"Warning: Could not read file {file_path}: {e}")
        return sources
    
    
    def retrieve(self, query: str, top_k: int = 5) -> Generator[AnyRagStatus, None, List[Document]]:
        """
        Retrieves the most relevant documents using a two-step process.
        """
        yield RagStart(message="🚀 Starting retrieval process...")
        
        # 1. 从所有索引源获取全部索引数据
        all_text_indices = self.index_manager.get_all_indices()
        all_indices = all_text_indices

        if not all_indices:
            print("No indices found to search from.")
            return []

        yield RagIndexLoaded(message=f"🔍 Found {len(all_indices)} total index items.", total_items=len(all_indices))
        yield RagSelectionStart(
            message=f"Phase 1: Selecting candidates using '{self.selection_strategy}' strategy...",
            strategy=self.selection_strategy
        )
        
        # 2. 阶段一：粗筛 (Candidate Selection)
        candidates: List[BaseIndexModel]
        if self.selection_strategy == 'llm':
            selector = LLMCandidateSelector(all_indices, self.index_manager)
            candidates = yield from selector.select(query, max_workers=10) # 并发LLM筛选
        elif self.selection_strategy == 'keyword':
            selector = CandidateSelector(all_indices)
            candidates = selector.select_by_keyword(query, top_n=50) # 关键词筛选
        else:
            raise ValueError(f"Unknown selection strategy: {self.selection_strategy}")

        if not candidates:
            print("No relevant candidates found after initial selection.")
            return []
        
        yield RagRerankStart(
            message="Phase 2: Using LLM to re-rank candidates...",
            candidate_count=len(candidates)
        )

        # 我们直接从已排序的候选中选取前 top_k 个
        final_candidates = candidates[:top_k]
        final_identifiers = [c.file_path for c in final_candidates]
        
        yield RagEnd(
            message=f"Retrieval process completed. Selected top {len(final_identifiers)} documents.",
            final_document_count=len(final_identifiers)
        )

        # 4. 根据最终的标识符列表，获取并返回文件/文本块内容
        return self._get_files_content(final_identifiers)
       
===== ./rag/rag_status.py =====
from pydantic import BaseModel, Field
from typing import Optional, Literal, Union

# --- 基础状态类 ---

class BaseRagStatus(BaseModel):
    """所有 RAG 状态更新的基类。"""
    type: Literal["rag_status"] = "rag_status"
    message: str = Field(description="向用户展示的友好信息。")

# --- 具体的状态子类 ---

class RagStart(BaseRagStatus):
    """表示RAG流程开始。"""
    subtype: Literal["start"] = "start"

class RagIndexLoaded(BaseRagStatus):
    """表示索引已加载。"""
    subtype: Literal["index_loaded"] = "index_loaded"
    total_items: int = Field(description="加载的索引项总数。")

class RagSelectionStart(BaseRagStatus):
    """表示候选选择阶段开始。"""
    subtype: Literal["selection_start"] = "selection_start"
    strategy: str = Field(description="使用的选择策略（如 'llm', 'keyword'）。")

class RagSelectionProgress(BaseRagStatus):
    """表示候选选择正在进行中（尤其适用于并行LLM筛选）。"""
    subtype: Literal["selection_progress"] = "selection_progress"
    processed_count: int
    total_count: int
    found_count: int

class RagSelectionEnd(BaseRagStatus):
    """表示候选选择阶段结束。"""
    subtype: Literal["selection_end"] = "selection_end"
    candidate_count: int = Field(description="此阶段后找到的候选文档数量。")

class RagRerankStart(BaseRagStatus):
    """表示LLM重排阶段开始。"""
    subtype: Literal["rerank_start"] = "rerank_start"
    candidate_count: int = Field(description="进入重排阶段的候选文档数量。")

class RagRerankEnd(BaseRagStatus):
    """表示LLM重排阶段结束。"""
    subtype: Literal["rerank_end"] = "rerank_end"
    final_count: int = Field(description="重排后最终选择的文档数量。")

class RagContentRetrievalStart(BaseRagStatus):
    """表示开始获取最终文档的内容。"""
    subtype: Literal["content_retrieval"] = "content_retrieval"
    document_count: int

class RagEnd(BaseRagStatus):
    """表示整个RAG流程成功结束。"""
    subtype: Literal["end"] = "end"
    final_document_count: int

class RagError(BaseRagStatus):
    """表示RAG流程中发生错误。"""
    subtype: Literal["error"] = "error"
    step: str = Field(description="发生错误的步骤（如 'selection', 'rerank'）。")
    error_details: Optional[str] = None

from typing import Annotated

AnyRagStatus = Annotated[
    Union[
        RagStart,
        RagIndexLoaded,
        RagSelectionStart,
        RagSelectionProgress,
        RagSelectionEnd,
        RagRerankStart,
        RagRerankEnd,
        RagContentRetrievalStart,
        RagEnd,
        RagError,
    ],
    Field(discriminator="subtype"),
]
===== ./rag/retrieval_result.py =====
# file: ddb_agent/rag/retrieval_result.py
from pydantic import BaseModel
from typing import List, Optional

class RetrievalResult(BaseModel):
    """
    A unified data structure for returning retrieval results from any index manager.
    """
    source: str # The path to the original source file or document
    content: str # The retrieved content (can be a full file or a chunk)
    score: float # The relevance score of this result
    metadata: Optional[dict] = None # For extra info, like line numbers
===== ./rag/text_index_manager.py =====
# file: ddb_agent/rag/text_index_manager.py

import os
import json
from typing import List, Tuple
from llm.llm_prompt import llm
from token_counter import count_tokens
from utils.json_parser import parse_json_string
from .types import TextChunkIndex
from utils.text_extractor import extract_text_from_file
from .base_manager import BaseIndexManager
from llm.models import ModelManager

class TextIndexManager(BaseIndexManager):
    """Manages indexing and retrieval for text documents."""

    MAX_TOKENS_PER_CHUNK = 100*1000

    def __init__(self, project_path: str, index_file: str = ".ddb_agent/text_index"):
        super().__init__(project_path, index_file)

    @llm.prompt(model="deepseek-chat")
    def _create_index_for_small_file(self, file_path: str, file_content: str):
        """
        你是一位专业的文档分析专家。你的任务是处DolphinDB文档，并为其提取用于搜索引擎的关键元数据。

        源文档 (Source Document): {{ source_document }}
        分块位置 (Chunk Location): Lines {{ start_line }} to {{ end_line }}
        
        文本分块内容 (Text Chunk Content):
        <CONTENT>
        {{ content }}
        </CONTENT>

        请执行以下操作：
        1.  **总结 (Summarize)**：用一个简洁的句子概括这个文本片段的核心要点。
        2.  **关键词 (Keywords)**：提取3-5个相关的关键词。
        3.  **虚拟问题 (Hypothetical Question)**：构思一个清晰、单一的问题，这个文本片段可以直接回答该问题。这个问题将用于搜索。

        你的回答**必须**遵循以下 JSON 格式。不要添加任何额外的文字或解释。
        ```json
        {
          "file_path": "{{ file_path }}",
          "chunk_id": "0"
          "source_document": "{{ source_document }}",
          "start_line": {{ start_line }},
          "end_line": {{ end_line }},
          "summary": "对文本片段内容的简洁摘要。",
          "keywords": ["关键词1", "关键词2", "关键词3"],
          "hypothetical_question": "一个该文本片段可以回答的问题。"
        }
        ```
        """
        return {
            "file_path": file_path,
            "chunk_id": "0",
            "source_document": file_path,
            "start_line": 1,
            "end_line": len(file_content.splitlines()),
            "content": file_content.replace('"', '\\"'),  # Escape double quotes for JSON
        }


        
    @llm.prompt()
    def _create_chunk_index_prompt(
        self, 
        file_path: str,
        chunk_id: str,
        source_document: str,
        start_line: int,
        end_line: int,
        content: str
    ) -> dict:
        """
        You are an expert document analyst. Your task is to process a chunk of text from a larger document 
        and extract key metadata for a search index.

        Source Document: {{ source_document }}
        Chunk Location: Lines {{ start_line }} to {{ end_line }}
        
        Text Chunk Content:
        <CONTENT>
        {{ content }}
        </CONTENT>

        Please perform the following actions:
        1.  **Summarize**: Write a concise, one-sentence summary of this chunk's main point.
        2.  **Keywords**: Extract 3-5 relevant keywords.
        3.  **Hypothetical Question**: Formulate a single, clear question that this chunk of text could directly answer. This question will be used for searching.

        Your response MUST be in the following JSON format. Do not add any extra text or explanations.
        ```json
        {
          "file_path": "{{ file_path }}",
          "chunk_id": "{{ chunk_id }}",
          "source_document": "{{ source_document }}",
          "start_line": {{ start_line }},
          "end_line": {{ end_line }},
          "summary": "A concise summary of the chunk's content.",
          "keywords": ["keyword1", "keyword2", "keyword3"],
          "hypothetical_question": "A question that this chunk can answer."
        }
        ```
        """
        # 使用 `e`过滤器来转义JSON字符串中的特殊字符
        return {"content": content.replace('"', '\\"')}

    # --- 文本分块与索引构建 ---

    def _chunk_text(self, text: str, chunk_size: int = MAX_TOKENS_PER_CHUNK, overlap: int = 128) -> List[Tuple[int, int, str]]:
        """
        Splits text into chunks based on line count, with overlap.
        Returns a list of (start_line, end_line, content) tuples.
        """
        lines = text.splitlines()
        chunks = []
        start_index = 0
        

        # 1 English character ≈ 0.3 token.
        # 1 Chinese character ≈ 0.6 token.
        while start_index < len(lines):
            end_index = min(start_index + chunk_size, len(lines))
            chunk_lines = lines[start_index:end_index]
            
            # Line numbers are 1-based
            chunks.append((start_index + 1, end_index, "\n".join(chunk_lines)))
            
            start_index += (chunk_size - overlap)
            if start_index >= end_index: # Ensure progress
                start_index = end_index

        return chunks
    
    
    def _save_index(self):
        """Saves the code project index to disk."""
        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
        with open(self.index_path, 'w', encoding='utf-8') as f:
            #f.write(self.project_index.model_dump_json(indent=2))
            json.dump(self.project_index.model_dump(), f, indent=2, ensure_ascii=False)

    def _update_internal_index(self, new_item: TextChunkIndex):
        """Updates the in-memory ProjectIndex with a new CodeIndex."""
        if not isinstance(new_item, TextChunkIndex):
            print(f"Warning: TextIndexManager received an item of wrong type: {type(new_item)}")
            return
            
        found = False
        for i, existing_file in enumerate(self.project_index.files):
            if existing_file.file_path == new_item.file_path:
                self.project_index.files[i] = new_item
                found = True
                break
        if not found:
            self.project_index.files.append(new_item)


    def _process_single_file(self, file_path: str):
        """
        Extracts text from a file, chunks it, and creates an index for each chunk.
        """
        print(f"Starting to index file: {file_path}")

        index_info = self.get_index_by_filepath(file_path)
        if index_info:
            print(f"  - File {file_path} is already indexed. Skipping.")
            return None
        
        try:
            # 1. 从文件提取纯文本
            full_text = extract_text_from_file(file_path)
            if not full_text:
                print(f"  - No text could be extracted from {file_path}. Skipping.")
                return
        except Exception as e:
            print(f"  - Error extracting text from {file_path}: {e}")
            return
        

        try:
            total_tokens = count_tokens(full_text)

            chunks = []
            final_chunk_index = []
            if total_tokens <= self.MAX_TOKENS_PER_CHUNK:
                response_generator = self._create_index_for_small_file(
                    file_path=file_path,
                    file_content=full_text
                )
                
                try:
                    while True:
                        part = next(response_generator)
                        pass
                except StopIteration as e:
                    response_str = e.value 

                json_content = parse_json_string(response_str.content)
                final_chunk_index = [TextChunkIndex(**json_content)]
            else:
                chunks = self._chunk_text(full_text)

                # 3. 为每个块创建索引 (可以并发处理)
                for i, (start_line, end_line, content) in enumerate(chunks):
                    chunk_id = f"{os.path.basename(file_path)}-chunk_{i}"
                    print(f"    - Indexing chunk {i+1}/{len(chunks)} (lines {start_line}-{end_line})...")
                    
                    try:
                        # 调用 LLM 生成索引元数据
                        response_str = self._create_chunk_index_prompt(
                            file_path=file_path,
                            chunk_id=chunk_id,
                            source_document=file_path,
                            start_line=start_line,
                            end_line=end_line,
                            content=content
                        )

                        json_content = parse_json_string(response_str)
                        chunk_index = TextChunkIndex(**json_content)
                        
                        final_chunk_index.append(chunk_index)
                    except Exception as e:
                        print(f"    - Error indexing chunk {i+1}: {e}")
                

            self._add_or_update_and_save(final_chunk_index)
            
            # with open(self.index_path, 'w', encoding='utf-8') as f:
            #     # Pydantic v2. `model_dump` is the new `dict`
            #     json.dump([idx.model_dump() for idx in final_chunk_index], f, indent=2)
                
            print(f"  - Successfully indexed {len(final_chunk_index)} chunks. Saved to {self.index_path}")
            return final_chunk_index
        except Exception as e:
            import traceback
            traceback.print_exc()


    @llm.prompt()
    def _get_relevant_files_prompt(self, user_query: str, index_content: str) -> str:
        # ... (和上一版中的 _get_relevant_files_prompt 完全相同) ...
        """
        You are a smart file retrieval assistant for a DolphinDB project.
        Based on the user's query, your task is to identify the most relevant files from the project index.

        User Query:
        {{ user_query }}

        Project Index (contains a list of all files with their summaries and defined symbols):
        ```json
        {{ index_content }}
        ```

        Analyze the project index and determine which files are most relevant to answering the user's query.
        Consider file summaries and the symbols they contain.

        Your response MUST be a JSON list of strings, containing only the file paths of the relevant files.
        Example:
        ```json
        [
            "path/to/relevant_file1.dos",
            "path/to/relevant_file2.dos"
        ]
        ```
        Return an empty list if no files seem relevant. Do not add any other text.
        """
        pass

    def get_relevant_files(self, query: str, top_k: int = 5) -> List[str]:
        """
        Uses LLM to find the most relevant files for a given query based on the index.
        """
        if not self.project_index.files:
            return []

        index_json_str = self.project_index.model_dump_json()

        response_str = self._get_relevant_files_prompt(
            user_query=query,
            index_content=index_json_str
        )
        
        try:
            relevant_files = parse_json_string(response_str)
            print("relevant_files:", relevant_files)
            # You might want to respect top_k here if the LLM returns too many files
            return relevant_files[:top_k]
        except (json.JSONDecodeError, IndexError) as e:
            print(f"Error parsing LLM response for relevant files: {e}")
            return []
===== ./rag/types.py =====
# file: ddb_agent/rag/types.py

import pydantic
from typing import Annotated, List, Literal, Optional, Union

class BaseIndexModel(pydantic.BaseModel):
    """
    Marker base class for all index model types (e.g., CodeIndex, TextChunkIndex).
    """
    pass


class Symbol(pydantic.BaseModel):
    """
    Represents a code symbol (e.g., class, function).
    """
    name: str = pydantic.Field(description="The name of the symbol.")
    type: str = pydantic.Field(description="The type of the symbol (e.g., 'class', 'function').")

class CodeIndex(BaseIndexModel):
    """
    Represents the indexed metadata for a single source file.
    """
    model_type: Literal["code"] = "code"
    file_path: str = pydantic.Field(description="The path to the source file.")
    file_summary: str = pydantic.Field(description="A brief summary of the file's purpose and functionality.")
    symbols: List[Symbol] = pydantic.Field(description="A list of key symbols defined in the file.")
    # Optional: You can add other metadata if needed, e.g., dependencies
    dependencies: Optional[List[str]] = pydantic.Field(default_factory=list, description="List of modules this file depends on.")
    is_aggregated: bool = pydantic.Field(default=False, description="Indicates if this index is an aggregation of multiple chunks.")
    chunk_count: Optional[int] = pydantic.Field(None, description="Number of chunks if aggregated.")
    tokens: Optional[int] = pydantic.Field(None, description="file tokens.")
    file_hash: Optional[str] = pydantic.Field(None, description="The MD5 hash of the file content at the time of indexing.")

class TextChunkIndex(BaseIndexModel):
    """
    Represents the indexed metadata for a single chunk of text.
    """
    model_type: Literal["text_chunk"] = "text_chunk"
    
    file_path: str = pydantic.Field(description="The path to the source file containing the chunk.")
    chunk_id: str = pydantic.Field(description="A unique identifier for the chunk (e.g., 'doc_name-chunk__0').")
    source_document: str = pydantic.Field(description="The path or name of the source document.")
    
    start_line: int = pydantic.Field(description="The starting line number of the chunk in the original document.")
    end_line: int = pydantic.Field(description="The ending line number of the chunk in the original document.")
    
    summary: str = pydantic.Field(description="A concise summary of the chunk's content.")
    keywords: List[str] = pydantic.Field(description="A list of keywords representing the chunk's topics.")
    hypothetical_question: Optional[str] = pydantic.Field(None, description="A representative question this chunk can answer.")
    tokens: Optional[int] = pydantic.Field(None, description="file tokens.")
    file_hash: Optional[str] = pydantic.Field(None, description="The MD5 hash of the file content at the time of indexing.")


class ProjectIndex(BaseIndexModel):
    """
    Represents the entire project's index, containing a collection of file/chunk indexes.
    """
    # 这是关键！
    # 我们告诉 Pydantic，files 是一个列表，列表中的每个元素
    # 都属于 AnyIndexModel (即 TextChunkIndex 或 CodeIndex)。
    # Pydantic 应该查看每个元素的 'model_type' 字段来决定使用哪个具体模型。
    files: List[Annotated[Union[TextChunkIndex, CodeIndex], pydantic.Field(discriminator='model_type')]]
    
    # 你还可以添加其他元数据
    project_name: Optional[str] = None
    version: str = "1.0"
===== ./test.py =====
def run_non_iter(x):
    return 123   # 普通对象，不可迭代

def run_iterable(x):
    return [1, 2, 3]  # 可迭代对象（list）

def run_generator(x):
    yield 1
    yield 2
    return 99   # 生成器有 return 值


def outer(run_func, name):
    print(f"\n=== 测试 {name} ===")
    try:
        z = yield from run_func(1)
        print(f"最终 z = {z!r}")
    except Exception as e:
        print(f"出错: {e!r}")


# 用来执行 outer 生成器
def test(run_func, name):
    gen = outer(run_func, name)
    for val in gen:
        print(f"yield 得到: {val!r}")


if __name__ == "__main__":
    test(run_non_iter, "非迭代对象")
    test(run_iterable, "可迭代对象")
    test(run_generator, "生成器")

===== ./tests/__init__.py =====

===== ./tests/locustfile.py =====
import os
import json
from locust import HttpUser, task, between
class BackendUser(HttpUser):
    """
    一个模拟用户，用于测试后端的登录和聊天功能。
    """
    # 每个模拟用户在执行任务之间会随机等待1到3秒
    wait_time = between(1, 3)
    # --- 配置 ---
    # 1. 设置被测试后端的主机地址
    # 推荐通过环境变量 API_HOST 设置, 例如: export API_HOST=http://127.0.0.1:8001
    # 如果不设置，则默认为 http://127.0.0.1:8001
    host = os.getenv("API_HOST", "http://127.0.0.1:8001")

    # 2. 设置用于登录的凭证
    # 同样，推荐使用环境变量 LOCUST_USER 和 LOCUST_PASSWORD
    username = os.getenv("LOCUST_USER", "admin")
    password = os.getenv("LOCUST_PASSWORD", "JZJZ112233")

    def on_start(self):
        """
        当一个 Locust 虚拟用户启动时被调用。
        每个虚拟用户都会在这里登录一次，并为后续的请求设置好认证头。
        """
        print(f"用户启动 - 正在为用户 {self.username} 登录...")
        
        # /token 接口需要 application/x-www-form-urlencoded 格式的数据
        response = self.client.post(
            "/api/v1/auth/token",
            data={"username": self.username, "password": self.password},
            name="/auth/token"  # 在Locust报告中为此请求命名
        )
        
        if response.status_code == 200:
            token = response.json().get("access_token")
            if token:
                print("登录成功，获取到Token。")
                # 为该用户的所有后续请求设置 Authorization 头
                self.client.headers["Authorization"] = f"Bearer {token}"
            else:
                print("登录响应成功，但未找到 access_token。")
                self.environment.runner.quit() # 严重错误，停止测试
        else:
            print(f"登录失败。状态码: {response.status_code}, 响应: {response.text}")
            # 如果登录失败，这个用户就无法继续测试，直接停止
            self.stop()

    @task
    def chat_request(self):
        """
        模拟用户向 /api/v1/chat 接口发送一个简单的聊天请求。
        这个任务会被用户反复执行。
        """
        # 这是一个对 /chat 接口有效的最小化、合法的请求体。
        # 我们在这里不提供 env_id，让后端进入“数据库无关”的模式，
        # 这样测试可以独立于任何活动的DolphinDB环境。
        chat_payload = {
            "conversation_history": [
                {
                    "role": "user",
                    "content": "Hello, can you tell me about DolphinDB? I want to know its core features."
                }
            ],
            "injected_context": None,
            "env_id": None
        }
        
        self.client.post(
            "/api/v1/chat",
            json=chat_payload,
            name="/chat"  # 将所有聊天请求在报告中归为一类
        )


===== ./test_write_file_tool.py =====
# FILE: ./test_write_file_tool.py

import asyncio
from dotenv import load_dotenv
import os
from concurrent.futures import ThreadPoolExecutor

# --- 关键：首先加载环境变量 ---
# 这会加载 .env 文件，让后续的 aiyun_service 和数据库连接能获取到凭证
load_dotenv()

# --- 引入我们需要测试的组件和依赖 ---
from agent.execution_result import ExecutionResult
from db.session import SessionLocal
from db import crud
from agent.tools.file_tools import WriteFileTool, WriteFileInput # 引入重构后的工具和输入模型

# --- 测试参数 ---
# !!! 请将这里的 env_id 替换为您要测试的、正在运行的环境的 ID !!!
TEST_ENV_ID = "ddb-env-b3008f5f" # <--- 修改这里

# --- 定义测试用例 ---
test_cases = [
    {
        "name": "Simple Text File Creation",
        "input": {
            "file_path": "ai_test_simple.txt",
            "content": "Hello from the final, working test script!"
        }
    }
]

async def run_test():
    """
    主测试函数, 用于协调和运行所有测试。
    """
    print("--- Starting WriteFileTool Standalone Test (Final, Corrected Version) ---")
    
    main_loop = asyncio.get_running_loop()
    print(f"Main event loop ID: {id(main_loop)}")
    
    # ... (从数据库获取 env 的逻辑不变)
    db = SessionLocal()
    try:
        env = await crud.get_environment(db, env_id=TEST_ENV_ID)
    finally:
        await db.close()
    
    if not env:
        print(f"FATAL: Environment with ID '{TEST_ENV_ID}' not found.")
        return
    # ...

    # 实例化 WriteFileTool，传递主事件循环
    write_tool = WriteFileTool(
        region_id=env.region_id,
        container_group_id=env.code_server_group_id,
        main_event_loop=main_loop
    )

    with ThreadPoolExecutor(max_workers=1) as executor:
        for i, case in enumerate(test_cases, 1):
            print(f"\n--- Running Test Case #{i}: {case['name']} ---")
            
            tool_input = WriteFileInput(**case['input'])
            
            # --- 关键修复：直接提交 run 方法，不使用 next() 或 lambda ---
            future = executor.submit(write_tool.run, tool_input)
            
            # 阻塞等待线程完成，并获取 run 方法返回的 ExecutionResult 对象
            result = future.result() 

            print(f"Execution Result for Test Case #{i}:")
            if result and isinstance(result, ExecutionResult):
                if result.success:
                    print(f"✅ SUCCESS: {result.data}")
                else:
                    print(f"❌ FAILED: {result.error_message}")
            else:
                print(f"❌ FAILED: Tool did not return a valid ExecutionResult. Got: {type(result)}")
    
    print("\n--- All Tests Finished ---")
    print(f"Please manually check the code-server UI for env '{TEST_ENV_ID}' to verify.")

if __name__ == "__main__":
    try:
        asyncio.run(run_test())
    except Exception as e:
        import traceback
        print(f"\nAn error occurred during the test run: {type(e).__name__}: {e}")
        traceback.print_exc()
===== ./debug_write_file.py =====
# FILE: ./debug_write_file.py

import asyncio
import os
import sys
from concurrent.futures import Future, ThreadPoolExecutor
from typing import Tuple, List

from dotenv import load_dotenv

# --- 阿里云 SDK Imports ---
from alibabacloud_eci20180808.client import Client as EciClient
from alibabacloud_eci20180808 import models as eci_models
from alibabacloud_tea_openapi import models as open_api_models
from alibabacloud_tea_util import models as util_models
import aiohttp
import shlex

# -----------------------------------------------------------------------------
# 1. 配置区域
# -----------------------------------------------------------------------------
load_dotenv()
REGION_ID = os.getenv("ALIYUN_REGION_ID")
ACCESS_KEY_ID = os.getenv("ALIYUN_ACCESS_KEY_ID")
ACCESS_KEY_SECRET = os.getenv("ALIYUN_ACCESS_KEY_SECRET")

# !!! 请确保这里的配置与您要测试的环境完全一致 !!!
CONTAINER_GROUP_ID = "eci-bp19jc3yztavttkor0p0" # 替换为您的 code-server ECI ID
CONTAINER_NAME = "code-server-container"       # 确认容器名称是否正确

# --- 测试用例 ---
FILE_PATH = "final_test.py"
FILE_CONTENT = "print('This is the final test and it must work.')"

# -----------------------------------------------------------------------------
# 2. 核心功能的自包含实现 (严格模拟 SDK 范式)
# -----------------------------------------------------------------------------

class AliyunECIExecutor:
    """一个独立的、严格遵循 Tea SDK 范式的 ECI 执行器"""

    def __init__(self, region_id, access_key_id, access_key_secret):
        self.region_id = region_id
        self.client = self._create_client(access_key_id, access_key_secret)
        print("AliyunECIExecutor initialized.")

    def _create_client(self, access_key_id, access_key_secret) -> EciClient:
        config = open_api_models.Config(
            access_key_id=access_key_id,
            access_key_secret=access_key_secret,
            region_id=self.region_id
        )
        config.endpoint = f'eci.{self.region_id}.aliyuncs.com'
        return EciClient(config)

    async def execute_command_in_container(self, container_group_id: str, container_name: str, command_list: List[str]) -> Tuple[bool, str]:
        print(f"\n[ASYNC TASK] Preparing to execute command list: {command_list}")

        # --- 核心修复 1: 严格按照 TeaModel 的方式创建和填充请求对象 ---
        request = eci_models.ExecContainerCommandRequest()
        request.region_id = self.region_id
        request.container_group_id = container_group_id
        request.container_name = container_name
        request.command = command_list # 直接传递列表
        
        runtime = util_models.RuntimeOptions()

        try:
            print(f"[ASYNC TASK] Calling Aliyun API with request object: {request.to_map()}")
            exec_response = await self.client.exec_container_command_with_options_async(request, runtime)
            print("[ASYNC TASK] Aliyun API call finished successfully.")
            
            websocket_uri = exec_response.body.web_socket_uri
            if not websocket_uri:
                return False, "API response did not contain a WebSocket URI."
            
            # ... 后续的 WebSocket 连接逻辑保持不变 ...
            print(f"[ASYNC TASK] Got WebSocket URI, connecting via aiohttp...")
            output_buffer = []
            async with aiohttp.ClientSession() as session:
                async with session.ws_connect(websocket_uri, timeout=30) as ws:
                    print("[ASYNC TASK] WebSocket connected. Receiving data...")
                    async for msg in ws:
                        if msg.type == aiohttp.WSMsgType.BINARY:
                            output_buffer.append(msg.data[1:].decode('utf-8', errors='ignore'))
                        elif msg.type == aiohttp.WSMsgType.ERROR:
                            return False, f"Exec WebSocket connection error: {ws.exception()}"
            
            full_output = "".join(output_buffer)
            print(f"[ASYNC TASK] WebSocket closed. Full output received: {repr(full_output)}")
            return True, full_output

        except Exception as e:
            print(f"[ASYNC TASK] An exception occurred during API call: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            return False, str(e)


def run_file_write_in_thread(
    main_loop, 
    region_id, access_key_id, access_key_secret, 
    container_group_id, container_name, 
    file_path, content
):
    print(f"\n[WORKER THREAD] Thread started.")
    
    # --- 核心修复 2: 构建原生命令列表，而不是 shell 字符串 ---
    # 我们将使用 /bin/sh -c '...' 的方式，这是最可靠的。
    # `shlex.quote` 依然是处理单引号和特殊字符的最佳方式。
    dir_path = os.path.dirname(file_path)
    command_str = ""
    if dir_path:
        command_str += f"mkdir -p {shlex.quote(dir_path)} && "
    command_str += f"echo {shlex.quote(content)} > {shlex.quote(file_path)} && echo WRITE_SUCCESS"
    
    # 最终传递给 API 的命令列表
    command_list_for_api = ["/bin/sh", "-c", command_str]
    
    print(f"[WORKER THREAD] Prepared command list for API: {command_list_for_api}")

    try:
        executor = AliyunECIExecutor(region_id, access_key_id, access_key_secret)
        coro = executor.execute_command_in_container(container_group_id, container_name, command_list_for_api)
        
        future: Future = asyncio.run_coroutine_threadsafe(coro, main_loop)
        
        print("[WORKER THREAD] Waiting for future.result()...")
        success, output = future.result(timeout=60)
        print(f"[WORKER THREAD] future.result() completed. Success: {success}")
        
        if success and "WRITE_SUCCESS" in output:
            return True, f"Successfully wrote to {file_path}"
        elif success:
            return False, f"Command may have failed (no success marker). Output: {repr(output)}"
        else:
            return False, f"API call failed. Details: {output}"

    except Exception as e:
        print(f"[WORK-THREAD] An exception occurred in the thread: {type(e).__name__}: {e}")
        return False, str(e)

# -----------------------------------------------------------------------------
# 3. 主执行逻辑
# -----------------------------------------------------------------------------

async def main():
    print("--- Independent ECI Exec Debug Script (Strict SDK Model) ---")
    if not all([ACCESS_KEY_ID, ACCESS_KEY_SECRET, REGION_ID]):
        print("FATAL: Aliyun credentials or region not found in .env file. Exiting.")
        return

    main_loop = asyncio.get_running_loop()
    print(f"[MAIN THREAD] Main event loop ID: {id(main_loop)}")

    with ThreadPoolExecutor(max_workers=1) as executor:
        result_future = main_loop.run_in_executor(
            executor,
            run_file_write_in_thread,
            main_loop,
            REGION_ID, ACCESS_KEY_ID, ACCESS_KEY_SECRET,
            CONTAINER_GROUP_ID, CONTAINER_NAME,
            FILE_PATH, FILE_CONTENT
        )
        
        print("[MAIN THREAD] Waiting for the worker thread to finish...")
        final_success, final_output = await result_future
        print("\n--- TEST SUMMARY ---")
        print(f"Success: {final_success}")
        print(f"Output: {final_output}")
        print("--------------------")

if __name__ == "__main__":
    if sys.version_info >= (3, 7):
        asyncio.run(main())
    else:
        loop = asyncio.get_event_loop()
        loop.run_until_complete(main())
===== ./debug_sdk_call.py =====
# FILE: ./debug_sdk_call.py

import asyncio
import os
from dotenv import load_dotenv

# --- 阿里云 SDK Imports ---
from alibabacloud_eci20180808.client import Client as EciClient
from alibabacloud_eci20180808 import models as eci_models
from alibabacloud_tea_openapi import models as open_api_models
from alibabacloud_tea_util import models as util_models
import aiohttp

# -----------------------------------------------------------------------------
# 1. 配置区域
# -----------------------------------------------------------------------------
load_dotenv()
REGION_ID = os.getenv("ALIYUN_REGION_ID")
ACCESS_KEY_ID = os.getenv("ALIYUN_ACCESS_KEY_ID")
ACCESS_KEY_SECRET = os.getenv("ALIYUN_ACCESS_KEY_SECRET")

# !!! 请确保这里的配置与您要测试的环境完全一致 !!!
CONTAINER_GROUP_ID = "eci-bp19jc3yztavttkor0p0" 
CONTAINER_NAME = "code-server-container"

async def main():
    print("--- Ultimate SDK Call Sanity Check ---")
    if not all([ACCESS_KEY_ID, ACCESS_KEY_SECRET, REGION_ID]):
        print("FATAL: Aliyun credentials or region not found in .env file. Exiting.")
        return

    # 1. 创建客户端
    config = open_api_models.Config(
        access_key_id=ACCESS_KEY_ID,
        access_key_secret=ACCESS_KEY_SECRET,
        region_id=REGION_ID
    )
    config.endpoint = f'eci.{REGION_ID}.aliyuncs.com'
    client = EciClient(config)
    print("Aliyun ECI Client created.")

    # 2. 创建最简单的请求对象
    request = eci_models.ExecContainerCommandRequest()
    request.region_id = REGION_ID
    request.container_group_id = CONTAINER_GROUP_ID
    request.container_name = CONTAINER_NAME
    request.command = ["ls", "-l", "/"] # 最简单、最不可能出错的命令

    runtime = util_models.RuntimeOptions()

    try:
        print(f"Calling API with request object: {request.to_map()}")
        exec_response = await client.exec_container_command_with_options_async(request, runtime)
        print("✅ Aliyun API call SUCCEEDED.")
        
        websocket_uri = exec_response.body.web_socket_uri
        print(f"WebSocket URI: {websocket_uri}")

        # 如果成功，尝试连接WebSocket获取输出
        if websocket_uri:
            async with aiohttp.ClientSession() as session:
                async with session.ws_connect(websocket_uri, timeout=30) as ws:
                    print("WebSocket connected. Output from 'ls -l /':")
                    async for msg in ws:
                        if msg.type == aiohttp.WSMsgType.BINARY:
                            print(msg.data[1:].decode('utf-8', errors='ignore'), end='')
            print("\n--- TEST PASSED ---")
        
    except Exception as e:
        print("\n❌ --- TEST FAILED ---")
        print(f"An exception occurred during the most basic API call:")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
===== ./debug_sdk_direct_call.py =====
# FILE: ./debug_sdk_direct_call.py

import asyncio
import os
from dotenv import load_dotenv

# --- 阿里云 SDK Imports ---
from alibabacloud_eci20180808.client import Client as EciClient
# 不再需要 eci_models
from alibabacloud_tea_openapi import models as open_api_models
from alibabacloud_tea_util import models as util_models
from alibabacloud_openapi_util.client import Client as OpenApiUtilClient # 引入这个工具类
import aiohttp

# -----------------------------------------------------------------------------
# 1. 配置区域
# -----------------------------------------------------------------------------
load_dotenv()
REGION_ID = os.getenv("ALIYUN_REGION_ID")
ACCESS_KEY_ID = os.getenv("ALIYUN_ACCESS_KEY_ID")
ACCESS_KEY_SECRET = os.getenv("ALIYUN_ACCESS_KEY_SECRET")

CONTAINER_GROUP_ID = "eci-bp1etbkiytfv88zpvxiu" 
CONTAINER_NAME = "code-server-container"

async def main():
    print("--- Ultimate SDK Call Sanity Check (Direct Dictionary Method) ---")
    if not all([ACCESS_KEY_ID, ACCESS_KEY_SECRET, REGION_ID]):
        print("FATAL: Aliyun credentials or region not found in .env file. Exiting.")
        return

    # --- 关键修复：完整地创建 config 对象 ---
    print(f"Creating Aliyun client for region: {REGION_ID}")
    config = open_api_models.Config(
        access_key_id=ACCESS_KEY_ID,
        access_key_secret=ACCESS_KEY_SECRET,
        region_id=REGION_ID
    )
    # 必须手动设置 endpoint
    config.endpoint = f'eci.{REGION_ID}.aliyuncs.com'
    
    client = EciClient(config)
    print("Aliyun ECI Client created successfully.")

    # 2. 手动构建 query 字典
    command_list = ["ls", "-l", "/"]
    
    # 阿里云API规定 Command 参数必须是 JSON 字符串格式
    import json
    command_str = json.dumps(command_list)
    
    query = {
        "RegionId": REGION_ID,
        "ContainerGroupId": CONTAINER_GROUP_ID,
        "ContainerName": CONTAINER_NAME,
        "Command": command_str # 将 JSON 字符串传递给 Command
    }
    
    # 3. 构建 OpenApiRequest
    req = open_api_models.OpenApiRequest(
        query=OpenApiUtilClient.query(query)
    )
    params = open_api_models.Params(
        action='ExecContainerCommand',
        version='2018-08-08',
        protocol='HTTPS',
        pathname='/',
        method='POST',
        auth_type='AK',
        style='RPC',
        req_body_type='formData',
        body_type='json'
    )
    
    runtime = util_models.RuntimeOptions()

    try:
        print(f"Calling API with direct query dictionary: {query}")
        response_dict = await client.call_api_async(params, req, runtime)
        
        print("✅ Aliyun API call SUCCEEDED (raw dictionary response).")
        
        websocket_uri = response_dict.get('body', {}).get('WebSocketUri')
        print(f"WebSocket URI: {websocket_uri}")

        if websocket_uri:
            print("Connecting to WebSocket to get command output...")
            async with aiohttp.ClientSession() as session:
                async with session.ws_connect(websocket_uri, timeout=30) as ws:
                    print("WebSocket connected. Output from 'ls -l /':")
                    async for msg in ws:
                        if msg.type == aiohttp.WSMsgType.BINARY:
                            print(msg.data[1:].decode('utf-8', errors='ignore'), end='')
            print("\n\n--- TEST PASSED ---")
        
    except Exception as e:
        print("\n❌ --- TEST FAILED ---")
        print(f"An exception occurred during the direct API call:")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
